{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offsets_index\n",
      " tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17])\n",
      "offsets_index shape\n",
      " torch.Size([1, 18, 3, 3])\n",
      "offset tensor([[[[-1.4198, -1.1172,  0.2967],\n",
      "          [-0.2645,  0.2241, -0.8772],\n",
      "          [ 0.2525,  0.9097,  1.4551]],\n",
      "\n",
      "         [[ 1.7059,  2.2560, -1.0157],\n",
      "          [ 0.7616, -0.1871,  0.2778],\n",
      "          [-1.3908,  1.7640, -0.5128]],\n",
      "\n",
      "         [[ 0.7245, -0.1834, -0.7508],\n",
      "          [ 0.1997, -1.2497, -1.5803],\n",
      "          [-1.1233, -2.1145,  0.1157]],\n",
      "\n",
      "         [[ 0.2094, -0.1484,  0.1239],\n",
      "          [-1.2003, -0.1065, -0.7535],\n",
      "          [-1.6961, -0.2545,  0.1769]],\n",
      "\n",
      "         [[ 0.2005,  0.0423,  0.1879],\n",
      "          [ 1.0130,  0.7128, -0.0151],\n",
      "          [-0.2724, -0.6138, -0.5169]],\n",
      "\n",
      "         [[-0.5861,  0.4597,  0.0547],\n",
      "          [ 0.4451,  0.0561, -0.2759],\n",
      "          [-0.2804,  0.3914,  1.2081]],\n",
      "\n",
      "         [[ 0.9473, -1.7411,  0.8102],\n",
      "          [-2.0532, -0.4960, -0.9598],\n",
      "          [-1.9330, -1.1477, -1.1995]],\n",
      "\n",
      "         [[ 0.7339,  0.2506,  0.2429],\n",
      "          [ 0.1098,  1.9246, -0.1143],\n",
      "          [-0.7480, -2.0139, -0.5664]],\n",
      "\n",
      "         [[-2.2153, -0.0082, -0.0749],\n",
      "          [ 0.1219,  0.4713, -1.3671],\n",
      "          [ 0.1131, -0.9015,  0.4715]],\n",
      "\n",
      "         [[-0.1565,  0.0068, -0.4249],\n",
      "          [-2.0544,  1.1329, -0.3185],\n",
      "          [-0.4030,  0.3459,  0.2588]],\n",
      "\n",
      "         [[-0.1396, -0.4961,  0.4771],\n",
      "          [ 1.2183, -0.4475, -1.5108],\n",
      "          [ 0.4155, -0.0749, -1.1666]],\n",
      "\n",
      "         [[ 0.3016, -0.7272,  1.1872],\n",
      "          [-0.9621,  0.8268, -0.3396],\n",
      "          [-0.0277,  0.1728,  0.4008]],\n",
      "\n",
      "         [[-0.6864,  1.0962, -0.8755],\n",
      "          [ 1.4192, -0.3279, -1.0053],\n",
      "          [-1.2477, -0.9518,  0.9188]],\n",
      "\n",
      "         [[ 1.7772,  1.3696,  0.3280],\n",
      "          [ 1.1846, -0.6128,  2.5185],\n",
      "          [-0.4844, -1.4328,  0.2933]],\n",
      "\n",
      "         [[ 2.4652,  0.8679, -0.2578],\n",
      "          [-0.5238, -0.9831, -0.1250],\n",
      "          [ 0.5502, -0.0780, -0.9231]],\n",
      "\n",
      "         [[-0.7890,  0.1658, -1.3108],\n",
      "          [-0.2278,  0.8038, -1.3212],\n",
      "          [-1.5956,  1.0258, -0.1759]],\n",
      "\n",
      "         [[-0.4161,  0.3789,  0.8239],\n",
      "          [-0.6386,  1.1732, -0.0243],\n",
      "          [-0.4298,  1.0107, -0.2242]],\n",
      "\n",
      "         [[ 0.1430, -1.5206, -0.3060],\n",
      "          [-1.1533,  1.0946, -0.5413],\n",
      "          [-0.4892, -0.9850, -1.9700]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DeformConv2D(nn.Module):\n",
    "    def __init__(self, inc, outc, kernel_size=3, padding=1, bias=None):\n",
    "        super(DeformConv2D, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.zero_padding = nn.ZeroPad2d(padding)\n",
    "        self.conv_kernel = nn.Conv2d(inc, outc, kernel_size=kernel_size, stride=kernel_size, bias=bias)\n",
    "\n",
    "    #kernel size 为3的时候，offset的Tensor尺寸是[b, 18, h, w]，offset传入的其实就是每个像素点的坐标偏移，也就是一个坐标量，最终每个点的像素还需要这个坐标偏移和原图进行对应求出。\n",
    "    def forward(self, x, offset):\n",
    "        dtype = offset.data.type()\n",
    "        ks = self.kernel_size\n",
    "        # N=9=3x3\n",
    "        N = offset.size(1) // 2\n",
    "\t\t\n",
    "\t\t#这里其实没必要，我们反正这个顺序是我们自己定义的，那我们直接按照[x1, x2, .... y1, y2, ...]定义不就好了。\n",
    "        # 将offset的顺序从[x1, y1, x2, y2, ...] 改成[x1, x2, .... y1, y2, ...]\n",
    "        offsets_index = Variable(torch.cat([torch.arange(0, 2*N, 2), torch.arange(1, 2*N+1, 2)]), requires_grad=False).type_as(x).long()\n",
    "        print(\"offsets_index\\n\",offsets_index)\n",
    "        # torch.unsqueeze()是为了增加维度,使offsets_index维度等于offset\n",
    "        offsets_index = offsets_index.unsqueeze(dim=0).unsqueeze(dim=-1).unsqueeze(dim=-1).expand(*offset.size())\n",
    "        print(\"offsets_index shape\\n\",offsets_index.shape)\n",
    "        # 根据维度dim按照索引列表index将offset重新排序，得到[x1, x2, .... y1, y2, ...]这样顺序的offset\n",
    "        offset = torch.gather(offset, dim=1, index=offsets_index)\n",
    "        print(\"offset\",offset)\n",
    "        # ------------------------------------------------------------------------\n",
    "\n",
    "        # 对输入x进行padding\n",
    "        if self.padding:\n",
    "            x = self.zero_padding(x)\n",
    "\n",
    "        # 将offset放到网格上，也就是标定出每一个坐标位置\n",
    "        # (b, 2N, h, w)\n",
    "        p = self._get_p(offset, dtype)\n",
    "\n",
    "        # 维度变换\n",
    "        # (b, h, w, 2N)\n",
    "        p = p.contiguous().permute(0, 2, 3, 1)\n",
    "        # floor是向下取整\n",
    "        q_lt = Variable(p.data, requires_grad=False).floor()\n",
    "        # +1相当于向上取整，这里为什么不用向上取整函数呢？是因为如果正好是整数的话，向上取整跟向下取整就重合了，这是我们不想看到的。\n",
    "        q_rb = q_lt + 1\n",
    "\n",
    "        # 将lt限制在图像范围内，其中[..., :N]代表x坐标，[..., N:]代表y坐标\n",
    "        q_lt = torch.cat([torch.clamp(q_lt[..., :N], 0, x.size(2)-1), torch.clamp(q_lt[..., N:], 0, x.size(3)-1)], dim=-1).long()\n",
    "        # 将rb限制在图像范围内\n",
    "        q_rb = torch.cat([torch.clamp(q_rb[..., :N], 0, x.size(2)-1), torch.clamp(q_rb[..., N:], 0, x.size(3)-1)], dim=-1).long()\n",
    "        # 获得lb\n",
    "        q_lb = torch.cat([q_lt[..., :N], q_rb[..., N:]], -1)\n",
    "        # 获得rt\n",
    "        q_rt = torch.cat([q_rb[..., :N], q_lt[..., N:]], -1)\n",
    "        \n",
    "        # 限制在一定的区域内,其实这部分可以写的很简单。有点花里胡哨的感觉。。在numpy中这样写：\n",
    "        #p = np.where(p >= 1, p, 0)\n",
    "        #p = np.where(p <x.shape[2]-1, p, x.shape[2]-1)\n",
    "\n",
    "        # 插值的时候需要考虑一下padding对原始索引的影响\n",
    "        # (b, h, w, N)\n",
    "        # torch.lt() 逐元素比较input和other，即是否input < other\n",
    "        # torch.rt() 逐元素比较input和other，即是否input > other\n",
    "        mask = torch.cat([p[..., :N].lt(self.padding)+p[..., :N].gt(x.size(2)-1-self.padding),\n",
    "                          p[..., N:].lt(self.padding)+p[..., N:].gt(x.size(3)-1-self.padding)], dim=-1).type_as(p)\n",
    "        #禁止反向传播\n",
    "        mask = mask.detach()\n",
    "\t\t#p - (p - torch.floor(p))不就是torch.floor(p)呢。。。\n",
    "        floor_p = p - (p - torch.floor(p))\n",
    "        #总的来说就是把超出图像的偏移量向下取整\n",
    "        p = p*(1-mask) + floor_p*mask\n",
    "        p = torch.cat([torch.clamp(p[..., :N], 0, x.size(2)-1), torch.clamp(p[..., N:], 0, x.size(3)-1)], dim=-1)\n",
    "\n",
    "\n",
    "        # bilinear kernel (b, h, w, N)\n",
    "        # 插值的4个系数\n",
    "        g_lt = (1 + (q_lt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_lt[..., N:].type_as(p) - p[..., N:]))\n",
    "        g_rb = (1 - (q_rb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_rb[..., N:].type_as(p) - p[..., N:]))\n",
    "        g_lb = (1 + (q_lb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_lb[..., N:].type_as(p) - p[..., N:]))\n",
    "        g_rt = (1 - (q_rt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_rt[..., N:].type_as(p) - p[..., N:]))\n",
    "\n",
    "        # (b, c, h, w, N)\n",
    "        x_q_lt = self._get_x_q(x, q_lt, N)\n",
    "        x_q_rb = self._get_x_q(x, q_rb, N)\n",
    "        x_q_lb = self._get_x_q(x, q_lb, N)\n",
    "        x_q_rt = self._get_x_q(x, q_rt, N)\n",
    "\n",
    "        # (b, c, h, w, N)\n",
    "        # 插值的最终操作在这里\n",
    "        x_offset = g_lt.unsqueeze(dim=1) * x_q_lt + \\\n",
    "                   g_rb.unsqueeze(dim=1) * x_q_rb + \\\n",
    "                   g_lb.unsqueeze(dim=1) * x_q_lb + \\\n",
    "                   g_rt.unsqueeze(dim=1) * x_q_rt\n",
    "\n",
    "        #偏置点含有九个方向的偏置，_reshape_x_offset() 把每个点9个方向的偏置转化成 3×3 的形式，\n",
    "        # 于是就可以用 3×3 stride=3 的卷积核进行 Deformable Convolution，\n",
    "        # 它等价于使用 1×1 的正常卷积核（包含了这个点9个方向的 context）对原特征直接进行卷积。\n",
    "        x_offset = self._reshape_x_offset(x_offset, ks)\n",
    "        \n",
    "        out = self.conv_kernel(x_offset)\n",
    "\n",
    "        return out\n",
    "\n",
    "    #求每个点的偏置方向\n",
    "    def _get_p_n(self, N, dtype):\n",
    "        p_n_x, p_n_y = np.meshgrid(range(-(self.kernel_size-1)//2, (self.kernel_size-1)//2+1),\n",
    "                          range(-(self.kernel_size-1)//2, (self.kernel_size-1)//2+1), indexing='ij')\n",
    "        # (2N, 1)\n",
    "        p_n = np.concatenate((p_n_x.flatten(), p_n_y.flatten()))\n",
    "        p_n = np.reshape(p_n, (1, 2*N, 1, 1))\n",
    "        p_n = Variable(torch.from_numpy(p_n).type(dtype), requires_grad=False)\n",
    "\n",
    "        return p_n\n",
    "\n",
    "    @staticmethod\n",
    "    #求每个点的坐标\n",
    "    def _get_p_0(h, w, N, dtype):\n",
    "        p_0_x, p_0_y = np.meshgrid(range(1, h+1), range(1, w+1), indexing='ij')\n",
    "        p_0_x = p_0_x.flatten().reshape(1, 1, h, w).repeat(N, axis=1)\n",
    "        p_0_y = p_0_y.flatten().reshape(1, 1, h, w).repeat(N, axis=1)\n",
    "        p_0 = np.concatenate((p_0_x, p_0_y), axis=1)\n",
    "        p_0 = Variable(torch.from_numpy(p_0).type(dtype), requires_grad=False)\n",
    "\n",
    "        return p_0\n",
    "\n",
    "    #求最后的偏置后的点=每个点的坐标+偏置方向+偏置\n",
    "    def _get_p(self, offset, dtype):\n",
    "        # N = 9, h, w\n",
    "        N, h, w = offset.size(1)//2, offset.size(2), offset.size(3)\n",
    "\n",
    "        # (1, 2N, 1, 1)\n",
    "        p_n = self._get_p_n(N, dtype)\n",
    "        # (1, 2N, h, w)\n",
    "        p_0 = self._get_p_0(h, w, N, dtype)\n",
    "        p = p_0 + p_n + offset\n",
    "        return p\n",
    "\n",
    "    #求出p点周围四个点的像素\n",
    "    def _get_x_q(self, x, q, N):\n",
    "        b, h, w, _ = q.size()\n",
    "        padded_w = x.size(3)\n",
    "        c = x.size(1)\n",
    "        # (b, c, h*w)将图片压缩到1维，方便后面的按照index索引提取\n",
    "        x = x.contiguous().view(b, c, -1)\n",
    "\n",
    "        # (b, h, w, N)这个目的就是将index索引均匀扩增到图片一样的h*w大小\n",
    "        index = q[..., :N]*padded_w + q[..., N:]  # offset_x*w + offset_y\n",
    "        # (b, c, h*w*N)\n",
    "        index = index.contiguous().unsqueeze(dim=1).expand(-1, c, -1, -1, -1).contiguous().view(b, c, -1)\n",
    "        \n",
    "        #双线性插值法就是4个点再乘以对应与 p 点的距离。获得偏置点 p 的值，这个 p 点是 9 个方向的偏置所以最后的 x_offset 是 b×c×h×w×9。\n",
    "        x_offset = x.gather(dim=-1, index=index).contiguous().view(b, c, h, w, N)\n",
    "\n",
    "        return x_offset\n",
    "\n",
    "    #_reshape_x_offset() 把每个点9个方向的偏置转化成 3×3 的形式\n",
    "    @staticmethod\n",
    "    def _reshape_x_offset(x_offset, ks):\n",
    "        b, c, h, w, N = x_offset.size()\n",
    "        x_offset = torch.cat([x_offset[..., s:s+ks].contiguous().view(b, c, h, w*ks) for s in range(0, N, ks)], dim=-1)\n",
    "        x_offset = x_offset.contiguous().view(b, c, h*ks, w*ks)\n",
    "\n",
    "        return x_offset\n",
    "    \n",
    "net = DeformConv2D(1, 1)\n",
    "offset = torch.randn(1, 18, 3, 3)\n",
    "x = torch.randn(1, 1, 3, 3)\n",
    "out = net(x, offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用的样例\n",
    "class DeformNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeformNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.offsets = nn.Conv2d(128, 18, kernel_size=3, padding=1)\n",
    "        self.conv4 = DeformConv2D(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.classifier = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convs\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        # deformable convolution\n",
    "        offsets = self.offsets(x)\n",
    "        x = F.relu(self.conv4(x, offsets))\n",
    "        x = self.bn4(x)\n",
    "        print(x)\n",
    "\n",
    "        x = F.avg_pool2d(x, kernel_size=28, stride=1).view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
