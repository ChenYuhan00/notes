{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"DeepLearning/","title":"\u6df1\u5ea6\u5b66\u4e60","text":"<p>\u8bb0\u5f55\u4e86\u5b66\u4e60pytorch\u548c\u795e\u7ecf\u7f51\u7edc</p> <p>\u611f\u89c9\u4f7f\u7528pytorch\u8fdb\u884c\u8bad\u7ec3\u57fa\u672c\u5927\u540c\u5c0f\u5f02\uff0c\u4e3b\u8981\u8981\u64cd\u4f5c\u7684\u5c31\u662f\u5b9a\u4e49\u7f51\u7edc\u3001dataloader\u548cdataiter\uff1b\u8bb8\u591a\u795e\u7ecf\u7f51\u7edc\u7684\u5927\u6982\u4e5f\u6709\u6240\u4e86\u89e3\uff1b\u76ee\u524d\u4e3b\u8981\u95ee\u9898\u662fpytorch\u7684\u5185\u90e8\u5b9e\u73b0\u4e0d\u61c2\uff0cnumpy\u7b49\u4f7f\u7528\u4e0d\u719f\u7ec3\uff0c\u7f51\u7edc\u5177\u4f53\u7ed3\u6784\u4e0d\u6e05\u695a</p> <p>\u53c2\u8003 \u6df1\u5165\u6d45\u51faPyTorch \u4ee5\u53ca dive into deep learning\u548c\u4ed6\u7684b\u7ad9\u89c6\u9891</p> <p></p>"},{"location":"DeepLearning/#pytorch","title":"Pytorch\u57fa\u7840","text":"<p>deep-learning-computation\u7ae0\u8282</p>"},{"location":"DeepLearning/#pytorch_1","title":"Pytorch\u81ea\u52a8\u6c42\u5bfc","text":"<p>\u6df1\u5165\u6d45\u51faPytroch \u7b2c\u4e8c\u7ae0</p>"},{"location":"DeepLearning/#_1","title":"\u8ba1\u7b97\u56fe","text":"<p>\u5c06\u4ee3\u7801\u5206\u89e3\u6210\u64cd\u4f5c\u5b50\uff0c\u5c06\u8ba1\u7b97\u8868\u793a\u4e3a\u4e00\u4e2a\u65e0\u73af\u56fe \u4e00\u822c\u53ea\u6709leaf node\u6709grad</p> <p>\u6700\u540e\u7684\u8f93\u51fa\u770b\u6210\u5173\u4e8e\u7f51\u7edc\u6743\u91cd\u7684\u51fd\u6570\uff0cbackward\u51fd\u6570\u8ba1\u7b97\u51fa\u6743\u91cd\u7684\u68af\u5ea6\uff08\u5168\u5fae\u5206\uff09</p> <p>\u5bf9\u4e8eleaf\u548crequire_grad\u7684\u8282\u70b9\u4e0d\u80fd\u591f\u8fdb\u884cinplace operation</p> <p>retain_graph \u9632\u6b62backward\u4e4b\u540e\u91ca\u653e\u76f8\u5173\u5185\u5b58</p> <p>.detach return a new tensor ,detached from the current graph,the result will never require gradient \u5c06\u8ba1\u7b97\u79fb\u52a8\u5230\u8ba1\u7b97\u56fe\u4e4b\u5916\uff0c\u5f53\u4f5c\u5e38\u6570</p>"},{"location":"DeepLearning/#_2","title":"\u6570\u636e\u8bfb\u53d6","text":""},{"location":"DeepLearning/#_3","title":"\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d","text":"<p>\u968f\u673a\u91c7\u6837b\u4e2a\u6837\u672c i_1,i_2,...,i_b\u6765\u8fd1\u4f3c\u635f\u5931 $$     \\frac{1}{b}\\sum_{i \\in I_b}l(\\hat y_i,y_i) $$ \u4e00\u6b21\u8fed\u4ee3\u7528b\u4e2a\u6570\u636e\u8ba1\u7b97\u540e\u66f4\u65b0\u53c2\u6570 \u4e00\u4e2aepoch\u5c06\u6570\u636e\u96c6\u7684\u6570\u636e\u90fd\u7528\u4e00\u904d</p>"},{"location":"DeepLearning/#data-iterator","title":"data iterator","text":"<p>\u4e00\u6b21\u968f\u673a\u8bfb\u53d6batch_size\u4e2a\u6570\u636e</p> <pre><code>def data_iter(batch_size, features, labels):\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)\n    for i in range(0, num_examples, batch_size):\n        batch_indices = torch.tensor(\n            indices[i: min(i + batch_size, num_examples)])\n        yield features[batch_indices], labels[batch_indices]\n</code></pre>"},{"location":"DeepLearning/#_4","title":"\u8865\u5145","text":"<ul> <li>cat**\u548c**stack \u53c2\u8003\u535a\u5ba2</li> <li>\u6279\u91cf\u77e9\u9635\u4e58\u6cd5 torch.bmm(X,Y),X\u7684shape\u4e3a(n,a,b)\uff0cY\u7684shape\u4e3a(n,b,c)\uff0c\u8f93\u51fa\u5f62\u72b6(n,a,c)</li> <li>torch.unsqueeze() \u5728\u6307\u5b9a\u7eac\u5ea6\u63d2\u5165\u7eac\u5ea61</li> <li>X.shape = (2,3) X.unsqueeze(0).shape = (1\uff0c2\uff0c3) X.unsqueeze(1).shape = (2,1,3)</li> </ul>"},{"location":"DeepLearning/#mlp","title":"MLP","text":"<p>dive into deep learning \u591a\u5c42\u611f\u77e5\u673a\u7ae0\u8282</p> <pre><code>num_inputs, num_outputs, num_hiddens = 784, 10, 256\n\nW1 = nn.Parameter(torch.randn(\n    num_inputs, num_hiddens, requires_grad=True) * 0.01)\nb1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\nW2 = nn.Parameter(torch.randn(\n    num_hiddens, num_outputs, requires_grad=True) * 0.01)\nb2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\n\nparams = [W1, b1, W2, b2]\n\n\ndef relu(X):\n    a = torch.zeros_like(X)\n    return torch.max(X, a)\n\ndef net(X):\n    X = X.reshape((-1, num_inputs))\n    H = relu(X@W1 + b1)  # \u8fd9\u91cc\u201c@\u201d\u4ee3\u8868\u77e9\u9635\u4e58\u6cd5\n    return (H@W2 + b2)\n</code></pre> <p>\u7528**torch.nn**</p> <pre><code>net = nn.Sequential(nn.Flatten(),\n                    nn.Linear(784, 256),\n                    nn.ReLU(),\n                    nn.Linear(256, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights);\n</code></pre> <p><code>X = X.reshape((-1, num_inputs))</code> -- <code>nn.Flatten()</code></p> <p><code>H = relu(X@W1 + b1)</code>--<code>nn.Linear(784, 256)</code>\u548c<code>nn.ReLu()</code></p> <p><code>(H@W2 + b2)</code>--<code>nn.Linear(256, 10)</code></p>"},{"location":"DeepLearning/#_5","title":"\u8fc7\u62df\u5408","text":""},{"location":"DeepLearning/#_6","title":"\u6b63\u5219\u5316","text":"\\begin{aligned} L(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 \\end{aligned}   \\begin{aligned} \\mathbf{w} &amp; \\leftarrow \\left(1- \\eta\\lambda \\right) \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned}"},{"location":"DeepLearning/#drop-out","title":"Drop out","text":"<p>$$ \\begin{aligned} h' = \\begin{cases}     0 &amp; \\text{ \u6982\u7387\u4e3a } p \\     \\frac{h}{1-p} &amp; \\text{ \u5176\u4ed6\u60c5\u51b5} \\end{cases} \\end{aligned} $$ \u671f\u671b\u503c\u4fdd\u6301\u4e0d\u53d8\uff0c\u5373E[h'] = h\u3002</p> <pre><code>def dropout_layer(X, dropout):\n    assert 0 &lt;= dropout &lt;= 1\n    # \u5728\u672c\u60c5\u51b5\u4e2d\uff0c\u6240\u6709\u5143\u7d20\u90fd\u88ab\u4e22\u5f03\n    if dropout == 1:\n        return torch.zeros_like(X)\n    # \u5728\u672c\u60c5\u51b5\u4e2d\uff0c\u6240\u6709\u5143\u7d20\u90fd\u88ab\u4fdd\u7559\n    if dropout == 0:\n        return X\n    mask = (torch.rand(X.shape) &gt; dropout).float()\n    return mask * X / (1.0 - dropout)\n</code></pre> <pre><code>dropout1, dropout2 = 0.2, 0.5\n\nclass Net(nn.Module):\n    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,\n                 is_training = True):\n        super(Net, self).__init__()\n        self.num_inputs = num_inputs\n        self.training = is_training\n        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\n        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\n        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\n        # \u53ea\u6709\u5728\u8bad\u7ec3\u6a21\u578b\u65f6\u624d\u4f7f\u7528dropout\n        if self.training == True:\n            # \u5728\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42\n            H1 = dropout_layer(H1, dropout1)\n        H2 = self.relu(self.lin2(H1))\n        if self.training == True:\n            # \u5728\u7b2c\u4e8c\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42\n            H2 = dropout_layer(H2, dropout2)\n        out = self.lin3(H2)\n        return out\n\n\nnet = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)\n</code></pre> <p>\u76f4\u63a5\u7528<code>nn.Dropout(p)</code></p> <pre><code>net = nn.Sequential(nn.Flatten(),\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        # \u5728\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42\n        nn.Dropout(dropout1),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n        # \u5728\u7b2c\u4e8c\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42\n        nn.Dropout(dropout2),\n        nn.Linear(256, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights);\n</code></pre>"},{"location":"DeepLearning/#_7","title":"\u53c2\u6570\u521d\u59cb\u5316","text":""},{"location":"DeepLearning/#xavier","title":"Xavier\u521d\u59cb\u5316","text":"o_{i} = \\sum_{j=1}^{n_\\mathrm{in}} w_{ij} x_j.  \\begin{aligned}     E[o_i] &amp; = \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij} x_j] \\\\&amp;= \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij}] E[x_j] \\\\&amp;= 0, \\\\     \\mathrm{Var}[o_i] &amp; = E[o_i^2] - (E[o_i])^2 \\\\         &amp; = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\\\         &amp; = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\\\         &amp; = n_\\mathrm{in} \\sigma^2 \\gamma^2. \\end{aligned}  <p>\u4f7f\u65b9\u5dee\u6ee1\u8db3 $$ \\begin{aligned} \\frac{1}{2} (n_\\mathrm{in} + n_\\mathrm{out}) \\sigma^2 = 1 \\text{ or } \\sigma = \\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}. \\end{aligned} $$</p> <p>Xavier\u521d\u59cb\u5316\u901a\u5e38\u4ece\u65b9\u5dee\u6ee1\u8db3\u4e0a\u8ff0\u7684\u5747\u5300\u5206\u5e03\u6216\u9ad8\u65af\u5206\u5e03\u4e2d\u91c7\u6837\u6743\u91cd</p>"},{"location":"DeepLearning/#cnn","title":"CNN","text":"<p>\u5377\u79ef\u64cd\u4f5c\u5b9e\u73b0</p> <pre><code>def corr2d(X, K): \n    h, w = K.shape\n    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n    return Y\n</code></pre> <p>\u5377\u79ef\u5c42</p> <pre><code>class Conv2D(nn.Module):\n    def __init__(self, kernel_size):\n        super().__init__()\n        self.weight = nn.Parameter(torch.rand(kernel_size))\n        self.bias = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        return corr2d(x, self.weight) + self.bias\n</code></pre> <p>backprop\u8bad\u7ec3\u5377\u79ef\u6838</p> <p>nn.Conv2d()\u8f93\u5165\u548c\u8f93\u51fa:\u6279\u91cf\u5927\u5c0f\u3001\u901a\u9053\u3001\u9ad8\u5ea6\u3001\u5bbd\u5ea6</p>"},{"location":"DeepLearning/#_8","title":"\u591a\u8f93\u5165\u8f93\u51fa\u901a\u9053","text":"<p>\u591a\u8f93\u5165\u901a\u9053:</p> <pre><code>def corr2d_multi_in(X, K):\n    # \u5148\u904d\u5386\u201cX\u201d\u548c\u201cK\u201d\u7684\u7b2c0\u4e2a\u7ef4\u5ea6\uff08\u901a\u9053\u7ef4\u5ea6\uff09\uff0c\u518d\u628a\u5b83\u4eec\u52a0\u5728\u4e00\u8d77\n    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))\n</code></pre> <p>\u591a\u8f93\u51fa\u901a\u9053:</p> <pre><code>def corr2d_multi_in_out(X, K):\n    # \u8fed\u4ee3\u201cK\u201d\u7684\u7b2c0\u4e2a\u7ef4\u5ea6\uff0c\u6bcf\u6b21\u90fd\u5bf9\u8f93\u5165\u201cX\u201d\u6267\u884c\u4e92\u76f8\u5173\u8fd0\u7b97\u3002\n    # \u6700\u540e\u5c06\u6240\u6709\u7ed3\u679c\u90fd\u53e0\u52a0\u5728\u4e00\u8d77\n    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)\n</code></pre> <p>\u8f93\u5165\\mathbf{X} : c_i \\times n_h \\times n_w \u6838$\\mathbf{{W}} : c_0 \\times c_i \\times k_h \\times k_w $ \u8f93\u51fa\\mathbf{Y} : c_o \\times m_h \\times m_w </p>"},{"location":"DeepLearning/#pooling","title":"Pooling","text":"<pre><code>nn.MaxPool2d(3, padding=1, stride=2)\npool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n</code></pre> <p>\u591a\u901a\u9053\u5bf9\u6bcf\u4e2a\u901a\u9053\u8fdb\u884cpooling\uff0c\u8f93\u51fa\u901a\u9053\u6570\u4e0e\u8f93\u5165\u76f8\u540c</p>"},{"location":"DeepLearning/#lenet","title":"LeNet","text":"<pre><code>net = nn.Sequential(\n    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Flatten(),\n    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n    nn.Linear(120, 84), nn.Sigmoid(),\n    nn.Linear(84, 10))\n</code></pre> <p>Conv2d output shape: torch.Size([1, 6, 28, 28]) Sigmoid output shape: torch.Size([1, 6, 28, 28]) AvgPool2d output shape: torch.Size([1, 6, 14, 14]) Conv2d output shape: torch.Size([1, 16, 10, 10]) Sigmoid output shape: torch.Size([1, 16, 10, 10]) AvgPool2d output shape: torch.Size([1, 16, 5, 5]) Flatten output shape: torch.Size([1, 400]) Linear output shape: torch.Size([1, 120]) Sigmoid output shape: torch.Size([1, 120]) Linear output shape: torch.Size([1, 84]) Sigmoid output shape: torch.Size([1, 84]) Linear output shape: torch.Size([1, 10])</p>"},{"location":"DeepLearning/#alexnet","title":"AlexNet","text":"<p>\u4f7f\u7528ReLU\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0cDropout\uff0c\u6570\u636e\u96c6\u9884\u5904\u7406\uff08\u88c1\u5207\u3001\u7ffb\u8f6c\u3001\u53d8\u8272\uff09</p> <pre><code>net = nn.Sequential(\n    # \u8fd9\u91cc\u4f7f\u7528\u4e00\u4e2a11*11\u7684\u66f4\u5927\u7a97\u53e3\u6765\u6355\u6349\u5bf9\u8c61\u3002\n    # \u540c\u65f6\uff0c\u6b65\u5e45\u4e3a4\uff0c\u4ee5\u51cf\u5c11\u8f93\u51fa\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002\n    # \u53e6\u5916\uff0c\u8f93\u51fa\u901a\u9053\u7684\u6570\u76ee\u8fdc\u5927\u4e8eLeNet\n    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    # \u51cf\u5c0f\u5377\u79ef\u7a97\u53e3\uff0c\u4f7f\u7528\u586b\u5145\u4e3a2\u6765\u4f7f\u5f97\u8f93\u5165\u4e0e\u8f93\u51fa\u7684\u9ad8\u548c\u5bbd\u4e00\u81f4\uff0c\u4e14\u589e\u5927\u8f93\u51fa\u901a\u9053\u6570\n    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    # \u4f7f\u7528\u4e09\u4e2a\u8fde\u7eed\u7684\u5377\u79ef\u5c42\u548c\u8f83\u5c0f\u7684\u5377\u79ef\u7a97\u53e3\u3002\n    # \u9664\u4e86\u6700\u540e\u7684\u5377\u79ef\u5c42\uff0c\u8f93\u51fa\u901a\u9053\u7684\u6570\u91cf\u8fdb\u4e00\u6b65\u589e\u52a0\u3002\n    # \u5728\u524d\u4e24\u4e2a\u5377\u79ef\u5c42\u4e4b\u540e\uff0c\u6c47\u805a\u5c42\u4e0d\u7528\u4e8e\u51cf\u5c11\u8f93\u5165\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\n    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    nn.Flatten(),\n    # \u8fd9\u91cc\uff0c\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\u6570\u91cf\u662fLeNet\u4e2d\u7684\u597d\u51e0\u500d\u3002\u4f7f\u7528dropout\u5c42\u6765\u51cf\u8f7b\u8fc7\u62df\u5408\n    nn.Linear(6400, 4096), nn.ReLU(),\n    nn.Dropout(p=0.5),\n    nn.Linear(4096, 4096), nn.ReLU(),\n    nn.Dropout(p=0.5),\n    # \u6700\u540e\u662f\u8f93\u51fa\u5c42\u3002\u7531\u4e8e\u8fd9\u91cc\u4f7f\u7528Fashion-MNIST\uff0c\u6240\u4ee5\u7528\u7c7b\u522b\u6570\u4e3a10\uff0c\u800c\u975e\u8bba\u6587\u4e2d\u76841000\n    nn.Linear(4096, 10))\n</code></pre> <p>Conv2d output shape:torch.Size([1, 96, 54, 54]) ReLU output shape:torch.Size([1, 96, 54, 54]) MaxPool2d output shape:torch.Size([1, 96, 26, 26]) Conv2d output shape:torch.Size([1, 256, 26, 26]) ReLU output shape:torch.Size([1, 256, 26, 26]) MaxPool2d output shape:torch.Size([1, 256, 12, 12]) Conv2d output shape:torch.Size([1, 384, 12, 12]) ReLU output shape:torch.Size([1, 384, 12, 12]) Conv2d output shape:torch.Size([1, 384, 12, 12]) ReLU output shape:torch.Size([1, 384, 12, 12]) Conv2d output shape:torch.Size([1, 256, 12, 12]) ReLU output shape:torch.Size([1, 256, 12, 12]) MaxPool2d output shape:torch.Size([1, 256, 5, 5]) Flatten output shape:torch.Size([1, 6400]) Linear output shape:torch.Size([1, 4096]) ReLU output shape:torch.Size([1, 4096]) Dropout output shape:torch.Size([1, 4096]) Linear output shape:torch.Size([1, 4096]) ReLU output shape:torch.Size([1, 4096]) Dropout output shape:torch.Size([1, 4096]) Linear output shape:torch.Size([1, 10])</p>"},{"location":"DeepLearning/#vgg","title":"VGG","text":"<p>VGG\u5757 kernel\u5927\u5c0f\u90fd\u4e3a3 \\times 3</p> <pre><code>def vgg_block(num_convs, in_channels, out_channels):\n    layers = []\n    for _ in range(num_convs):\n        layers.append(nn.Conv2d(in_channels, out_channels,\n                                kernel_size=3, padding=1))\n        layers.append(nn.ReLU())\n        in_channels = out_channels\n    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n    return nn.Sequential(*layers)\n</code></pre> <p>VGG\u7f51\u7edc VGG-11</p> <pre><code>conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\ndef vgg(conv_arch):\n    conv_blks = []\n    in_channels = 1\n    # \u5377\u79ef\u5c42\u90e8\u5206\n    for (num_convs, out_channels) in conv_arch:\n        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n        in_channels = out_channels\n\n    return nn.Sequential(\n        *conv_blks, nn.Flatten(),\n        # \u5168\u8fde\u63a5\u5c42\u90e8\u5206\n        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n        nn.Linear(4096, 10))\n\nnet = vgg(conv_arch)\n</code></pre>"},{"location":"DeepLearning/#nin","title":"NIN","text":"<p>\u5728\u6bcf\u4e2a\u50cf\u7d20\u7684\u901a\u9053\u4e0a\u5206\u522b\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a</p> <pre><code>def nin_block(in_channels, out_channels, kernel_size, strides, padding):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\n        nn.ReLU(),\n        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())\n</code></pre> <pre><code>net = nn.Sequential(\n    nin_block(1, 96, kernel_size=11, strides=4, padding=0),\n    nn.MaxPool2d(3, stride=2),\n    nin_block(96, 256, kernel_size=5, strides=1, padding=2),\n    nn.MaxPool2d(3, stride=2),\n    nin_block(256, 384, kernel_size=3, strides=1, padding=1),\n    nn.MaxPool2d(3, stride=2),\n    nn.Dropout(0.5),\n    # \u6807\u7b7e\u7c7b\u522b\u6570\u662f10\n    nin_block(384, 10, kernel_size=3, strides=1, padding=1),\n    nn.AdaptiveAvgPool2d((1, 1)),\n    # \u5c06\u56db\u7ef4\u7684\u8f93\u51fa\u8f6c\u6210\u4e8c\u7ef4\u7684\u8f93\u51fa\uff0c\u5176\u5f62\u72b6\u4e3a(\u6279\u91cf\u5927\u5c0f,10)\n    nn.Flatten())\n</code></pre> <p><code>nn.AdaptiveAvgPool2d()</code>\u53c2\u6570\u4e3a\u6307\u5b9a\u8f93\u51fasize</p>"},{"location":"DeepLearning/#googlenet","title":"GoogLeNet","text":"<pre><code>class Inception(nn.Module):\n    # c1--c4\u662f\u6bcf\u6761\u8def\u5f84\u7684\u8f93\u51fa\u901a\u9053\u6570\n    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n        super(Inception, self).__init__(**kwargs)\n        # \u7ebf\u8def1\uff0c\u53551x1\u5377\u79ef\u5c42\n        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n        # \u7ebf\u8def2\uff0c1x1\u5377\u79ef\u5c42\u540e\u63a53x3\u5377\u79ef\u5c42\n        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n        # \u7ebf\u8def3\uff0c1x1\u5377\u79ef\u5c42\u540e\u63a55x5\u5377\u79ef\u5c42\n        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n        # \u7ebf\u8def4\uff0c3x3\u6700\u5927\u6c47\u805a\u5c42\u540e\u63a51x1\u5377\u79ef\u5c42\n        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n\n    def forward(self, x):\n        p1 = F.relu(self.p1_1(x))\n        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n        p4 = F.relu(self.p4_2(self.p4_1(x)))\n        # \u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u8fde\u7ed3\u8f93\u51fa\n        return torch.cat((p1, p2, p3, p4), dim=1)\n</code></pre> <p>\u8d85\u53c2\u6570\u4e3b\u8981\u4e3a\u901a\u9053\u6570,\u6bd4\u5982 <code>Inception(192, 64, (96, 128), (16, 32), 32)</code> \u8868\u793a\u4e00\u4e2aInception\u7684\u8f93\u5165\u901a\u9053\u6570\u4e3a192\uff0c\u8f93\u51fa\u901a\u9053\u6570\u4e3a 64 + 128 + 32 + 32 = 512</p> <p>\u7f51\u7edc\u6574\u4f53\u7ed3\u6784\u7565</p>"},{"location":"DeepLearning/#batch-norm","title":"Batch-Norm","text":"\\mathrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}. <p>\u5f52\u4e00\u5316\u540e\u7684\u65b9\u5dee\u4e3a\\gamma\uff0c\u5747\u503c\u4e3a1 + \\beta</p> <p>\\gamma,\\beta \u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570</p> <p>\u5377\u79ef\u5c42\u6709\u591a\u4e2a\u901a\u9053\u65f6\uff0c\u5bf9\u6bcf\u4e2a\u901a\u9053\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u5047\u8bbebatch_size\u4e3am\uff0c\u5377\u79ef\u5c42\u8f93\u51fa\u7684\u5927\u5c0f\u4e3ap\\timesq,\u5728\u6bcf\u4e2a\u8f93\u51fa\u901a\u9053\u4e0a\u7684mqp\u4e2a\u5143\u7d20\u4e0a\u6c42\u5747\u503c\u548c\u65b9\u5dee</p> <pre><code>def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n    '''\n    moving_mean\u548cmoving_var\u8fd1\u4f3c\u8ba4\u4e3a\u5168\u5c40\u503c\n    eps\u9632\u6b62\u65b9\u5dee\u4e3a0\uff0c\u56fa\u5b9a\u503c\n    gmma,beta,moving_mean,moving_var\u7684\u5f62\u72b6\u548cX\u76f8\u540c\n    '''\n    # \u901a\u8fc7is_grad_enabled\u6765\u5224\u65ad\u5f53\u524d\u6a21\u5f0f\u662f\u8bad\u7ec3\u6a21\u5f0f\u8fd8\u662f\u9884\u6d4b\u6a21\u5f0f\n    if not torch.is_grad_enabled():\n        # \u5982\u679c\u662f\u5728\u9884\u6d4b\u6a21\u5f0f\u4e0b\uff0c\u76f4\u63a5\u4f7f\u7528\u4f20\u5165\u7684\u79fb\u52a8\u5e73\u5747\u6240\u5f97\u7684\u5747\u503c\u548c\u65b9\u5dee\n        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n    else:\n        assert len(X.shape) in (2, 4)\n        if len(X.shape) == 2:\n            # \u4f7f\u7528\u5168\u8fde\u63a5\u5c42\u7684\u60c5\u51b5\uff0c\u8ba1\u7b97\u7279\u5f81\u7ef4\u4e0a\u7684\u5747\u503c\u548c\u65b9\u5dee\n            mean = X.mean(dim=0)\n            var = ((X - mean) ** 2).mean(dim=0)\n        else:\n            # \u4f7f\u7528\u4e8c\u7ef4\u5377\u79ef\u5c42\u7684\u60c5\u51b5\uff0c\u8ba1\u7b97\u901a\u9053\u7ef4\u4e0a\uff08axis=1\uff09\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002\n            # \u8fd9\u91cc\u6211\u4eec\u9700\u8981\u4fdd\u6301X\u7684\u5f62\u72b6\u4ee5\u4fbf\u540e\u9762\u53ef\u4ee5\u505a\u5e7f\u64ad\u8fd0\u7b97\n            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n        # \u8bad\u7ec3\u6a21\u5f0f\u4e0b\uff0c\u7528\u5f53\u524d\u7684\u5747\u503c\u548c\u65b9\u5dee\u505a\u6807\u51c6\u5316\n        X_hat = (X - mean) / torch.sqrt(var + eps)\n        # \u66f4\u65b0\u79fb\u52a8\u5e73\u5747\u7684\u5747\u503c\u548c\u65b9\u5dee\n        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n        moving_var = momentum * moving_var + (1.0 - momentum) * var\n    Y = gamma * X_hat + beta  # \u7f29\u653e\u548c\u79fb\u4f4d\n    return Y, moving_mean.data, moving_var.data\n</code></pre> <pre><code>class BatchNorm(nn.Module):\n    # num_features\uff1a\u5b8c\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\u6570\u91cf\u6216\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u3002\n    # num_dims\uff1a2\u8868\u793a\u5b8c\u5168\u8fde\u63a5\u5c42\uff0c4\u8868\u793a\u5377\u79ef\u5c42\n    def __init__(self, num_features, num_dims):\n        super().__init__()\n        if num_dims == 2:\n            shape = (1, num_features)\n        else:\n            shape = (1, num_features, 1, 1)\n        # \u53c2\u4e0e\u6c42\u68af\u5ea6\u548c\u8fed\u4ee3\u7684\u62c9\u4f38\u548c\u504f\u79fb\u53c2\u6570\uff0c\u5206\u522b\u521d\u59cb\u5316\u62101\u548c0\n        self.gamma = nn.Parameter(torch.ones(shape))\n        self.beta = nn.Parameter(torch.zeros(shape))\n        # \u975e\u6a21\u578b\u53c2\u6570\u7684\u53d8\u91cf\u521d\u59cb\u5316\u4e3a0\u548c1\n        self.moving_mean = torch.zeros(shape)\n        self.moving_var = torch.ones(shape)\n\n    def forward(self, X):\n        # \u5982\u679cX\u4e0d\u5728\u5185\u5b58\u4e0a\uff0c\u5c06moving_mean\u548cmoving_var\n        # \u590d\u5236\u5230X\u6240\u5728\u663e\u5b58\u4e0a\n        if self.moving_mean.device != X.device:\n            self.moving_mean = self.moving_mean.to(X.device)\n            self.moving_var = self.moving_var.to(X.device)\n        # \u4fdd\u5b58\u66f4\u65b0\u8fc7\u7684moving_mean\u548cmoving_var\n        Y, self.moving_mean, self.moving_var = batch_norm(\n            X, self.gamma, self.beta, self.moving_mean,\n            self.moving_var, eps=1e-5, momentum=0.9)\n        return Y\n</code></pre> <p>pytorch\u4f7f\u7528batch_norm:<code>nn.BatchNorm1d(),nn.BatchNorm2d()</code>\uff0c\u53c2\u6570\u4e3a\u901a\u9053\u6570\uff0c\u5206\u522b\u8868\u793a\u5168\u8fde\u63a5\u5c42\u548c\u5377\u79ef\u5c42</p>"},{"location":"DeepLearning/#resnet","title":"ResNet","text":"<p>\u8f93\u5165X\u548c\u8f93\u51faY\u7684\u5f62\u72b6\u8981\u76f8\u540c,3\\times3\u7684\u5377\u79ef\u6838\u6ca1\u6709\u6539\u53d8\u5f62\u72b6\uff0c\u4e3b\u8981\u662f\u901a\u9053\u6570\uff0c\u4e5f\u53ef\u4ee5\u75281\\times1\u7684\u5377\u79ef\u6838\u4fee\u6539X\u7684\u901a\u9053\u6570</p> <pre><code>class Residual(nn.Module): \n    def __init__(self, input_channels, num_channels,\n                 use_1x1conv=False, strides=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(input_channels, num_channels,\n                               kernel_size=3, padding=1, stride=strides)\n        self.conv2 = nn.Conv2d(num_channels, num_channels,\n                               kernel_size=3, padding=1)\n        if use_1x1conv:\n            self.conv3 = nn.Conv2d(input_channels, num_channels,\n                                   kernel_size=1, stride=strides)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n\n    def forward(self, X):\n        Y = F.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3:\n            X = self.conv3(X)\n        Y += X\n        return F.relu(Y)\n</code></pre>"},{"location":"DeepLearning/#densenet","title":"DenseNet","text":"<p>\u7f51\u7edc\u4e3b\u8981\u7531\u4e24\u90e8\u5206 \u7a20\u5bc6\u5c42\u548c\u8fc7\u6e21\u5c42</p> <p>\u628a\u524d\u9762\u7f51\u7edc\u5c42\u7684\u8f93\u51fa\u90fd\u4f5c\u4e3a\u8f93\u51fa\uff0c\u6bcf\u4e2a\u7f51\u7edc\u7684\u8f93\u5165\u4e3a\u524d\u9762\u6240\u6709\u7f51\u7edc\u5c42\u7684\u8f93\u51fa\u52a0\u4e0ainput</p> <pre><code>def conv_block(input_channels, num_channels):\n    return nn.Sequential(\n        nn.BatchNorm2d(input_channels), nn.ReLU(),\n        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_convs, input_channels, num_channels):\n        super(DenseBlock, self).__init__()\n        layer = []\n        for i in range(num_convs):\n            layer.append(conv_block(\n                num_channels * i + input_channels, num_channels))\n        self.net = nn.Sequential(*layer)\n\n    def forward(self, X):\n        for blk in self.net:\n            Y = blk(X)\n            # \u8fde\u63a5\u901a\u9053\u7ef4\u5ea6\u4e0a\u6bcf\u4e2a\u5757\u7684\u8f93\u5165\u548c\u8f93\u51fa\n            X = torch.cat((X, Y), dim=1)\n        return X\n</code></pre> <p>\u901a\u8fc7\u8fc7\u6e21\u5c42\u901a\u9053\u6570\u51cf\u5c11\uff0c\u9ad8\u548c\u5bbd\u51cf\u534a</p> <pre><code>def transition_block(input_channels, num_channels):\n    return nn.Sequential(\n        nn.BatchNorm2d(input_channels), nn.ReLU(),\n        nn.Conv2d(input_channels, num_channels, kernel_size=1),\n        nn.AvgPool2d(kernel_size=2, stride=2))\n</code></pre>"},{"location":"DeepLearning/#rnn","title":"RNN","text":""},{"location":"DeepLearning/#_9","title":"\u5e8f\u5217\u6a21\u578b","text":"<p>\u901a\u8fc7\\mathbf{x}_t = [x_{t-\\tau}, \\ldots, x_{t-1}] \u7684\u53d6\u503c\uff0c\u9884\u6d4by_t = x_t\u3002</p>"},{"location":"DeepLearning/#_10","title":"\u6587\u672c\u9884\u5904\u7406","text":"<ol> <li>\u8bfb\u53d6\u6570\u636e\u96c6\u5f97\u5230\u6240\u6709\u6587\u672c\u884c\uff08lines\uff09 <code>the time machine by h g wells</code></li> <li>\u8bcd\u5143\u5316\uff08tokenize\uff09 \u628alines\u5206\u6210\u4e00\u4e2a\u4e00\u4e2a\u8bcd <code>['the', 'time', 'machine', 'by', 'h', 'g', 'wells']</code></li> <li>\u5efa\u7acb\u8bcd\u8868\uff08vocabulary\uff09</li> </ol> <pre><code>class Vocab:  \n    \"\"\"\u6587\u672c\u8bcd\u8868\"\"\"\n    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n        if tokens is None:\n            tokens = []\n        if reserved_tokens is None:\n            reserved_tokens = []\n        # \u6309\u51fa\u73b0\u9891\u7387\u6392\u5e8f\n        counter = count_corpus(tokens)\n        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n                                   reverse=True)\n        # \u672a\u77e5\u8bcd\u5143\u7684\u7d22\u5f15\u4e3a0\n        self.idx_to_token = ['&lt;unk&gt;'] + reserved_tokens\n        self.token_to_idx = {token: idx\n                             for idx, token in enumerate(self.idx_to_token)}\n        for token, freq in self._token_freqs:\n            if freq &lt; min_freq:\n                break\n            if token not in self.token_to_idx:\n                self.idx_to_token.append(token)\n                self.token_to_idx[token] = len(self.idx_to_token) - 1\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):#\u7ed9token\u8fd4\u56deindex\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):#\u7ed9index\u8fd4\u56detoken\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n\n    @property\n    def unk(self):  # \u672a\u77e5\u8bcd\u5143\u7684\u7d22\u5f15\u4e3a0\n        return 0\n\n    @property\n    def token_freqs(self):\n        return self._token_freqs\n\ndef count_corpus(tokens):  #@save\n    \"\"\"\u7edf\u8ba1\u8bcd\u5143\u7684\u9891\u7387\"\"\"\n    # \u8fd9\u91cc\u7684tokens\u662f1D\u5217\u8868\u62162D\u5217\u8868\n    if len(tokens) == 0 or isinstance(tokens[0], list):\n        # \u5c06\u8bcd\u5143\u5217\u8868\u5c55\u5e73\u6210\u4e00\u4e2a\u5217\u8868\n        tokens = [token for line in tokens for token in line]\n    return collections.Counter(tokens)\n</code></pre> <p>\u8f6c\u6362\u7ed3\u679c\uff08\u8bcd\u9891\u9ad8\u4e0b\u6807\u5c0f\uff09</p> <pre><code>vocab = Vocab(tokens)\nprint(list(vocab.token_to_idx.items())[:10])\n</code></pre> <p>[('', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]"},{"location":"DeepLearning/#language-model","title":"Language Model","text":"<p>n\u5143\u8bed\u6cd5 $$ \\begin{aligned} P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2) P(x_3) P(x_4),\\ P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \\mid  x_1) P(x_3  \\mid  x_2) P(x_4  \\mid  x_3),\\ P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \\mid  x_1) P(x_3  \\mid  x_1, x_2) P(x_4  \\mid  x_2, x_3). \\end{aligned} $$</p> <p>\u5f97\u5230\u4e8c\u5143\u8bcd</p> <pre><code>bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]\nbigram_vocab = d2l.Vocab(bigram_tokens)\n</code></pre> <p>\u7b2ci\u4e2a\u6700\u5e38\u7528\u7684\u9891\u7387n_i \\log n_i = -\\alpha \\log i + c, \u867d\u7136n\u5143\u8bcd\u7ec4\u5408\u65b9\u5f0f\u589e\u52a0\uff0c\u4f46\u662f\u7531\u4e8e\u5927\u81f4\u9075\u5faa\u4e0a\u8ff0\u89c4\u5f8b\uff0c\u5c06\u8bcd\u9891\u5c0f\u4e8e\u67d0\u4e2a\u503c\u7684\u7565\u53bb\uff0c\u8bcd\u8868\u4e2d\u8bb0\u5f55\u53cd\u800c\u66f4\u5c11</p>"},{"location":"DeepLearning/#_11","title":"\u8bfb\u53d6\u957f\u5e8f\u5217\u6570\u636e","text":""},{"location":"DeepLearning/#_12","title":"\u968f\u673a\u91c7\u6837","text":"<pre><code>def seq_data_iter_random(corpus, batch_size, num_steps):\n    \"\"\"\u4f7f\u7528\u968f\u673a\u62bd\u6837\u751f\u6210\u4e00\u4e2a\u5c0f\u6279\u91cf\u5b50\u5e8f\u5217\"\"\"\n    # \u4ece\u968f\u673a\u504f\u79fb\u91cf\u5f00\u59cb\u5bf9\u5e8f\u5217\u8fdb\u884c\u5206\u533a\uff0c\u968f\u673a\u8303\u56f4\u5305\u62ecnum_steps-1\n    corpus = corpus[random.randint(0, num_steps - 1):]\n    # \u51cf\u53bb1\uff0c\u662f\u56e0\u4e3a\u6211\u4eec\u9700\u8981\u8003\u8651\u6807\u7b7e\n    num_subseqs = (len(corpus) - 1) // num_steps\n    # \u957f\u5ea6\u4e3anum_steps\u7684\u5b50\u5e8f\u5217\u7684\u8d77\u59cb\u7d22\u5f15\n    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n    # \u5728\u968f\u673a\u62bd\u6837\u7684\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\uff0c\n    # \u6765\u81ea\u4e24\u4e2a\u76f8\u90bb\u7684\u3001\u968f\u673a\u7684\u3001\u5c0f\u6279\u91cf\u4e2d\u7684\u5b50\u5e8f\u5217\u4e0d\u4e00\u5b9a\u5728\u539f\u59cb\u5e8f\u5217\u4e0a\u76f8\u90bb\n    random.shuffle(initial_indices)\n\n    def data(pos):\n        # \u8fd4\u56de\u4ecepos\u4f4d\u7f6e\u5f00\u59cb\u7684\u957f\u5ea6\u4e3anum_steps\u7684\u5e8f\u5217\n        return corpus[pos: pos + num_steps]\n\n    num_batches = num_subseqs // batch_size\n    for i in range(0, batch_size * num_batches, batch_size):\n        # \u5728\u8fd9\u91cc\uff0cinitial_indices\u5305\u542b\u5b50\u5e8f\u5217\u7684\u968f\u673a\u8d77\u59cb\u7d22\u5f15\n        initial_indices_per_batch = initial_indices[i: i + batch_size]\n        X = [data(j) for j in initial_indices_per_batch]\n        Y = [data(j + 1) for j in initial_indices_per_batch]\n        yield torch.tensor(X), torch.tensor(Y)\n</code></pre> <p>\u751f\u6210\u6837\u4f8b(0-34\u5e8f\u5217,batch_size=2,num_steps=5)</p> <p>X:  tensor([ [13, 14, 15, 16, 17],         [28, 29, 30, 31, 32] ]) Y: tensor([ [14, 15, 16, 17, 18],         [29, 30, 31, 32, 33] ]) X:  tensor([ [ 3,  4,  5,  6,  7],         [18, 19, 20, 21, 22] ]) Y: tensor([ [ 4,  5,  6,  7,  8],         [19, 20, 21, 22, 23] ]) X:  tensor([ [ 8,  9, 10, 11, 12],         [23, 24, 25, 26, 27] ]) Y: tensor([ [ 9, 10, 11, 12, 13],         [24, 25, 26, 27, 28] ])</p>"},{"location":"DeepLearning/#_13","title":"\u987a\u5e8f\u91c7\u6837","text":"<pre><code>def seq_data_iter_sequential(corpus, batch_size, num_steps):\n    \"\"\"\u4f7f\u7528\u987a\u5e8f\u5206\u533a\u751f\u6210\u4e00\u4e2a\u5c0f\u6279\u91cf\u5b50\u5e8f\u5217\"\"\"\n    # \u4ece\u968f\u673a\u504f\u79fb\u91cf\u5f00\u59cb\u5212\u5206\u5e8f\u5217\n    offset = random.randint(0, num_steps)\n    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n    num_batches = Xs.shape[1] // num_steps\n    for i in range(0, num_steps * num_batches, num_steps):\n        X = Xs[:, i: i + num_steps]\n        Y = Ys[:, i: i + num_steps]\n        yield X, Y\n</code></pre> <p>\u751f\u6210\u6837\u4f8b</p> <p>X:  tensor([ [ 0,  1,  2,  3,  4],         [17, 18, 19, 20, 21] ]) Y: tensor([ [ 1,  2,  3,  4,  5],         [18, 19, 20, 21, 22] ]) X:  tensor([ [ 5,  6,  7,  8,  9],         [22, 23, 24, 25, 26] ]) Y: tensor([ [ 6,  7,  8,  9, 10],         [23, 24, 25, 26, 27] ]) X:  tensor([ [10, 11, 12, 13, 14],         [27, 28, 29, 30, 31] ]) Y: tensor([ [11, 12, 13, 14, 15],         [28, 29, 30, 31, 32] ])</p>"},{"location":"DeepLearning/#rnn_1","title":"RNN\u5b9e\u73b0","text":"<p>\u66f4\u65b0\u9690\u85cf\u72b6\u6001 $$     h_t = \\phi(W_{hh} h_{t-1} + W_{hx}\\mathbf{x}{t-1}+b_h) $$ \u8f93\u51fa $$     o_t = \\phi(W{ho} h_t + b_o) $$</p> <p>\u56f0\u60d1\u5ea6(perplexity) \u8861\u91cf\u8bed\u8a00\u6a21\u578b\u7684\u597d\u574f\u53ef\u4ee5\u7528\u5e73\u5747\u4ea4\u53c9\u71b5 \\pi = \\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1), \u56f0\u60d1\u5ea6\u4e3a\\exp(\\pi)</p> <p>\u68af\u5ea6\u526a\u88c1 \u9884\u9632\u68af\u5ea6\u7206\u70b8 \u68af\u5ea6\u957f\u5ea6\u8d85\u8fc7\\theta\uff0c\u90a3\u4e48\u62d6\u56de\u957f\u5ea6\\theta $$     g \\leftarrow \\min(1,\\frac{\\theta}{\\Vert \\mathbf{g} \\Vert}) \\mathbf{g} $$</p> <p>\u53c2\u6570\u521d\u59cb\u5316:</p> <pre><code>def get_params(vocab_size, num_hiddens, device):\n    num_inputs = num_outputs = vocab_size\n    #\u56e0\u4e3a\u8fd9\u91cc\u7528\u7684onehot\u7801\n    def normal(shape):\n        return torch.randn(size=shape, device=device) * 0.01\n\n    # \u9690\u85cf\u5c42\u53c2\u6570\n    W_xh = normal((num_inputs, num_hiddens))\n    W_hh = normal((num_hiddens, num_hiddens))\n    b_h = torch.zeros(num_hiddens, device=device)\n    # \u8f93\u51fa\u5c42\u53c2\u6570\n    W_hq = normal((num_hiddens, num_outputs))\n    b_q = torch.zeros(num_outputs, device=device)\n    # \u9644\u52a0\u68af\u5ea6\n    params = [W_xh, W_hh, b_h, W_hq, b_q]\n    for param in params:\n        param.requires_grad_(True)\n    return params\n\n#\u521d\u59cb\u5316\u9690\u85cf\u72b6\u6001\ndef init_rnn_state(batch_size, num_hiddens, device):\n    return (torch.zeros((batch_size, num_hiddens), device=device), )\n</code></pre> <p>X\u7684\u5f62\u72b6\u4e3a\uff08batch_size,vocab_size\uff09\uff0c\u6267\u884c\u64cd\u4f5c $$     H_{b \\times h} = \\phi(X_{b \\times x}W_{x \\times h} + H_{b \\times h}W_{h \\times h } + b_h(Broadcast)) $$ \u53ef\u4ee5\u770b\u51fa\u5bf9\u4e8e\u6bcf\u4e2a\u6279\u91cf\u7684\u9690\u72b6\u6001\u662f\u72ec\u7acb\u5b58\u50a8\u66f4\u65b0\u7684</p> <p>\u8f93\u51faY\u7684\u5f62\u72b6\u4e3a(batch_size,vocab_size)\uff0ccat\u5b8c\u4e86return\u7684\u5f62\u72b6\u4e3a(time_steps\\timesbatch_size,vocab_size)</p> <pre><code>def rnn(inputs, state, params):\n    # inputs\u7684\u5f62\u72b6\uff1a(\u65f6\u95f4\u6b65\u6570\u91cf\uff0c\u6279\u91cf\u5927\u5c0f\uff0c\u8bcd\u8868\u5927\u5c0f)\n    W_xh, W_hh, b_h, W_hq, b_q = params\n    H, = state\n    outputs = []\n    # X\u7684\u5f62\u72b6\uff1a(\u6279\u91cf\u5927\u5c0f\uff0c\u8bcd\u8868\u5927\u5c0f)\n    for X in inputs:\n        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh)+b_h)#torch.mm\u8868\u793a\u77e9\u9635\u4e58\u8d77\u6765\n        Y = torch.mm(H, W_hq) + b_q\n        outputs.append(Y)\n    return torch.cat(outputs, dim=0), (H,)\n</code></pre> <p>\u5b8c\u6574\u7684\u5982\u4e0b</p> <pre><code>class RNNModelScratch:\n    def __init__(self, vocab_size, num_hiddens, device,\n                 get_params, init_state, forward_fn):\n        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n        self.params = get_params(vocab_size, num_hiddens, device)\n        self.init_state, self.forward_fn = init_state, forward_fn\n\n    def __call__(self, X, state):\n        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n        return self.forward_fn(X, state, self.params)\n\n    def begin_state(self, batch_size, device):\n        return self.init_state(batch_size, self.num_hiddens, device)\n\nnum_hiddens = 512\nnet = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n                      init_rnn_state, rnn)\nstate = net.begin_state(X.shape[0], d2l.try_gpu())\nY, new_state = net(X.to(d2l.try_gpu()), state)\n</code></pre> <p>\u4f7f\u7528\u7f51\u7edc\u8fdb\u884c\u9884\u6d4b\uff0c\u524d\u51e0\u4e2a\uff08prefix\uff09\u4e0d\u4ea7\u751f\u8f93\u51fa\u4f46\u662f\u66f4\u65b0\u9690\u72b6\u6001</p> <pre><code>def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n    \"\"\"\u5728prefix\u540e\u9762\u751f\u6210\u65b0\u5b57\u7b26\"\"\"\n    state = net.begin_state(batch_size=1, device=device)\n    outputs = [vocab[prefix[0]]]\n    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1)) #\u7528\u4e0a\u6b21\u8f93\u51fa\u7684\u6700\u540e\u7684\u5b57\u7b26\n    #\u4e00\u4e2a\u4e00\u4e2a\u8fdb\u53bb\u5f97\u5230\u8f93\u51fa\n    for y in prefix[1:]:  # \u9884\u70ed\u671f\n        _, state = net(get_input(), state)\n        outputs.append(vocab[y])\n    for _ in range(num_preds):  # \u9884\u6d4bnum_preds\u6b65\n        y, state = net(get_input(), state)\n        outputs.append(int(y.argmax(dim=1).reshape(1)))\n    return ''.join([vocab.idx_to_token[i] for i in outputs])\n</code></pre> <p>\u8bad\u7ec3\u793a\u4f8b</p> <pre><code>def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n    state, timer = None, d2l.Timer()\n    metric = d2l.Accumulator(2)  # \u8bad\u7ec3\u635f\u5931\u4e4b\u548c,\u8bcd\u5143\u6570\u91cf\n    for X, Y in train_iter:\n        if state is None or use_random_iter:\n            # \u5728\u7b2c\u4e00\u6b21\u8fed\u4ee3\u6216\u4f7f\u7528\u968f\u673a\u62bd\u6837\u65f6\u8981\u91cd\u65b0\u521d\u59cb\u5316state\n            state = net.begin_state(batch_size=X.shape[0], device=device)\n        else:\n            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n                # state\u5bf9\u4e8enn.GRU\u662f\u4e2a\u5f20\u91cf\n                state.detach_()\n            else:\n                for s in state:\n                    s.detach_()\n        y = Y.T.reshape(-1)\n        X, y = X.to(device), y.to(device)\n        y_hat, state = net(X, state)\n        l = loss(y_hat, y.long()).mean()\n        #\u8fd9\u91ccy_hat\u4e3a\u4e8c\u7ef4\u5f20\u91cf\uff0cy\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u7c7b\u4f3c\u4e8e\u591a\u5206\u7c7b\u95ee\u9898\n        if isinstance(updater, torch.optim.Optimizer):\n            updater.zero_grad()\n            l.backward()\n            grad_clipping(net, 1)#\u68af\u5ea6\u526a\u88c1\n            updater.step()\n        else:\n            l.backward()\n            grad_clipping(net, 1)\n            # \u56e0\u4e3a\u5df2\u7ecf\u8c03\u7528\u4e86mean\u51fd\u6570\n            updater(batch_size=1)\n        metric.add(l * y.numel(), y.numel())#numel\u51fd\u6570\u8fd4\u56de\u5f20\u91cf\u5143\u7d20\u603b\u6570\u91cf\uff0c\n    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n</code></pre> <p>\u4f7f\u7528pytorch API</p> <pre><code>rnn_layer = nn.RNN(len(vocab), num_hiddens)\nstate = torch.zeros((1, batch_size, num_hiddens))#(\u9690\u85cf\u5c42\u6570\uff0c\u6279\u91cf\u5927\u5c0f\uff0c\u9690\u53d8\u91cf\u957f\u5ea6)\nX = torch.rand(size=(num_steps, batch_size, len(vocab)))\nY, state_new = rnn_layer(X, state)\n</code></pre>"},{"location":"DeepLearning/#_14","title":"\u53cd\u5411\u4f20\u64ad","text":"<p>\u4e3b\u8981\u662f\u5faa\u73af\u8ba1\u7b97\u68af\u5ea6\u7684\u65b9\u6cd5\u548c\u8fd1\u4f3c\u65b9\u6cd5\uff0ct\u5927\u65f6\u4ea7\u751f\u68af\u5ea6\u6d88\u5931\u6216\u8005\u7206\u70b8\u7684\u95ee\u9898 \u5177\u4f53\u53c2\u8003\u52a8\u624b\u6df1\u5ea6\u5b66\u4e60bptt\u7ae0\u8282</p>"},{"location":"DeepLearning/#gru","title":"GRU","text":"<p>\u91cd\u7f6e\u95e8\u548c\u66f4\u65b0\u95e8(\\mathbb{R}_{b \\times h})\u66f4\u65b0 $$ \\begin{aligned} \\mathbf{R}t = \\sigma(\\mathbf{X}_t \\mathbf{W}{xr} + \\mathbf{H}{t-1} \\mathbf{W}{hr} + \\mathbf{b}r),\\ \\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}{xz} + \\mathbf{H}{t-1} \\mathbf{W}{hz} + \\mathbf{b}_z), \\end{aligned} $$ ($\\sigma \u4e3a $sigmoid\u51fd\u6570)</p> <p>\u9690\u5019\u9009\u72b6\u6001 \\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h),</p> <p>\u9690\u72b6\u6001\u66f4\u65b0 \\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.</p> <p>\u76f8\u6bd4\u524d\u9762\u57fa\u672cRNN\u53ea\u662f\u9690\u72b6\u6001\u66f4\u65b0\u516c\u5f0f\u66f4\u4e3a\u590d\u6742</p> <p>pytorch\u6846\u67b6</p> <pre><code>num_inputs = vocab_size\ngru_layer = nn.GRU(num_inputs, num_hiddens)\nmodel = d2l.RNNModel(gru_layer, len(vocab))\n</code></pre> <p>d2l.RNNModel</p> <pre><code>class RNNModel(nn.Module):\n    \"\"\"\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\"\"\"\n    def __init__(self, rnn_layer, vocab_size, **kwargs):\n        super(RNNModel, self).__init__(**kwargs)\n        self.rnn = rnn_layer\n        self.vocab_size = vocab_size\n        self.num_hiddens = self.rnn.hidden_size\n        # \u5982\u679cRNN\u662f\u53cc\u5411\u7684\uff08\u4e4b\u540e\u5c06\u4ecb\u7ecd\uff09\uff0cnum_directions\u5e94\u8be5\u662f2\uff0c\u5426\u5219\u5e94\u8be5\u662f1\n        if not self.rnn.bidirectional:\n            self.num_directions = 1\n            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n        else:\n            self.num_directions = 2\n            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n\n    def forward(self, inputs, state):\n        X = F.one_hot(inputs.T.long(), self.vocab_size)\n        X = X.to(torch.float32)\n        Y, state = self.rnn(X, state)\n        # \u5168\u8fde\u63a5\u5c42\u9996\u5148\u5c06Y\u7684\u5f62\u72b6\u6539\u4e3a(\u65f6\u95f4\u6b65\u6570*\u6279\u91cf\u5927\u5c0f,\u9690\u85cf\u5355\u5143\u6570)\n        # \u5b83\u7684\u8f93\u51fa\u5f62\u72b6\u662f(\u65f6\u95f4\u6b65\u6570*\u6279\u91cf\u5927\u5c0f,\u8bcd\u8868\u5927\u5c0f)\u3002\n        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n        return output, state\n\n    def begin_state(self, device, batch_size=1):\n        if not isinstance(self.rnn, nn.LSTM):\n            # nn.GRU\u4ee5\u5f20\u91cf\u4f5c\u4e3a\u9690\u72b6\u6001\n            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n                                 batch_size, self.num_hiddens),\n                                device=device)\n        else:\n            # nn.LSTM\u4ee5\u5143\u7ec4\u4f5c\u4e3a\u9690\u72b6\u6001\n            return (torch.zeros((\n                self.num_directions * self.rnn.num_layers,\n                batch_size, self.num_hiddens), device=device),\n                    torch.zeros((\n                        self.num_directions * self.rnn.num_layers,\n                        batch_size, self.num_hiddens), device=device))\n</code></pre>"},{"location":"DeepLearning/#lstm","title":"LSTM","text":"<p>\u8f93\u5165\u95e8\u662f\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}\uff0c \u9057\u5fd8\u95e8\u662f\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}\uff0c \u8f93\u51fa\u95e8\u662f\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}\u3002</p> <p>\u66f4\u65b0\u516c\u5f0f\u4e3a $$ \\begin{aligned} \\mathbf{I}t &amp;= \\sigma(\\mathbf{X}_t \\mathbf{W}{xi} + \\mathbf{H}{t-1} \\mathbf{W}{hi} + \\mathbf{b}i),\\ \\mathbf{F}_t &amp;= \\sigma(\\mathbf{X}_t \\mathbf{W}{xf} + \\mathbf{H}{t-1} \\mathbf{W}{hf} + \\mathbf{b}f),\\ \\mathbf{O}_t &amp;= \\sigma(\\mathbf{X}_t \\mathbf{W}{xo} + \\mathbf{H}{t-1} \\mathbf{W}{ho} + \\mathbf{b}_o), \\end{aligned} $$</p> <p>\u5019\u9009\u8bb0\u5fc6\u5143 \\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c),</p> <p>\u8bb0\u5fc6\u5143 \\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t.</p> <p>\u9690\u72b6\u6001 \\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t).</p> <p>pytorch\u6846\u67b6</p> <pre><code>num_inputs = vocab_size\nlstm_layer = nn.LSTM(num_inputs, num_hiddens)\nmodel = d2l.RNNModel(lstm_layer, len(vocab))\n</code></pre>"},{"location":"DeepLearning/#deep-rnn","title":"Deep RNN","text":"<p>\u8bbe\u7f6e\\mathbf{H}_t^{(0)} = \\mathbf{X}_t\uff0c</p> <p>\u7b2cl\u5c42\u7684\u9690\u72b6\u6001\u66f4\u65b0 \\mathbf{H}_t^{(l)} = \\phi_l(\\mathbf{H}_t^{(l-1)} \\mathbf{W}_{xh}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{hh}^{(l)}  + \\mathbf{b}_h^{(l)}),</p> <p>\u6700\u540e\uff0c\u8f93\u51fa\u5c42\u7684\u8ba1\u7b97\u4ec5\u57fa\u4e8e\u7b2cl\u4e2a\u9690\u85cf\u5c42\u6700\u7ec8\u7684\u9690\u72b6\u6001\uff1a</p> \\mathbf{O}_t = \\mathbf{H}_t^{(L)} \\mathbf{W}_{hq} + \\mathbf{b}_q, <p>\u4e5f\u80fd\u7528GRU\u6216\u8005LSTM\u6765\u4ee3\u66ff\u8fd9\u91cc\u7684\u9690\u72b6\u6001</p> <p>pytorch\u5b9e\u73b0\u6df1\u5ea6\u7684LSTM</p> <pre><code>lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)\nmodel = d2l.RNNModel(lstm_layer, len(vocab))\n</code></pre>"},{"location":"DeepLearning/#brnn","title":"BRNN","text":"\\begin{aligned} \\overrightarrow{\\mathbf{H}}_t &amp;= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{hh}^{(f)}  + \\mathbf{b}_h^{(f)}),\\\\ \\overleftarrow{\\mathbf{H}}_t &amp;= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{hh}^{(b)}  + \\mathbf{b}_h^{(b)}), \\end{aligned}  <p>\u6b63\u5411\u9690\u72b6\u6001\\overrightarrow{\\mathbf{H}}_t\u548c\u53cd\u5411\u9690\u72b6\u6001\\overleftarrow{\\mathbf{H}}_t\u8fde\u63a5\u8d77\u6765\uff0c\u83b7\u5f97\u9700\u8981\u9001\u5165\u8f93\u51fa\u5c42\u7684\u9690\u72b6\u6001\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}\u3002</p> <p>\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}\uff08q\u662f\u8f93\u51fa\u5355\u5143\u7684\u6570\u76ee\uff09:</p> \\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q. <p>\u8fd9\u91cc\\mathbf{W}_{hq} \\in \\mathbb{R}^{2h \\times q}</p>"},{"location":"DeepLearning/#seq2seq","title":"Seq2Seq","text":"<p>\u673a\u5668\u7ffb\u8bd1</p> <p>\u7f16\u7801\u5668\u662f\u4e00\u4e2aRNN(\u53ef\u4ee5\u662f\u53cc\u5411)\uff0c\u8bfb\u53d6\u8f93\u5165\u53e5\u5b50 \u89e3\u7801\u5668\u7528\u53e6\u4e00\u4e2aRNN\u6765\u8f93\u51fa</p> <p>\u7f16\u7801\u5668\u6700\u540e\u65f6\u95f4\u6b65\u7684\u9690\u72b6\u6001\u7528\u4f5c\u7f16\u7801\u5668\u7684\u521d\u59cb\u9690\u72b6\u6001</p> <p>\u8bad\u7ec3\u65f6\u89e3\u7801\u5668\u4f7f\u7528\u76ee\u6807\u53e5\u5b50\u4f5c\u4e3a\u8f93\u5165\uff08\u5c31\u7b97\u67d0\u4e00\u6b65\u9884\u6d4b\u9519\u4e86\uff0c\u4e0b\u4e00\u6b65\u8f93\u5165\u7684\u8fd8\u662f\u6b63\u786e\u7684\u7ffb\u8bd1\u7ed3\u679c\uff09</p>"},{"location":"DeepLearning/#_15","title":"\u9884\u6d4b\u5e8f\u5217\u8bc4\u4f30","text":"<p>\u8861\u91cf\u751f\u6210\u5e8f\u5217\u597d\u574f\u7684BLEU</p>  \\exp\\left(\\min\\left(0, 1 - \\frac{\\mathrm{len}_{\\text{label}}}{\\mathrm{len}_{\\text{pred}}}\\right)\\right) \\prod_{n=1}^k p_n^{1/2^n}, <p>p_n\u8868\u793a\u9884\u6d4b\u4e2d\u6240\u6709n_gram\uff08n\u5143\u8bed\u6cd5\uff09\u7684\u7cbe\u5ea6</p> <p>Example \u6807\u7b7e\u5e8f\u5217ABCDEF\uff0c\u9884\u6d4b\u5e8f\u5217\u4e3aABBCD p_1 = \u2158 p_2 = \u00be p_3 = \u2153 p_4 = 0</p> <pre><code>class Seq2SeqEncoder(d2l.Encoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqEncoder, self).__init__(**kwargs)\n        # \u5d4c\u5165\u5c42\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n                          dropout=dropout)\n\n    def forward(self, X, *args):\n        # \u8f93\u5165'X'\u5f62\u72b6(batch_size,num_steps) \u8f93\u51fa'X'\u7684\u5f62\u72b6\uff1a(batch_size,num_steps,embed_size)\n        X = self.embedding(X)\n        # \u5728\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\uff0c\u7b2c\u4e00\u4e2a\u8f74\u5bf9\u5e94\u4e8e\u65f6\u95f4\u6b65\n        X = X.permute(1, 0, 2)\n        # \u5982\u679c\u672a\u63d0\u53ca\u72b6\u6001\uff0c\u5219\u9ed8\u8ba4\u4e3a0\n        output, state = self.rnn(X)\n        # output\u7684\u5f62\u72b6:(num_steps,batch_size,num_hiddens)\n        # state\u7684\u5f62\u72b6:(num_layers,batch_size,num_hiddens)\n        return output, state\n</code></pre> <p>Decoder\u7684embedding\u548cEncoder\u4e0d\u540c</p> <pre><code>class Seq2SeqDecoder(d2l.Decoder):\n    \"\"\"\u7528\u4e8e\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u89e3\u7801\u5668\"\"\"\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqDecoder, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n                          dropout=dropout)\n        self.dense = nn.Linear(num_hiddens, vocab_size)\n\n    def init_state(self, enc_outputs, *args):\n        #outputs:(output,state)\n        return enc_outputs[1]\n\n    def forward(self, X, state):\n        # \u8f93\u51fa'X'\u7684\u5f62\u72b6\uff1a(batch_size,num_steps,embed_size)\n        X = self.embedding(X).permute(1, 0, 2)\n        # \u5e7f\u64adcontext\uff0c\u4f7f\u5176\u5177\u6709\u4e0eX\u76f8\u540c\u7684num_steps\n        context = state[-1].repeat(X.shape[0], 1, 1)#\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u6700\u540e\u4e00\u5c42\u8f93\u51fa\n        X_and_context = torch.cat((X, context), 2)\n        output, state = self.rnn(X_and_context, state)\n        output = self.dense(output).permute(1, 0, 2)\n        # output\u7684\u5f62\u72b6:(batch_size,num_steps,vocab_size)\n        # state\u7684\u5f62\u72b6:(num_layers,batch_size,num_hiddens)\n        return output, state\n</code></pre> <p>\u5728\u5e8f\u5217\u4e2d\u5c4f\u853d\u4e0d\u76f8\u5173\u7684\u9879</p> <pre><code>def sequence_mask(X, valid_len, value=0):\n    maxlen = X.size(1)\n    mask = torch.arange((maxlen), dtype=torch.float32,\n                        device=X.device)[None, :] &lt; valid_len[:, None]\n    X[~mask] = value\n    return X\n\nX = torch.tensor([[1, 2, 3], [4, 5, 6]])\nsequence_mask(X, torch.tensor([1, 2]))\n</code></pre> <p>tensor([ [1, 0, 0],         [4, 5, 0] ])</p> <pre><code>class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n    \"\"\"\u5e26\u906e\u853d\u7684softmax\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\"\"\"\n    # pred\u7684\u5f62\u72b6\uff1a(batch_size,num_steps,vocab_size)\n    # label\u7684\u5f62\u72b6\uff1a(batch_size,num_steps)\n    # valid_len\u7684\u5f62\u72b6\uff1a(batch_size,)\n    def forward(self, pred, label, valid_len):\n        weights = torch.ones_like(label)\n        weights = sequence_mask(weights, valid_len)\n        self.reduction='none'\n        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n            pred.permute(0, 2, 1), label)\n        weighted_loss = (unweighted_loss * weights).mean(dim=1) #\u65e0\u6548\u7684\u5168\u90e8\u7f6e\u4e3a0\uff0c\u5bf9\u6bcf\u4e2a\u6837\u672c\u8fd4\u56deloss\n        return weighted_loss\n</code></pre> <p>\uff08\u5177\u4f53\u6ca1\u770b\u5f88\u61c2\u600e\u4e48\u7ec3\u7684\uff0c\u5148\u8fc7\u4e86\u540e\u9762\u8865</p>"},{"location":"DeepLearning/#attention","title":"Attention","text":"<p>\u6bcf\u4e2a\u503c\uff08Value\uff09\u548c\u4e00\u4e2a\u952e\uff08key\uff09\u4e00\u4e00\u5bf9\u5e94\uff0c\u901a\u8fc7\u67e5\u8be2\uff08query\uff09\u4e0e\u952e\u8fdb\u884c\u5339\u914d\uff0c\u5f97\u5230\u6700\u5339\u914d\u7684\u503c\u3002</p>"},{"location":"DeepLearning/#nadaraya-waston","title":"Nadaraya-Waston \u6838\u56de\u5f52","text":"f(x) = \\sum_{i=1}^n \\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)} y_i <p>\u66f4\u4e00\u822c\u7684\u8868\u793a</p> f(x) = \\sum_{i=1}^n \\alpha(x, x_i) y_i, <p>\u7528\u9ad8\u65af\u6838K(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{u^2}{2}).\u5e26\u5165\u7b2c\u4e00\u4e2a\u5f0f\u5b50</p> <p>\u5f97\u5230 \\begin{aligned} f(x) &amp;= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}(x - x_i)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}(x - x_j)^2\\right)} y_i \\\\&amp;= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}(x - x_i)^2\\right) y_i. \\end{aligned}</p> <p>\u5e26\u5b66\u4e60\u53c2\u6570w</p> \\begin{aligned}f(x) &amp;= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}((x - x_i)w)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}((x - x_j)w)^2\\right)} y_i \\\\&amp;= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}((x - x_i)w)^2\\right) y_i.\\end{aligned}"},{"location":"DeepLearning/#_16","title":"\u6ce8\u610f\u529b\u8bc4\u5206\u51fd\u6570","text":"<p>\u7528\u6570\u5b66\u8bed\u8a00\u63cf\u8ff0\uff0c\u5047\u8bbe\u6709\u4e00\u4e2a\u67e5\u8be2\\mathbf{q} \\in \\mathbb{R}^q\u548cm\u4e2a\u201c\u952e\uff0d\u503c\u201d\u5bf9(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)\uff0c \u5176\u4e2d\\mathbf{k}_i \\in \\mathbb{R}^k\uff0c\\mathbf{v}_i \\in \\mathbb{R}^v\u3002 \u6ce8\u610f\u529b\u6c47\u805a\u51fd\u6570f\u5c31\u88ab\u8868\u793a\u6210\u503c\u7684\u52a0\u6743\u548c\uff1a</p> f(\\mathbf{q}, (\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)) = \\sum_{i=1}^m \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i \\in \\mathbb{R}^v, <p>\u5176\u4e2d\u67e5\u8be2\\mathbf{q}\u548c\u952e\\mathbf{k}_i\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff08\u6807\u91cf\uff09\u662f\u901a\u8fc7\u6ce8\u610f\u529b\u8bc4\u5206\u51fd\u6570a\u5c06\u4e24\u4e2a\u5411\u91cf\u6620\u5c04\u6210\u6807\u91cf\uff0c\u518d\u7ecf\u8fc7softmax\u8fd0\u7b97\u5f97\u5230\u7684\uff1a</p> \\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}_i))}{\\sum_{j=1}^m \\exp(a(\\mathbf{q}, \\mathbf{k}_j))} \\in \\mathbb{R}."},{"location":"DeepLearning/#additive-attention","title":"Additive Attention","text":"a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R}, <p>\u5176\u4e2d\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u662f\\mathbf W_q\\in\\mathbb R^{h\\times q}\u3001\\mathbf W_k\\in\\mathbb R^{h\\times k}\u548c\\mathbf w_v\\in\\mathbb R^{h}\u3002</p> <pre><code>class AdditiveAttention(nn.Module):\n    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n        super(AdditiveAttention, self).__init__(**kwargs)\n        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, queries, keys, values, valid_lens):\n        queries, keys = self.W_q(queries), self.W_k(keys)\n        # \u5728\u7ef4\u5ea6\u6269\u5c55\u540e\uff0c\n        # queries\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570\uff0c1\uff0cnum_hidden)\n        # key\u7684\u5f62\u72b6\uff1a(batch_size\uff0c1\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0cnum_hiddens)\n        # \u4f7f\u7528\u5e7f\u64ad\u65b9\u5f0f\u8fdb\u884c\u6c42\u548c\n        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n        features = torch.tanh(features)\n        # self.w_v\u4ec5\u6709\u4e00\u4e2a\u8f93\u51fa\uff0c\u56e0\u6b64\u4ece\u5f62\u72b6\u4e2d\u79fb\u9664\u6700\u540e\u90a3\u4e2a\u7ef4\u5ea6\u3002\n        # scores\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570\uff0c\u201c\u952e-\u503c\u201d\u5bf9\u7684\u4e2a\u6570)\n        scores = self.w_v(features).squeeze(-1)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        # values\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0c\u503c\u7684\u7ef4\u5ea6)\n        # \u8f93\u51fa\u7684\u5f62\u72b6\uff1a(batch_size,\u67e5\u8be2\u7684\u4e2a\u6570,\u503c\u7684\u7ef4\u5ea6)\n        return torch.bmm(self.dropout(self.attention_weights), values)\n</code></pre>"},{"location":"DeepLearning/#scaled-dot-product-attention","title":"Scaled dot-product attention","text":"<p>\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\uff08scaled dot-product attention\uff09\u8981\u6c42query\u548cKey\u957f\u5ea6\u76f8\u540c\uff0c\u8bc4\u5206\u51fd\u6570\u4e3a\uff1a</p> a(\\mathbf q, \\mathbf k) = \\mathbf{q}^\\top \\mathbf{k}  /\\sqrt{d}. <p>\u57fa\u4e8en\u4e2a\u67e5\u8be2\u548cm\u4e2a\u952e\uff0d\u503c\u5bf9\u8ba1\u7b97\u6ce8\u610f\u529b\uff0c\u5176\u4e2d\u67e5\u8be2\u548c\u952e\u7684\u957f\u5ea6\u5747\u4e3ad\uff0c\u503c\u7684\u957f\u5ea6\u4e3av\u3002\u67e5\u8be2\\mathbf Q\\in\\mathbb R^{n\\times d}\u3001\u952e\\mathbf K\\in\\mathbb R^{m\\times d}\u548c\u503c\\mathbf V\\in\\mathbb R^{m\\times v}\u7684\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u662f\uff1a</p>  \\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}. <pre><code>class DotProductAttention(nn.Module):\n    \"\"\"\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\"\"\"\n    def __init__(self, dropout, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    # queries\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570\uff0cd)\n    # keys\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0cd)\n    # values\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0c\u503c\u7684\u7ef4\u5ea6)\n    # valid_lens\u7684\u5f62\u72b6:(batch_size\uff0c)\u6216\u8005(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570)\n    def forward(self, queries, keys, values, valid_lens=None):\n        d = queries.shape[-1]\n        # \u8bbe\u7f6etranspose_b=True\u4e3a\u4e86\u4ea4\u6362keys\u7684\u6700\u540e\u4e24\u4e2a\u7ef4\u5ea6\n        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        return torch.bmm(self.dropout(self.attention_weights), values)\n</code></pre>"},{"location":"DeepLearning/#attention-decoder","title":"Attention Decoder","text":"<p>\u4e0e\u4e4b\u524dSeq2Seq\u76f8\u6bd4\uff0c\u4e0a\u4e0b\u6587\u53d8\u91cf\\mathbf{c}\u5728\u4efb\u4f55\u89e3\u7801\u65f6\u95f4\u6b65t'\u90fd\u4f1a\u88ab\\mathbf{c}_{t'}\u66ff\u6362\u3002\u5047\u8bbe\u8f93\u5165\u5e8f\u5217\u4e2d\u6709T\u4e2a\u8bcd\u5143\uff0c\u89e3\u7801\u65f6\u95f4\u6b65t'\u7684\u4e0a\u4e0b\u6587\u53d8\u91cf\u662f\u6ce8\u610f\u529b\u96c6\u4e2d\u7684\u8f93\u51fa\uff1a</p> \\mathbf{c}_{t'} = \\sum_{t=1}^T \\alpha(\\mathbf{s}_{t' - 1}, \\mathbf{h}_t) \\mathbf{h}_t <p>\u5176\u4e2d\uff0c\u65f6\u95f4\u6b65t' - 1\u65f6\u7684\u89e3\u7801\u5668\u9690\u72b6\u6001\\mathbf{s}_{t' - 1}\u662f\u67e5\u8be2\uff0c\u7f16\u7801\u5668\u9690\u72b6\u6001\\mathbf{h}_t\u65e2\u662f\u952e\uff0c\u4e5f\u662f\u503c\u3002</p> <pre><code>class Seq2SeqAttentionDecoder(AttentionDecoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n        self.attention = d2l.AdditiveAttention(\n            num_hiddens, num_hiddens, num_hiddens, dropout)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.GRU(\n            embed_size + num_hiddens, num_hiddens, num_layers,\n            dropout=dropout)\n        self.dense = nn.Linear(num_hiddens, vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_lens, *args):\n        # outputs\u7684\u5f62\u72b6\u4e3a(batch_size\uff0cnum_steps\uff0cnum_hiddens).\n        # hidden_state\u7684\u5f62\u72b6\u4e3a(num_layers\uff0cbatch_size\uff0cnum_hiddens)\n        outputs, hidden_state = enc_outputs\n        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n\n    def forward(self, X, state):\n        # enc_outputs\u7684\u5f62\u72b6\u4e3a(batch_size,num_steps,num_hiddens).\n        # hidden_state\u7684\u5f62\u72b6\u4e3a(num_layers,batch_size,\n        # num_hiddens)\n        enc_outputs, hidden_state, enc_valid_lens = state\n        # \u8f93\u51faX\u7684\u5f62\u72b6\u4e3a(num_steps,batch_size,embed_size)\n        X = self.embedding(X).permute(1, 0, 2)\n        outputs, self._attention_weights = [], []\n        for x in X:\n            # query\u7684\u5f62\u72b6\u4e3a(batch_size,1,num_hiddens)\n            query = torch.unsqueeze(hidden_state[-1], dim=1)\n            # context\u7684\u5f62\u72b6\u4e3a(batch_size,1,num_hiddens)\n            context = self.attention(\n                query, enc_outputs, enc_outputs, enc_valid_lens)\n            # \u5728\u7279\u5f81\u7ef4\u5ea6\u4e0a\u8fde\u7ed3\n            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n            # \u5c06x\u53d8\u5f62\u4e3a(1,batch_size,embed_size+num_hiddens)\n            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n            outputs.append(out)\n            self._attention_weights.append(self.attention.attention_weights)\n        # \u5168\u8fde\u63a5\u5c42\u53d8\u6362\u540e\uff0coutputs\u7684\u5f62\u72b6\u4e3a\n        # (num_steps,batch_size,vocab_size)\n        outputs = self.dense(torch.cat(outputs, dim=0))\n        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,\n                                          enc_valid_lens]\n\n    @property\n    def attention_weights(self):\n        return self._attention_weights\n</code></pre>"},{"location":"DeepLearning/#mutihead-attention","title":"Mutihead-Attention","text":"<p>\u7ed9\u5b9a\u67e5\u8be2\\mathbf{q} \\in \\mathbb{R}^{d_q}\u3001\u952e\\mathbf{k} \\in \\mathbb{R}^{d_k}\u548c\u503c\\mathbf{v} \\in \\mathbb{R}^{d_v}\uff0c\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\\mathbf{h}_i\uff08i = 1, \\ldots, h\uff09\u7684\u8ba1\u7b97\u65b9\u6cd5\u4e3a\uff1a</p> \\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v}, <p>\u5176\u4e2d\uff0c\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u5305\u62ec\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}\u3001\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}\u548c \\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}\uff0c\u4ee5\u53ca\u4ee3\u8868\u6ce8\u610f\u529b\u6c47\u805a\u7684\u51fd\u6570f\u3002 \u591a\u5934\u6ce8\u610f\u529b\u7684\u8f93\u51fa\u9700\u8981\u7ecf\u8fc7\u53e6\u4e00\u4e2a\u7ebf\u6027\u8f6c\u6362\uff0c\u5b83\u5bf9\u5e94\u7740h\u4e2a\u5934\u8fde\u7ed3\u540e\u7684\u7ed3\u679c\uff0c\u53ef\u5b66\u4e60\u53c2\u6570\u662f\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}\uff1a</p> \\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}. <p>\u5728\u5b9e\u73b0\u8fc7\u7a0b\u4e2d\u901a\u5e38\u9009\u62e9\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u4f5c\u4e3a\u6bcf\u4e00\u4e2a\u6ce8\u610f\u529b\u5934\u3002 \u8bbe\u5b9ap_q = p_k = p_v = p_o / h\u3002\u5982\u679c\u5c06\u67e5\u8be2\u3001\u952e\u548c\u503c\u7684\u7ebf\u6027\u53d8\u6362\u7684\u8f93\u51fa\u6570\u91cf\u8bbe\u7f6e\u4e3ap_q h = p_k h = p_v h = p_o\uff0c\u5219\u53ef\u4ee5\u5e76\u884c\u8ba1\u7b97h\u4e2a\u5934\u3002</p>"},{"location":"DeepLearning/#self-attention","title":"Self-Attention","text":"<p>\u7ed9\u5b9a\u4e00\u4e2a\u7531\u8bcd\u5143\u7ec4\u6210\u7684\u8f93\u5165\u5e8f\u5217\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\uff0c\u5176\u4e2d\u4efb\u610f\\mathbf{x}_i \\in \\mathbb{R}^d\uff081 \\leq i \\leq n\uff09\u3002\u8be5\u5e8f\u5217\u7684\u81ea\u6ce8\u610f\u529b\u8f93\u51fa\u4e3a\u4e00\u4e2a\u957f\u5ea6\u76f8\u540c\u7684\u5e8f\u5217 \\mathbf{y}_1, \\ldots, \\mathbf{y}_n\uff0c\u5176\u4e2d\uff1a</p> \\mathbf{y}_i = f(\\mathbf{x}_i, (\\mathbf{x}_1, \\mathbf{x}_1), \\ldots, (\\mathbf{x}_n, \\mathbf{x}_n)) \\in \\mathbb{R}^d"},{"location":"DeepLearning/#positional-encoding","title":"Positional-Encoding","text":"<p>\u7531\u4e8eSelf Attention \u6ca1\u6709\u8bb0\u5f55\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5047\u8bbe\u957f\u5ea6\u4e3an\u7684\u5e8f\u5217\\mathbf{X} \\in \\mathbb{R}^{n \\times d},\u7528\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635\\mathbf{P} \\in \\mathbb{R}^{n \\times d} \u6765\u8ba1\u7b97\\mathbf{X} + \\mathbf{P} \u4f5c\u4e3aSelf Attention \u7684\u8f93\u5165\u3002</p> <p>\u77e9\u9635\\mathbf{P}\u7b2ci\u884c\u3001\u7b2c2j\u5217\u548c2j+1\u5217\u4e0a\u7684\u5143\u7d20\u4e3a\uff1a</p> \\begin{aligned} p_{i, 2j} &amp;= \\sin\\left(\\frac{i}{10000^{2j/d}}\\right),\\\\p_{i, 2j+1} &amp;= \\cos\\left(\\frac{i}{10000^{2j/d}}\\right).\\end{aligned} <p>\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f (p_{i, 2j}, p_{i, 2j+1})\u90fd\u53ef\u4ee5\u7ebf\u6027\u6295\u5f71\u5230(p_{i+\\delta, 2j}, p_{i+\\delta, 2j+1})\uff1a</p> \\begin{aligned} &amp;\\begin{bmatrix} \\cos(\\delta \\omega_j) &amp; \\sin(\\delta \\omega_j) \\\\  -\\sin(\\delta \\omega_j) &amp; \\cos(\\delta \\omega_j) \\\\ \\end{bmatrix} \\begin{bmatrix} p_{i, 2j} \\\\  p_{i, 2j+1} \\\\ \\end{bmatrix}\\\\ =&amp;\\begin{bmatrix} \\cos(\\delta \\omega_j) \\sin(i \\omega_j) + \\sin(\\delta \\omega_j) \\cos(i \\omega_j) \\\\  -\\sin(\\delta \\omega_j) \\sin(i \\omega_j) + \\cos(\\delta \\omega_j) \\cos(i \\omega_j) \\\\ \\end{bmatrix}\\\\ =&amp;\\begin{bmatrix} \\sin\\left((i+\\delta) \\omega_j\\right) \\\\  \\cos\\left((i+\\delta) \\omega_j\\right) \\\\ \\end{bmatrix}\\\\ =&amp; \\begin{bmatrix} p_{i+\\delta, 2j} \\\\  p_{i+\\delta, 2j+1} \\\\ \\end{bmatrix}, \\end{aligned} <p>\u56e0\u6b64\u8fd9\u4e2a\u77e9\u9635\u4e0d\u4f9d\u8d56\u4e8e\u4efb\u4f55\u4f4d\u7f6e\u7684\u7d22\u5f15\u3002</p>"},{"location":"DeepLearning/#transformer","title":"Transformer","text":""},{"location":"DeepLearning/#addnorm","title":"Add&amp;Norm","text":"<p>\u4f7f\u7528LayerNorm \uff08\u4e0d\u6539\u53d8\u8bed\u4e49\u5411\u91cf\u7684\u65b9\u5411\uff0c\u6539\u53d8\u6a21\u957f\uff09</p> <pre><code>class AddNorm(nn.Module):\n    def __init__(self, normalized_shape, dropout, **kwargs):\n        super(AddNorm, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.ln = nn.LayerNorm(normalized_shape)\n\n    def forward(self, X, Y):\n        return self.ln(self.dropout(Y) + X)\n</code></pre>"},{"location":"DeepLearning/#encoder","title":"Encoder","text":"<pre><code>class EncoderBlock(nn.Module):\n    def __init__(self, key_size, query_size, value_size, num_hiddens,\n                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n                 dropout, use_bias=False, **kwargs):\n        super(EncoderBlock, self).__init__(**kwargs)\n        self.attention = d2l.MultiHeadAttention(\n            key_size, query_size, value_size, num_hiddens, num_heads, dropout,\n            use_bias)\n        self.addnorm1 = AddNorm(norm_shape, dropout)\n        self.ffn = PositionWiseFFN(\n            ffn_num_input, ffn_num_hiddens, num_hiddens)\n        self.addnorm2 = AddNorm(norm_shape, dropout)\n\n    def forward(self, X, valid_lens):\n        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n        return self.addnorm2(Y, self.ffn(Y))\n</code></pre> <p>Encoder\u5c42\u90fd\u4e0d\u4f1a\u6539\u53d8\u8f93\u5165\u7684\u5f62\u72b6\u3002</p> <pre><code>class TransformerEncoder(d2l.Encoder):\n    def __init__(self, vocab_size, key_size, query_size, value_size,\n                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n                 num_heads, num_layers, dropout, use_bias=False, **kwargs):\n        super(TransformerEncoder, self).__init__(**kwargs)\n        self.num_hiddens = num_hiddens\n        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n        self.blks = nn.Sequential()\n        for i in range(num_layers):\n            self.blks.add_module(\"block\"+str(i),\n                EncoderBlock(key_size, query_size, value_size, num_hiddens,\n                             norm_shape, ffn_num_input, ffn_num_hiddens,\n                             num_heads, dropout, use_bias))\n\n    def forward(self, X, valid_lens, *args):\n        # \u56e0\u4e3a\u4f4d\u7f6e\u7f16\u7801\u503c\u5728-1\u548c1\u4e4b\u95f4\uff0c\n        # \u56e0\u6b64\u5d4c\u5165\u503c\u4e58\u4ee5\u5d4c\u5165\u7ef4\u5ea6\u7684\u5e73\u65b9\u6839\u8fdb\u884c\u7f29\u653e\uff0c\n        # \u7136\u540e\u518d\u4e0e\u4f4d\u7f6e\u7f16\u7801\u76f8\u52a0\u3002\n        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n        self.attention_weights = [None] * len(self.blks)\n        for i, blk in enumerate(self.blks):\n            X = blk(X, valid_lens)\n            self.attention_weights[\n                i] = blk.attention.attention.attention_weights\n        return X\n</code></pre>"},{"location":"DeepLearning/#decoder","title":"Decoder","text":"<p>\u5728\u63a9\u853d\u591a\u5934\u89e3\u7801\u5668\u81ea\u6ce8\u610f\u529b\u5c42\uff08\u7b2c\u4e00\u4e2a\u5b50\u5c42\uff09\u4e2d\uff0c\u67e5\u8be2\u3001\u952e\u548c\u503c\u90fd\u6765\u81ea\u4e0a\u4e00\u4e2a\u89e3\u7801\u5668\u5c42\u7684\u8f93\u51fa\u3002\u4e3a\u4e86\u5728\u89e3\u7801\u5668\u4e2d\u4fdd\u7559\u81ea\u56de\u5f52\u7684\u5c5e\u6027\uff0c\u5176\u63a9\u853d\u81ea\u6ce8\u610f\u529b\u8bbe\u5b9a\u4e86\u53c2\u6570\uff0c\u4ee5\u4fbf\u4efb\u4f55\u67e5\u8be2\u90fd\u53ea\u4f1a\u4e0e\u89e3\u7801\u5668\u4e2d\u6240\u6709\u5df2\u7ecf\u751f\u6210\u8bcd\u5143\u7684\u4f4d\u7f6e\uff08\u5373\u76f4\u5230\u8be5\u67e5\u8be2\u4f4d\u7f6e\u4e3a\u6b62\uff09\u8fdb\u884c\u6ce8\u610f\u529b\u8ba1\u7b97\u3002</p> <pre><code>class DecoderBlock(nn.Module):\n    def __init__(self, key_size, query_size, value_size, num_hiddens,\n                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n                 dropout, i, **kwargs):\n        super(DecoderBlock, self).__init__(**kwargs)\n        self.i = i\n        self.attention1 = d2l.MultiHeadAttention(\n            key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n        self.addnorm1 = AddNorm(norm_shape, dropout)\n        self.attention2 = d2l.MultiHeadAttention(\n            key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n        self.addnorm2 = AddNorm(norm_shape, dropout)\n        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,\n                                   num_hiddens)\n        self.addnorm3 = AddNorm(norm_shape, dropout)\n\n    def forward(self, X, state):\n        enc_outputs, enc_valid_lens = state[0], state[1]\n        # \u8bad\u7ec3\u9636\u6bb5\uff0c\u8f93\u51fa\u5e8f\u5217\u7684\u6240\u6709\u8bcd\u5143\u90fd\u5728\u540c\u4e00\u65f6\u95f4\u5904\u7406\uff0c\n        # \u56e0\u6b64state[2][self.i]\u521d\u59cb\u5316\u4e3aNone\u3002\n        # \u9884\u6d4b\u9636\u6bb5\uff0c\u8f93\u51fa\u5e8f\u5217\u662f\u901a\u8fc7\u8bcd\u5143\u4e00\u4e2a\u63a5\u7740\u4e00\u4e2a\u89e3\u7801\u7684\uff0c\n        # \u56e0\u6b64state[2][self.i]\u5305\u542b\u7740\u76f4\u5230\u5f53\u524d\u65f6\u95f4\u6b65\u7b2ci\u4e2a\u5757\u89e3\u7801\u7684\u8f93\u51fa\u8868\u793a\n        if state[2][self.i] is None:\n            key_values = X\n        else:\n            key_values = torch.cat((state[2][self.i], X), axis=1)\n        state[2][self.i] = key_values\n        if self.training:\n            batch_size, num_steps, _ = X.shape\n            # dec_valid_lens\u7684\u5f00\u5934:(batch_size,num_steps),\n            # \u5176\u4e2d\u6bcf\u4e00\u884c\u662f[1,2,...,num_steps]\n            dec_valid_lens = torch.arange(\n                1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n        else:\n            dec_valid_lens = None\n\n        # \u81ea\u6ce8\u610f\u529b\n        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n        Y = self.addnorm1(X, X2)\n        # \u7f16\u7801\u5668\uff0d\u89e3\u7801\u5668\u6ce8\u610f\u529b\u3002\n        # enc_outputs\u7684\u5f00\u5934:(batch_size,num_steps,num_hiddens)\n        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n        Z = self.addnorm2(Y, Y2)\n        return self.addnorm3(Z, self.ffn(Z)), state\n</code></pre> <pre><code>class TransformerDecoder(d2l.AttentionDecoder):\n    def __init__(self, vocab_size, key_size, query_size, value_size,\n                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n                 num_heads, num_layers, dropout, **kwargs):\n        super(TransformerDecoder, self).__init__(**kwargs)\n        self.num_hiddens = num_hiddens\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n        self.blks = nn.Sequential()\n        for i in range(num_layers):\n            self.blks.add_module(\"block\"+str(i),\n                DecoderBlock(key_size, query_size, value_size, num_hiddens,\n                             norm_shape, ffn_num_input, ffn_num_hiddens,\n                             num_heads, dropout, i))\n        self.dense = nn.Linear(num_hiddens, vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_lens, *args):\n        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n\n    def forward(self, X, state):\n        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n        for i, blk in enumerate(self.blks):\n            X, state = blk(X, state)\n            # \u89e3\u7801\u5668\u81ea\u6ce8\u610f\u529b\u6743\u91cd\n            self._attention_weights[0][\n                i] = blk.attention1.attention.attention_weights\n            # \u201c\u7f16\u7801\u5668\uff0d\u89e3\u7801\u5668\u201d\u81ea\u6ce8\u610f\u529b\u6743\u91cd\n            self._attention_weights[1][\n                i] = blk.attention2.attention.attention_weights\n        return self.dense(X), state\n\n    @property\n    def attention_weights(self):\n        return self._attention_weights\n</code></pre>"},{"location":"DeepLearning/#multiple-gpus","title":"Multiple GPUs","text":""},{"location":"DeepLearning/#_17","title":"\u62c6\u5206\u6570\u636e","text":"<p>\u8fd9\u79cd\u65b9\u5f0f\u4e0b\uff0c\u6240\u6709GPU\u5c3d\u7ba1\u6709\u4e0d\u540c\u7684\u89c2\u6d4b\u7ed3\u679c\uff0c\u4f46\u662f\u6267\u884c\u7740\u76f8\u540c\u7c7b\u578b\u7684\u5de5\u4f5c\u3002\u5728\u5b8c\u6210\u6bcf\u4e2a\u5c0f\u6279\u91cf\u6570\u636e\u7684\u8bad\u7ec3\u4e4b\u540e\uff0c\u68af\u5ea6\u5728GPU\u4e0a\u805a\u5408\u3002 GPU\u7684\u6570\u91cf\u8d8a\u591a\uff0c\u5c0f\u6279\u91cf\u5305\u542b\u7684\u6570\u636e\u91cf\u5c31\u8d8a\u5927\uff0c\u4ece\u800c\u5c31\u80fd\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002 \u7f3a\u70b9\uff1a\u4e0d\u80fd\u591f\u8bad\u7ec3\u66f4\u5927\u7684\u6a21\u578b</p> <p>k\u4e2aGPU\u5e76\u884c\u8bad\u7ec3\u8fc7\u7a0b\u5982\u4e0b\uff1a *\u5728\u4efb\u4f55\u4e00\u6b21\u8bad\u7ec3\u8fed\u4ee3\u4e2d\uff0c\u7ed9\u5b9a\u7684\u968f\u673a\u7684\u5c0f\u6279\u91cf\u6837\u672c\u90fd\u5c06\u88ab\u5206\u6210k\u4e2a\u90e8\u5206\uff0c\u5e76\u5747\u5300\u5730\u5206\u914d\u5230GPU\u4e0a\uff1b *\u6bcf\u4e2aGPU\u6839\u636e\u5206\u914d\u7ed9\u5b83\u7684\u5c0f\u6279\u91cf\u5b50\u96c6\uff0c\u8ba1\u7b97\u6a21\u578b\u53c2\u6570\u7684\u635f\u5931\u548c\u68af\u5ea6\uff1b *\u5c06k\u4e2aGPU\u4e2d\u7684\u5c40\u90e8\u68af\u5ea6\u805a\u5408\uff0c\u4ee5\u83b7\u5f97\u5f53\u524d\u5c0f\u6279\u91cf\u7684\u968f\u673a\u68af\u5ea6\uff1b *\u805a\u5408\u68af\u5ea6\u88ab\u91cd\u65b0\u5206\u53d1\u5230\u6bcf\u4e2aGPU\u4e2d\uff1b *\u6bcf\u4e2aGPU\u4f7f\u7528\u8fd9\u4e2a\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\uff0c\u6765\u66f4\u65b0\u5b83\u6240\u7ef4\u62a4\u7684\u5b8c\u6574\u7684\u6a21\u578b\u53c2\u6570\u96c6\u3002</p> <p>\u5411\u591a\u4e2a\u8bbe\u5907\u590d\u5236\u53c2\u6570</p> <pre><code>def get_params(params, device):\n    new_params = [p.to(device) for p in params]\n    for p in new_params:\n        p.requires_grad_()\n    return new_params\n</code></pre> <p>\u80fd\u591f\u5c06\u6240\u6709\u8bbe\u5907\u4e0a\u7684\u68af\u5ea6\u8fdb\u884c\u76f8\u52a0</p> <pre><code>def allreduce(data):\n    for i in range(1, len(data)):\n        data[0][:] += data[i].to(data[0].device)\n    for i in range(1, len(data)):\n        data[i][:] = data[0].to(data[i].device)\n</code></pre> <p>\u5206\u53d1\u6570\u636e <code>nn.parallel.scatter(data, devices)</code></p> <pre><code>data = torch.arange(20).reshape(4, 5)\ndevices = [torch.device('cuda:0'), torch.device('cuda:1')]\nsplit = nn.parallel.scatter(data, devices)\nprint('input :', data)\nprint('load into', devices)\nprint('output:', split)\n</code></pre> <p>input : tensor([ [ 0,  1,  2,  3,  4],         [ 5,  6,  7,  8,  9],         [10, 11, 12, 13, 14],         [15, 16, 17, 18, 19] ]) load into [device(type='cuda', index=0), device(type='cuda', index=1)] output: (tensor([ [0, 1, 2, 3, 4],         [5, 6, 7, 8, 9] ], device='cuda:0'), tensor([ [10, 11, 12, 13, 14],         [15, 16, 17, 18, 19] ], device='cuda:1'))</p> <p>\u5206\u53d1\u6570\u636e\u548c\u6807\u7b7e</p> <pre><code>def split_batch(X, y, devices):\n    assert X.shape[0] == y.shape[0]\n    return (nn.parallel.scatter(X, devices),\n            nn.parallel.scatter(y, devices))\n</code></pre> <p>\u5b9e\u73b0\u591aGPU\u8bad\u7ec3</p> <pre><code>def train_batch(X, y, device_params, devices, lr):\n    X_shards, y_shards = split_batch(X, y, devices)\n    # \u5728\u6bcf\u4e2aGPU\u4e0a\u5206\u522b\u8ba1\u7b97\u635f\u5931\n    ls = [loss(lenet(X_shard, device_W), y_shard).sum()\n          for X_shard, y_shard, device_W in zip(\n              X_shards, y_shards, device_params)]\n    for l in ls:  # \u53cd\u5411\u4f20\u64ad\u5728\u6bcf\u4e2aGPU\u4e0a\u5206\u522b\u6267\u884c\n        l.backward()\n    # \u5c06\u6bcf\u4e2aGPU\u7684\u6240\u6709\u68af\u5ea6\u76f8\u52a0\uff0c\u5e76\u5c06\u5176\u5e7f\u64ad\u5230\u6240\u6709GPU\n    with torch.no_grad():\n        for i in range(len(device_params[0])):\n            allreduce([device_params[c][i].grad for c in range(len(devices))])\n    # \u5728\u6bcf\u4e2aGPU\u4e0a\u5206\u522b\u66f4\u65b0\u6a21\u578b\u53c2\u6570\n    for param in device_params:\n        d2l.sgd(param, lr, X.shape[0]) # \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4f7f\u7528\u5168\u5c3a\u5bf8\u7684\u5c0f\u6279\u91cf\n</code></pre> <p>\u8bc4\u4f30\u6a21\u578b\u7684\u65f6\u5019\u53ea\u5728\u4e00\u4e2aGPU\u4e0a</p> <pre><code>def train(num_gpus, batch_size, lr):\n    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n    # \u5c06\u6a21\u578b\u53c2\u6570\u590d\u5236\u5230num_gpus\u4e2aGPU\n    device_params = [get_params(params, d) for d in devices]\n    num_epochs = 10\n    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n    timer = d2l.Timer()\n    for epoch in range(num_epochs):\n        timer.start()\n        for X, y in train_iter:\n            # \u4e3a\u5355\u4e2a\u5c0f\u6279\u91cf\u6267\u884c\u591aGPU\u8bad\u7ec3\n            train_batch(X, y, device_params, devices, lr)\n            torch.cuda.synchronize()\n        timer.stop()\n        # \u5728GPU0\u4e0a\u8bc4\u4f30\u6a21\u578b\n        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(\n            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))\n    print(f'\u6d4b\u8bd5\u7cbe\u5ea6\uff1a{animator.Y[0][-1]:.2f}\uff0c{timer.avg():.1f}\u79d2/\u8f6e\uff0c'\n          f'\u5728{str(devices)}')\n</code></pre> <p>\u7b80\u6d01\u5b9e\u73b0<code>net = nn.DataParallel(net, device_ids=devices)</code>\uff0c\u548c\u4e4b\u524d\u51e0\u4e4e\u6ca1\u4ec0\u4e48\u533a\u522b</p> <pre><code>def train(net, num_gpus, batch_size, lr):\n    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n    def init_weights(m):\n        if type(m) in [nn.Linear, nn.Conv2d]:\n            nn.init.normal_(m.weight, std=0.01)\n    net.apply(init_weights)\n    # \u5728\u591a\u4e2aGPU\u4e0a\u8bbe\u7f6e\u6a21\u578b\n    net = nn.DataParallel(net, device_ids=devices)\n    trainer = torch.optim.SGD(net.parameters(), lr)\n    loss = nn.CrossEntropyLoss()\n    timer, num_epochs = d2l.Timer(), 10\n    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n    for epoch in range(num_epochs):\n        net.train()\n        timer.start()\n        for X, y in train_iter:\n            trainer.zero_grad()\n            X, y = X.to(devices[0]), y.to(devices[0])\n            l = loss(net(X), y)\n            l.backward()\n            trainer.step()\n        timer.stop()\n        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))\n    print(f'\u6d4b\u8bd5\u7cbe\u5ea6\uff1a{animator.Y[0][-1]:.2f}\uff0c{timer.avg():.1f}\u79d2/\u8f6e\uff0c'\n          f'\u5728{str(devices)}')\n</code></pre>"},{"location":"DeepLearning/#computer-vision","title":"Computer Vision","text":""},{"location":"DeepLearning/#image-augmentation","title":"Image Augmentation","text":"<ul> <li>\u7ffb\u8f6c\uff08\u4e0a\u4e0b\u7ffb\u8f6c\u3001\u5de6\u53f3\u7ffb\u8f6c\uff09 <code>torchvision.transforms.RandomHorizontalFlip()</code> <code>torchvision.transforms.RandomVerticalFlip()</code></li> <li>\u968f\u673a\u526a\u88c1 <code>torchvision.transforms.RandomResizedCrop((200, 200), scale=(0.1, 1), ratio=(0.5, 2))</code></li> <li>\u6539\u53d8\u989c\u8272\uff08\u4eae\u5ea6\u3001\u5bf9\u6bd4\u5ea6\u3001\u9971\u548c\u5ea6\u3001\u8272\u8c03\uff09 <code>torchvision.transforms.ColorJitter(brightness=0.5, contrast=0, saturation=0, hue=0)</code></li> </ul>"},{"location":"DeepLearning/#_18","title":"\u76ee\u6807\u68c0\u6d4b","text":""},{"location":"DeepLearning/#bounding-box","title":"Bounding Box","text":"<p>\u4e24\u79cd\u8868\u793a - \u5de6\u4e0a\u89d2\u5750\u6807\u548c\u53f3\u4e0b\u89d2\u5750\u6807 - \u4e2d\u5fc3\u5750\u6807\u548cw\uff0ch</p>"},{"location":"DeepLearning/#_19","title":"\u951a\u6846","text":""},{"location":"DeepLearning/#_20","title":"\u751f\u6210\u951a\u6846","text":"<p>\u8f93\u5165\u56fe\u50cf\u7684\u9ad8\u5ea6\u4e3ah\uff0c\u5bbd\u5ea6\u4e3aw\u3002\u4ee5\u56fe\u50cf\u7684\u6bcf\u4e2a\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u751f\u6210\u4e0d\u540c\u5f62\u72b6\u7684\u951a\u6846\uff1a\u7f29\u653e\u6bd4*\u4e3as\\in (0, 1]\uff0c*\u5bbd\u9ad8\u6bd4*\u4e3ar &gt; 0\u3002 \u90a3\u4e48**\u951a\u6846\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u5206\u522b\u662fhs\\sqrt{r}\u548chs/\\sqrt{r}\u3002* \u8bf7\u6ce8\u610f\uff0c\u5f53\u4e2d\u5fc3\u4f4d\u7f6e\u7ed9\u5b9a\u65f6\uff0c\u5df2\u77e5\u5bbd\u548c\u9ad8\u7684\u951a\u6846\u662f\u786e\u5b9a\u7684\u3002</p> <p>\u7f29\u653e\u6bd4\uff08scale\uff09\u53d6\u503cs_1,\\ldots, s_n\u548c\u5bbd\u9ad8\u6bd4\uff08aspect ratio\uff09\u53d6\u503cr_1,\\ldots, r_m\u3002\u4f7f\u7528\u8fd9\u4e9b\u6bd4\u4f8b\u548c\u957f\u5bbd\u6bd4\u7684\u6240\u6709\u7ec4\u5408\u4ee5\u6bcf\u4e2a\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u65f6\uff0c\u8f93\u5165\u56fe\u50cf\u5c06\u603b\u5171\u6709whnm\u4e2a\u951a\u6846\u3002 \u5728\u5b9e\u8df5\u4e2d\uff0c\u8003\u8651\u5305\u542bs_1\u6216r_1\u7684\u7ec4\u5408\uff1a</p> (s_1, r_1), (s_1, r_2), \\ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \\ldots, (s_n, r_1). <p>\u4e5f\u5c31\u662f\u8bf4\uff0c\u4ee5\u540c\u4e00\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u7684\u951a\u6846\u7684\u6570\u91cf\u662fn+m-1\u3002\u5bf9\u4e8e\u6574\u4e2a\u8f93\u5165\u56fe\u50cf\uff0c\u5c06\u5171\u751f\u6210wh(n+m-1)\u4e2a\u951a\u6846\u3002</p>"},{"location":"DeepLearning/#iou","title":"\u4ea4\u5e76\u6bd4(IoU)","text":"J(\\mathcal{A},\\mathcal{B}) = \\frac{\\left|\\mathcal{A} \\cap \\mathcal{B}\\right|}{\\left| \\mathcal{A} \\cup \\mathcal{B}\\right|}."},{"location":"DeepLearning/#_21","title":"\u951a\u6846\u5206\u914d","text":"<p>\u6bcf\u6b21\u53d6\u6700\u5927IoU\u7684\u951a\u6846\u548c\u771f\u5b9e\u6846\uff0c\u53bb\u6389\u4e4b\u540e\u91cd\u590d\uff0c\u6700\u540e\u6839\u636e\u9608\u503c\u786e\u5b9a\u662f\u5426\u4e3a\u951a\u6846\u5206\u914d\u771f\u5b9e\u6846</p>"},{"location":"DeepLearning/#_22","title":"\u6807\u8bb0\u7c7b\u522b","text":"<p>\u7ed9\u5b9a\u6846A\u548cB\uff0c\u4e2d\u5fc3\u5750\u6807\u5206\u522b\u4e3a(x_a, y_a)\u548c(x_b, y_b)\uff0c\u5bbd\u5ea6\u5206\u522b\u4e3aw_a\u548cw_b\uff0c\u9ad8\u5ea6\u5206\u522b\u4e3ah_a\u548ch_b\uff0c\u53ef\u4ee5\u5c06A\u7684\u504f\u79fb\u91cf\u6807\u8bb0\u4e3a\uff1a</p> \\left( \\frac{ \\frac{x_b - x_a}{w_a} - \\mu_x }{\\sigma_x}, \\frac{ \\frac{y_b - y_a}{h_a} - \\mu_y }{\\sigma_y}, \\frac{ \\log \\frac{w_b}{w_a} - \\mu_w }{\\sigma_w}, \\frac{ \\log \\frac{h_b}{h_a} - \\mu_h }{\\sigma_h}\\right),"},{"location":"DeepLearning/#_23","title":"\u975e\u6781\u5927\u503c\u6291\u5236","text":"<p>\u5728\u540c\u4e00\u5f20\u56fe\u50cf\u4e2d\uff0c\u6240\u6709\u9884\u6d4b\u7684\u975e\u80cc\u666f\u8fb9\u754c\u6846\u90fd\u6309\u7f6e\u4fe1\u5ea6\u964d\u5e8f\u6392\u5e8f\uff0c\u4ee5\u751f\u6210\u5217\u8868L\u3002</p> <ol> <li>\u4eceL\u4e2d\u9009\u53d6\u7f6e\u4fe1\u5ea6\u6700\u9ad8\u7684\u9884\u6d4b\u8fb9\u754c\u6846B_1\u4f5c\u4e3a\u57fa\u51c6\uff0c\u7136\u540e\u5c06\u6240\u6709\u4e0eB_1\u7684IoU\u8d85\u8fc7\u9884\u5b9a\u9608\u503c\\epsilon\u7684\u975e\u57fa\u51c6\u9884\u6d4b\u8fb9\u754c\u6846\u4eceL\u4e2d\u79fb\u9664\u3002</li> <li>\u4eceL\u4e2d\u9009\u53d6\u7f6e\u4fe1\u5ea6\u7b2c\u4e8c\u9ad8\u7684\u9884\u6d4b\u8fb9\u754c\u6846B_2\u4f5c\u4e3a\u53c8\u4e00\u4e2a\u57fa\u51c6\uff0c\u7136\u540e\u5c06\u6240\u6709\u4e0eB_2\u7684IoU\u5927\u4e8e\\epsilon\u7684\u975e\u57fa\u51c6\u9884\u6d4b\u8fb9\u754c\u6846\u4eceL\u4e2d\u79fb\u9664\u3002</li> <li>\u91cd\u590d\u4e0a\u8ff0\u8fc7\u7a0b\uff0c\u76f4\u5230L\u4e2d\u7684\u6240\u6709\u9884\u6d4b\u8fb9\u754c\u6846\u90fd\u66fe\u88ab\u7528\u4f5c\u57fa\u51c6\u3002\u6b64\u65f6\uff0cL\u4e2d\u4efb\u610f\u4e00\u5bf9\u9884\u6d4b\u8fb9\u754c\u6846\u7684IoU\u90fd\u5c0f\u4e8e\u9608\u503c\\epsilon\uff1b\u56e0\u6b64\uff0c\u6ca1\u6709\u4e00\u5bf9\u8fb9\u754c\u6846\u8fc7\u4e8e\u76f8\u4f3c\u3002</li> <li>\u8f93\u51fa\u5217\u8868L\u4e2d\u7684\u6240\u6709\u9884\u6d4b\u8fb9\u754c\u6846\u3002</li> </ol>"},{"location":"DeepLearning/#r-cnn","title":"R-CNN","text":""},{"location":"DeepLearning/#region-based-cnn","title":"Region-based CNN","text":"<ol> <li>\u4f7f\u7528\u542f\u53d1\u5f0f\u641c\u7d22\u9009\u62e9\u951a\u6846</li> <li>\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u6bcf\u4e2a\u951a\u6846\u62bd\u53d6\u7279\u5f81</li> <li>\u8bad\u7ec3\u4e00\u4e2aSVM\u6765\u5bf9\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b</li> <li>\u8bad\u7ec3\u4e00\u4e2a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u6765\u9884\u6d4b\u8fb9\u7f18\u6846\u504f\u79fb</li> </ol> <p>Rol Pooling \u7ed9\u5b9a\u4e00\u4e2a\u951a\u6846\uff0c\u5747\u5300\u5206\u6210n \\times m\u5757\uff0c\u8f93\u51fa\u6bcf\u5757\u91cc\u9762\u7684\u6700\u5927\u503c\uff0c\u4e0d\u7ba1\u951a\u6846\u5927\u5c0f\u4e3a\u591a\u5c11\uff0c\u90fd\u662fnm</p>"},{"location":"DeepLearning/#fast-rcnn","title":"Fast RCNN","text":"<p>\u4f7f\u7528CNN\u5bf9\u56fe\u7247\u62bd\u53d6\u7279\u5f81\uff08\u6574\u5f20\u56fe\u7247\uff09\uff0c\u4f7f\u7528Rol Pooling\u5c42\u5bf9\u6bcf\u4e2a\u951a\u6846\u751f\u6210\u56fa\u5b9a\u957f\u5ea6\u7684\u7279\u5f81</p>"},{"location":"DeepLearning/#faster-rcnn","title":"Faster RCNN","text":"<p>\u4f7f\u7528\u4e00\u4e2a\u7f51\u7edc\u6765\u66ff\u4ee3\u542f\u53d1\u5f0f\u641c\u7d22\u6765\u83b7\u5f97\u66f4\u597d\u7684\u951a\u6846</p>"},{"location":"DeepLearning/#mask-rcnn","title":"Mask RCNN","text":"<p>\u5982\u679c\u6709\u50cf\u7d20\u7ea7\u522b\u7684\u6807\u53f7\uff0c\u7528FCN\u6765\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f</p>"},{"location":"DeepLearning/#yolo","title":"YOLO","text":"<p>\u53ea\u770b\u4e00\u6b21 - SSD\u951a\u6846\u5927\u91cf\u91cd\u53e0\u6d6a\u8d39\u8ba1\u7b97 - YOLO\u5c06\u56fe\u7247\u5747\u5300\u5206\u6210S \\times S \u951a\u6846 - \u6bcf\u4e2a\u951a\u6846\u9884\u6d4bB\u4e2a\u8fb9\u7f18\u6846</p>"},{"location":"DeepLearning/#ssd","title":"\u5355\u53d1\u591a\u6846\u68c0\u6d4b\uff08SSD\uff09","text":""},{"location":"DeepLearning/#_24","title":"\u8bed\u4e49\u5206\u5272","text":"<p>\u56fe\u50cf\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272</p>"},{"location":"DeepLearning/#_25","title":"\u8f6c\u7f6e\u5377\u79ef","text":"<p>\u8f6c\u7f6e\u5377\u79ef\u53ef\u4ee5\u7528\u6765\u589e\u52a0\u9ad8\u5bbd</p> <p>Y[i: i + h, j: j + w] += X[i, j] * K</p> <p>\u8f6c\u7f6e? Y = X * W \u53ef\u4ee5\u5bf9W\u6784\u9020\u4e00\u4e2aV\uff0c\u4f7f\u5f97\u5377\u79ef\u7b49\u4ef7\u4e8e\u77e9\u9635\u4e58\u6cd5Y^ \\prime = V X^ \\prime, \u8fd9\u91ccX^ \\prime Y^\\prime\u662f XY\u5bf9\u5e94\u7684\u5411\u91cf\u7248\u672c \u8f6c\u7f6e\u5377\u79ef\u7b49\u4ef7\u4e8e $Y^\\prime = V^T X^\\prime $</p> <p>\u57fa\u672c\u64cd\u4f5c</p> <pre><code>def trans_conv(X, K):\n    h, w = K.shape\n    Y = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            Y[i: i + h, j: j + w] += X[i, j] * K\n    return Y\n</code></pre> <p>torch API</p> <pre><code>X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\ntconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)\n</code></pre> <p>tensor([ [ [ [ 0.,  0.,  1.],           [ 0.,  4.,  6.],           [ 4., 12.,  9.] ] ] ])</p> <p><code>tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)</code>padding\u5c06\u5220\u9664\u7b2c\u4e00\u548c\u6700\u540e\u4e00\u884c\u548c\u5217</p> <p>tensor([ [ [ [4.] ] ] ])</p> <p><code>tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)</code> \u6b65\u5e45\u4e3a2\u589e\u5927\u8f93\u51fa</p> <p>tensor([ [ [ [0., 0., 0., 1.],           [0., 0., 2., 3.],           [0., 2., 0., 3.],           [4., 6., 6., 9.] ] ] ])</p>"},{"location":"DeepLearning/#fcn","title":"\u5168\u5377\u79ef\u7f51\u7edc(FCN)","text":"<p>\u7528\u8f6c\u7f6e\u5377\u79ef\u5c42\u6765\u66ff\u6362CNN\u6700\u540e\u7684\u5168\u8fde\u63a5\u5c42\uff0c\u4ece\u800c\u5b9e\u73b0\u6bcf\u4e2a\u50cf\u7d20\u7684\u9884\u6d4b</p> <p>CNN \u2192 1x1 Conv \u2192 \u8f6c\u7f6e\u5377\u79ef \u2192 output</p> <p>\u5148\u7528Resnet18\u63d0\u53d6\u7279\u5f81<code>net = nn.Sequential(*list(pretrained_net.children())[:-2])</code></p> <p>\u7136\u540e\u52a01x1\u7684\u5377\u79ef\u5c42\u548c\u8f6c\u7f6e\u5377\u79ef\u5c42\uff0c\u4f7f\u5f97\u8f93\u51fa\u5927\u5c0f\u548c\u539f\u56fe\u50cf\u5927\u5c0f\u76f8\u540c</p> <pre><code>num_classes = 21\nnet.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\nnet.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,\n                                    kernel_size=64, padding=16, stride=32))\n</code></pre> <p></p>"},{"location":"DeepLearning_2/","title":"Deep Learning_2","text":""},{"location":"DeepLearning_2/#bert","title":"BERT","text":"<p>\u8f93\u5165\u8868\u793a \u628a\u4e24\u4e2a\u53e5\u5b50\u53d8\u6210BERT\u7684\u8f93\u5165</p> <pre><code>def get_tokens_and_segments(tokens_a, tokens_b=None):\n    \"\"\"\u83b7\u53d6\u8f93\u5165\u5e8f\u5217\u7684\u8bcd\u5143\u53ca\u5176\u7247\u6bb5\u7d22\u5f15\"\"\"\n    tokens = ['&lt;cls&gt;'] + tokens_a + ['&lt;sep&gt;']\n    # 0\u548c1\u5206\u522b\u6807\u8bb0\u7247\u6bb5A\u548cB\n    segments = [0] * (len(tokens_a) + 2)\n    if tokens_b is not None:\n        tokens += tokens_b + ['&lt;sep&gt;']\n        segments += [1] * (len(tokens_b) + 1)\n    return tokens, segments\n</code></pre>"},{"location":"DeepLearning_2/#vision-transformer","title":"Vision Transformer","text":"<p>\u8bba\u6587 \u4ee3\u7801\u6765\u6e90</p> <p>\u4e3b\u8981\u5c31\u662f\u56fe\u7247\u8f6c\u6362tokens\uff0cposition embedding\u8fd9\u91cc\u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570</p> <p>Example \u8f93\u5165224x224x3 -&gt; Embedding(16x16\u7684\u5377\u79ef\u6838\uff0c\u6b65\u8ddd\u4e3a16\u7684\u5377\u79ef\u5c42) 14x14x768 -&gt; Flatten 196x768 -&gt; Concat\u4e00\u4e2aClass token 197x768 -&gt; \u52a0\u4e0aposition embedding -&gt; Dropout -&gt; \u91cd\u590dL\u6b21 Transformer Encoder 197x768-&gt; LayerNorm -&gt; Extract class token 1x768(\u4e4b\u524dconcat\u7684class token)-&gt; MLP Head</p> <pre><code>class PatchEmbed(nn.Module):\n    \"\"\"\n    2D Image to Patch Embedding\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):\n        super().__init__()\n        img_size = (img_size, img_size)\n        patch_size = (patch_size, patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size  # \u6bcf\u4e2apatch\u7684\u5927\u5c0f\n        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])  # 224/16 -&gt; 14*14\n        self.num_patches = self.grid_size[0] * self.grid_size[1]  # patches\u7684\u6570\u76ee\n\n        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)  # \u5377\u79ef\u6838\u5927\u5c0f\u548cpatch_size\u90fd\u662f16*16\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()  # \u5982\u679c\u6ca1\u6709\u4f20\u5165norm\u5c42\uff0c\u5c31\u4f7f\u7528identity\n\n    def forward(self, x):\n        B, C, H, W = x.shape  # \u6ce8\u610f\uff0c\u5728vit\u6a21\u578b\u4e2d\u8f93\u5165\u5927\u5c0f\u5fc5\u987b\u662f\u56fa\u5b9a\u7684\uff0c\u9ad8\u5bbd\u548c\u8bbe\u5b9a\u503c\u76f8\u540c\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n\n        # flatten: [B, C, H, W] -&gt; [B, C, HW]\n        # transpose: [B, C, HW] -&gt; [B, HW, C]\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        x = self.norm(x)\n        return x\n</code></pre> <pre><code>class Attention(nn.Module):  # Multi-head selfAttention \u6a21\u5757\n    def __init__(self,\n                 dim,   # \u8f93\u5165token\u7684dim\n                 num_heads=8,  # head\u7684\u4e2a\u6570\n                 qkv_bias=False,  # \u751f\u6210qkv\u65f6\u662f\u5426\u4f7f\u7528\u504f\u7f6e\n                 qk_scale=None,\n                 attn_drop_ratio=0.,  # \u4e24\u4e2adropout ratio\n                 proj_drop_ratio=0.):\n        super(Attention, self).__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads  # \u6bcf\u4e2ahead\u7684dim\n        self.scale = qk_scale or head_dim ** -0.5  # \u4e0d\u53bb\u4f20\u5165qkscale\uff0c\u4e5f\u5c31\u662f1/\u221adim_k\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # \u4f7f\u7528\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\uff0c\u4e00\u6b21\u5f97\u5230qkv\n        self.attn_drop = nn.Dropout(attn_drop_ratio)\n        self.proj = nn.Linear(dim, dim)  # \u628a\u591a\u4e2ahead\u8fdb\u884cConcat\u64cd\u4f5c\uff0c\u7136\u540e\u901a\u8fc7Wo\u6620\u5c04\uff0c\u8fd9\u91cc\u7528\u5168\u8fde\u63a5\u5c42\u4ee3\u66ff\n        self.proj_drop = nn.Dropout(proj_drop_ratio)\n\n    def forward(self, x):\n        # [batch_size, num_patches + 1, total_embed_dim] \u52a01\u4ee3\u8868\u7c7b\u522b\uff0c\u9488\u5bf9ViT-B/16\uff0cdim\u662f768\n        B, N, C = x.shape\n\n        # qkv(): -&gt; [batch_size, num_patches + 1, 3 * total_embed_dim]\n        # reshape: -&gt; [batch_size, num_patches + 1, 3\uff08\u4ee3\u8868qkv\uff09, num_heads\uff08\u4ee3\u8868head\u6570\uff09, embed_dim_per_head\uff08\u6bcf\u4e2ahead\u7684qkv\u7ef4\u5ea6\uff09]\n        # permute: -&gt; [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        # transpose: -&gt; [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n        # @: multiply -&gt; [batch_size, num_heads, num_patches + 1, num_patches + 1]\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # \u6bcf\u4e2aheader\u7684q\u548ck\u76f8\u4e58\uff0c\u9664\u4ee5\u221adim_k\uff08\u76f8\u5f53\u4e8enorm\u5904\u7406\uff09\n        attn = attn.softmax(dim=-1)  # \u901a\u8fc7softmax\u5904\u7406\uff08\u76f8\u5f53\u4e8e\u5bf9\u6bcf\u4e00\u884c\u7684\u6570\u636esoftmax\uff09\n        attn = self.attn_drop(attn)  # dropOut\u5c42\n\n        # @: multiply -&gt; [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n        # transpose: -&gt; [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n        # reshape: -&gt; [batch_size, num_patches + 1, total_embed_dim]\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)  # \u5f97\u5230\u7684\u7ed3\u679c\u548cV\u77e9\u9635\u76f8\u4e58\uff08\u52a0\u6743\u6c42\u548c\uff09\uff0creshape\u76f8\u5f53\u4e8e\u628ahead\u62fc\u63a5\n        x = self.proj(x)  # \u901a\u8fc7\u5168\u8fde\u63a5\u8fdb\u884c\u6620\u5c04\uff08\u76f8\u5f53\u4e8e\u4e58\u8bba\u6587\u4e2d\u7684Wo\uff09\n        x = self.proj_drop(x)  # dropOut\n        return x\n</code></pre> <pre><code>class Mlp(nn.Module):  # Encoder\u4e2d\u7684MLP Block\n    \"\"\"\n    MLP as used in Vision Transformer, MLP-Mixer and related networks\n    \"\"\"\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features  # \u5982\u679c\u6ca1\u6709\u4f20\u5165out features\uff0c\u5c31\u9ed8\u8ba4\u662fin_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()  # \u9ed8\u8ba4\u662fGELU\u6fc0\u6d3b\u51fd\u6570\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n</code></pre> <pre><code># Transformer Encoder\u5c42\u4ee3\u7801\u89e3\u8bfb\nclass Block(nn.Module):  # Encoder Block\n    def __init__(self,\n                 dim,  # \u6bcf\u4e2atoken\u7684\u7ef4\u5ea6\n                 num_heads,  # head\u4e2a\u6570\n                 mlp_ratio=4.,  # \u7b2c\u4e00\u4e2a\u7ed3\u70b9\u4e2a\u6570\u662f\u8f93\u5165\u8282\u70b9\u7684\u56db\u500d\n                 qkv_bias=False,  # \u662f\u5426\u4f7f\u7528bias\n                 qk_scale=None,\n                 drop_ratio=0.,  # Attention\u6a21\u5757\u4e2d\u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4f7f\u7528\u7684drop_ratio\n                 attn_drop_ratio=0.,\n                 drop_path_ratio=0.,\n                 act_layer=nn.GELU,\n                 norm_layer=nn.LayerNorm):\n        super(Block, self).__init__()\n        self.norm1 = norm_layer(dim)  # layer norm\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)  # Multihead Attention\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio &gt; 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)  # MLP\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u7684\u4e2a\u6570\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre> <pre><code>class VisionTransformer(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000,\n                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True,\n                 qk_scale=None, representation_size=None, distilled=False, drop_ratio=0.,\n                 attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PatchEmbed, norm_layer=None,\n                 act_layer=None):\n\n        super(VisionTransformer, self).__init__()\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.num_tokens = 2 if distilled else 1  # \u4e00\u822c\u7b49\u4e8e1\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)  # \u4e3aNorm\u4f20\u5165\u9ed8\u8ba4\u53c2\u6570\n        act_layer = act_layer or nn.GELU\n\n        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)  # Patch Embedding\u5c42\n        num_patches = self.patch_embed.num_patches  # patches\u7684\u603b\u4e2a\u6570\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))  # \u6784\u5efa\u53ef\u8bad\u7ec3\u53c2\u6570\u76840\u77e9\u9635\uff0c\u7528\u4e8e\u7c7b\u522b\n        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None  # \u9ed8\u8ba4\u4e3aNone\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))  # \u4f4d\u7f6eembedding\uff0c\u548cconcat\u540e\u7684\u6570\u636e\u4e00\u6837\n        self.pos_drop = nn.Dropout(p=drop_ratio)  # DropOut\u5c42\uff08\u6dfb\u52a0\u4e86pos_embed\u4e4b\u540e\uff09\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # \u4ece0\u5230ratio\uff0c\u6709depth\u4e2a\u5143\u7d20\u7684\u7b49\u5dee\u5e8f\u5217\n        self.blocks = nn.Sequential(*[\n            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],\n                  norm_layer=norm_layer, act_layer=act_layer)\n            for i in range(depth)  # \u6709\u591a\u5c11\u5c42\u5faa\u73af\u591a\u5c11\u6b21\n        ])\n        self.norm = norm_layer(embed_dim)\n\n        # Representation layer\n        if representation_size and not distilled:  # representation_size\u4e3aTrue\u5c31\u5728MLPHead\u6784\u5efaPreLogits,\u5426\u5219\u53ea\u6709Linear\u5c42\n            self.has_logits = True\n            self.num_features = representation_size\n            self.pre_logits = nn.Sequential(OrderedDict([\n                (\"fc\", nn.Linear(embed_dim, representation_size)),\n                (\"act\", nn.Tanh())\n            ]))\n        else:\n            self.has_logits = False\n            self.pre_logits = nn.Identity()\n\n        # Classifier head(s) \u7ebf\u6027\u5206\u7c7b\u5c42\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes &gt; 0 else nn.Identity()\n        self.head_dist = None\n        if distilled:\n            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes &gt; 0 else nn.Identity()\n\n        # Weight init \u6743\u91cd\u521d\u59cb\u5316\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        if self.dist_token is not None:\n            nn.init.trunc_normal_(self.dist_token, std=0.02)\n\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        self.apply(_init_vit_weights)\n\n    def forward_features(self, x):\n        # [B, C, H, W] -&gt; [B, num_patches, embed_dim]\n        x = self.patch_embed(x)  # [B, 196, 768] \u8f93\u5165Patch Embedding\u5c42\n        # [1, 1, 768] -&gt; [B, 1, 768] \u5728batch\u7ef4\u5ea6\u590d\u5236batch_size\u4efd\n        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n        if self.dist_token is None:\n            x = torch.cat((cls_token, x), dim=1)  # [B, 197, 768] \u5c06cls_token\u4e0ex\u5728\u7ef4\u5ea61\u4e0a\u62fc\u63a5\u3002\u6ce8\u610f\uff1acls_token\u5728\u524d\uff0cx\u5728\u540e\n        else:\n            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n\n        x = self.pos_drop(x + self.pos_embed)  # concat\u540e\u7684\u6570\u636e\u52a0\u4e0aposition\uff0c\u518d\u7ecf\u8fc7dropout\u5c42\n        x = self.blocks(x)  # \u7ecf\u8fc7\u5806\u53e0\u7684Encoder blocks\n        x = self.norm(x)  # Layer Norm\u5c42\n        if self.dist_token is None:  # \u4e00\u822c\u4e3aNone\uff0c\u672c\u8d28\u4e0a\u662fIdentity\u5c42\n            return self.pre_logits(x[:, 0])  # \u63d0\u53d6cls_token\u4fe1\u606f\uff0c\u56e0\u4e3acls_token\u7ef4\u5ea6\u5728\u524d\uff0c\u6240\u4ee5\u7d22\u5f15\u4e3a0\u5c31\u662fcls\u672c\u8eab\n        else:\n            return x[:, 0], x[:, 1]\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        if self.head_dist is not None:\n            x, x_dist = self.head(x[0]), self.head_dist(x[1])\n            if self.training and not torch.jit.is_scripting():\n                # during inference, return the average of both classifier predictions\n                return x, x_dist\n            else:\n                return (x + x_dist) / 2\n        else:\n            x = self.head(x)\n        return x\n</code></pre>"},{"location":"DeepLearning_2/#swin-transformer","title":"Swin Transformer","text":"<p>\u8bba\u6587 \u4f5c\u8005\u56e2\u961f\u4ee3\u7801</p> <p>transformer \u7528\u4f5cCV\u9886\u57df\u7684\u9aa8\u5e72\u7f51\u7edc patch merging \u591a\u5c3a\u5ea6\u7684\u7279\u5f81\u56fe</p>"},{"location":"DeepLearning_2/#detr","title":"DETR","text":"<p>\u7aef\u5230\u7aef\u76ee\u6807\u68c0\u6d4b\uff0c\u57fa\u4e8e\u96c6\u5408\u7684\u76ee\u6807\u51fd\u6570\u53bb\u505a\u76ee\u6807\u68c0\u6d4b\uff0c\u7b80\u5316\u4e86\u4ee5\u5f80\u76ee\u6807\u68c0\u6d4b\u975e\u6781\u5927\u503c\u6291\u5236\u7684\u90e8\u5206</p> <p>\u8bad\u7ec3\uff1a CNN\u62bd\u7279\u5f81 -&gt; Transformer Encoder \u5168\u5c40\u7279\u5f81 -&gt; Decoder \u5f97\u5230\u56fa\u5b9a\u6570\u91cf\u9884\u6d4b\u6846 -&gt; \u4e0eGround truth \u505a\u4e8c\u5206\u56fe\u5339\u914d\uff08100\u4e2a\u6846 2\u4e2a\u6846\u5339\u914d\u4e0a\u4e86 \u5269\u4e0b\u7684\u6807\u8bb0\u4e3a\u80cc\u666f\uff09\u7b97loss</p> <p>\u9884\u6d4b\uff1a \u6700\u540e\u4e00\u6b65\u5361\u7f6e\u4fe1\u5ea6</p> <p>\u7ed3\u8bba\uff1a COCO\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u548cFaster RCNN \u5dee\u4e0d\u591a\u7684\u6548\u679c\uff0c\u5c0f\u7269\u4f53\u4e0a\u8868\u73b0\u4e0d\u600e\u4e48\u597d\uff0cDETR\u8bad\u7ec3\u592a\u6162\uff0c\u80fd\u591f\u4f5c\u4e3a\u7edf\u4e00\u7684\u6846\u67b6\u62d3\u5c55\u5230\u522b\u7684\u4efb\u52a1\u4e0a\uff08\u76ee\u6807\u8ffd\u8e2a\uff0c\u8bed\u4e49\u5206\u5272\uff0c\u89c6\u9891\u91cc\u7684\u59ff\u6001\u9884\u6d4b\u00b7\u00b7\u00b7\uff09</p>"}]}