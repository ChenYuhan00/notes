{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"\u4e3b\u9875"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"DeepLearning/","text":"\u8bb0\u5f55\u4e86\u5b66\u4e60pytorch\u548c\u795e\u7ecf\u7f51\u7edc \u611f\u89c9\u4f7f\u7528pytorch\u8fdb\u884c\u8bad\u7ec3\u57fa\u672c\u5927\u540c\u5c0f\u5f02\uff0c\u4e3b\u8981\u8981\u64cd\u4f5c\u7684\u5c31\u662f\u5b9a\u4e49\u7f51\u7edc\u3001dataloader\u548cdataiter\uff1b\u8bb8\u591a\u795e\u7ecf\u7f51\u7edc\u7684\u5927\u6982\u4e5f\u6709\u6240\u4e86\u89e3\uff1b\u76ee\u524d\u4e3b\u8981\u95ee\u9898\u662fpytorch\u7684\u5185\u90e8\u5b9e\u73b0\u4e0d\u61c2\uff0cnumpy\u7b49\u4f7f\u7528\u4e0d\u719f\u7ec3\uff0c\u7f51\u7edc\u5177\u4f53\u7ed3\u6784\u4e0d\u6e05\u695a \u53c2\u8003 \u6df1\u5165\u6d45\u51faPyTorch \u4ee5\u53ca dive into deep learning \u548c \u4ed6\u7684b\u7ad9\u89c6\u9891 Pytorch\u57fa\u7840 deep-learning-computation\u7ae0\u8282 Pytorch\u81ea\u52a8\u6c42\u5bfc \u6df1\u5165\u6d45\u51faPytroch \u7b2c\u4e8c\u7ae0 \u8ba1\u7b97\u56fe \u5c06\u4ee3\u7801\u5206\u89e3\u6210\u64cd\u4f5c\u5b50\uff0c\u5c06\u8ba1\u7b97\u8868\u793a\u4e3a\u4e00\u4e2a\u65e0\u73af\u56fe \u4e00\u822c\u53ea\u6709leaf node\u6709grad \u6700\u540e\u7684\u8f93\u51fa\u770b\u6210\u5173\u4e8e\u7f51\u7edc\u6743\u91cd\u7684\u51fd\u6570\uff0cbackward\u51fd\u6570\u8ba1\u7b97\u51fa\u6743\u91cd\u7684\u68af\u5ea6\uff08\u5168\u5fae\u5206\uff09 \u5bf9\u4e8eleaf\u548crequire_grad\u7684\u8282\u70b9\u4e0d\u80fd\u591f\u8fdb\u884cinplace operation retain_graph \u9632\u6b62backward\u4e4b\u540e\u91ca\u653e\u76f8\u5173\u5185\u5b58 .detach return a new tensor ,detached from the current graph,the result will never require gradient \u5c06\u8ba1\u7b97\u79fb\u52a8\u5230\u8ba1\u7b97\u56fe\u4e4b\u5916\uff0c\u5f53\u4f5c\u5e38\u6570 \u6570\u636e\u8bfb\u53d6 \u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d \u968f\u673a\u91c7\u6837b\u4e2a\u6837\u672c $i_1,i_2,...,i_b$\u6765\u8fd1\u4f3c\u635f\u5931 $$ \\frac{1}{b}\\sum_{i \\in I_b}l(\\hat y_i,y_i) $$ \u4e00\u6b21\u8fed\u4ee3\u7528b\u4e2a\u6570\u636e\u8ba1\u7b97\u540e\u66f4\u65b0\u53c2\u6570 \u4e00\u4e2aepoch\u5c06\u6570\u636e\u96c6\u7684\u6570\u636e\u90fd\u7528\u4e00\u904d data iterator \u4e00\u6b21\u968f\u673a\u8bfb\u53d6batch_size\u4e2a\u6570\u636e def data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) for i in range(0, num_examples, batch_size): batch_indices = torch.tensor( indices[i: min(i + batch_size, num_examples)]) yield features[batch_indices], labels[batch_indices] \u8865\u5145 cat \u548c stack \u53c2\u8003 \u535a\u5ba2 \u6279\u91cf\u77e9\u9635\u4e58\u6cd5 torch.bmm(X,Y),X\u7684shape\u4e3a(n,a,b)\uff0cY\u7684shape\u4e3a(n,b,c)\uff0c\u8f93\u51fa\u5f62\u72b6(n,a,c) torch.unsqueeze() \u5728\u6307\u5b9a\u7eac\u5ea6\u63d2\u5165\u7eac\u5ea61 X.shape = (2,3) X.unsqueeze(0).shape = (1\uff0c2\uff0c3) X.unsqueeze(1).shape = (2,1,3) MLP dive into deep learning \u591a\u5c42\u611f\u77e5\u673a\u7ae0\u8282 num_inputs, num_outputs, num_hiddens = 784, 10, 256 W1 = nn.Parameter(torch.randn( num_inputs, num_hiddens, requires_grad=True) * 0.01) b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True)) W2 = nn.Parameter(torch.randn( num_hiddens, num_outputs, requires_grad=True) * 0.01) b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True)) params = [W1, b1, W2, b2] def relu(X): a = torch.zeros_like(X) return torch.max(X, a) def net(X): X = X.reshape((-1, num_inputs)) H = relu(X@W1 + b1) # \u8fd9\u91cc\u201c@\u201d\u4ee3\u8868\u77e9\u9635\u4e58\u6cd5 return (H@W2 + b2) \u7528 torch.nn net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10)) def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights); X = X.reshape((-1, num_inputs)) -- nn.Flatten() H = relu(X@W1 + b1) -- nn.Linear(784, 256) \u548c nn.ReLu() (H@W2 + b2) -- nn.Linear(256, 10) \u8fc7\u62df\u5408 \u6b63\u5219\u5316 $$ \\begin{aligned} L(\\mathbf{w}, b) + \\frac{\\lambda}{2} |\\mathbf{w}|^2 \\end{aligned} $$ $$ \\begin{aligned} \\mathbf{w} & \\leftarrow \\left(1- \\eta\\lambda \\right) \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned} $$ Drop out $$ \\begin{aligned} h' = \\begin{cases} 0 & \\text{ \u6982\u7387\u4e3a } p \\ \\frac{h}{1-p} & \\text{ \u5176\u4ed6\u60c5\u51b5} \\end{cases} \\end{aligned} $$ \u671f\u671b\u503c\u4fdd\u6301\u4e0d\u53d8\uff0c\u5373$E[h'] = h$\u3002 def dropout_layer(X, dropout): assert 0 <= dropout <= 1 # \u5728\u672c\u60c5\u51b5\u4e2d\uff0c\u6240\u6709\u5143\u7d20\u90fd\u88ab\u4e22\u5f03 if dropout == 1: return torch.zeros_like(X) # \u5728\u672c\u60c5\u51b5\u4e2d\uff0c\u6240\u6709\u5143\u7d20\u90fd\u88ab\u4fdd\u7559 if dropout == 0: return X mask = (torch.rand(X.shape) > dropout).float() return mask * X / (1.0 - dropout) dropout1, dropout2 = 0.2, 0.5 class Net(nn.Module): def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2, is_training = True): super(Net, self).__init__() self.num_inputs = num_inputs self.training = is_training self.lin1 = nn.Linear(num_inputs, num_hiddens1) self.lin2 = nn.Linear(num_hiddens1, num_hiddens2) self.lin3 = nn.Linear(num_hiddens2, num_outputs) self.relu = nn.ReLU() def forward(self, X): H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs)))) # \u53ea\u6709\u5728\u8bad\u7ec3\u6a21\u578b\u65f6\u624d\u4f7f\u7528dropout if self.training == True: # \u5728\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42 H1 = dropout_layer(H1, dropout1) H2 = self.relu(self.lin2(H1)) if self.training == True: # \u5728\u7b2c\u4e8c\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42 H2 = dropout_layer(H2, dropout2) out = self.lin3(H2) return out net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2) \u76f4\u63a5\u7528 nn.Dropout(p) net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), # \u5728\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42 nn.Dropout(dropout1), nn.Linear(256, 256), nn.ReLU(), # \u5728\u7b2c\u4e8c\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42 nn.Dropout(dropout2), nn.Linear(256, 10)) def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights); \u53c2\u6570\u521d\u59cb\u5316 Xavier\u521d\u59cb\u5316 $$o_{i} = \\sum_{j=1}^{n_\\mathrm{in}} w_{ij} x_j.$$ $$ \\begin{aligned} E[o_i] & = \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij} x_j] \\&= \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij}] E[x_j] \\&= 0, \\ \\mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\ & = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\ & = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\ & = n_\\mathrm{in} \\sigma^2 \\gamma^2. \\end{aligned} $$ \u4f7f\u65b9\u5dee\u6ee1\u8db3 $$ \\begin{aligned} \\frac{1}{2} (n_\\mathrm{in} + n_\\mathrm{out}) \\sigma^2 = 1 \\text{ or } \\sigma = \\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}. \\end{aligned} $$ Xavier\u521d\u59cb\u5316\u901a\u5e38\u4ece\u65b9\u5dee\u6ee1\u8db3\u4e0a\u8ff0\u7684\u5747\u5300\u5206\u5e03\u6216\u9ad8\u65af\u5206\u5e03\u4e2d\u91c7\u6837\u6743\u91cd CNN \u5377\u79ef\u64cd\u4f5c\u5b9e\u73b0 def corr2d(X, K): h, w = K.shape Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] = (X[i:i + h, j:j + w] * K).sum() return Y \u5377\u79ef\u5c42 class Conv2D(nn.Module): def __init__(self, kernel_size): super().__init__() self.weight = nn.Parameter(torch.rand(kernel_size)) self.bias = nn.Parameter(torch.zeros(1)) def forward(self, x): return corr2d(x, self.weight) + self.bias backprop\u8bad\u7ec3\u5377\u79ef\u6838 nn.Conv2d()\u8f93\u5165\u548c\u8f93\u51fa:\u6279\u91cf\u5927\u5c0f\u3001\u901a\u9053\u3001\u9ad8\u5ea6\u3001\u5bbd\u5ea6 \u591a\u8f93\u5165\u8f93\u51fa\u901a\u9053 \u591a\u8f93\u5165\u901a\u9053: def corr2d_multi_in(X, K): # \u5148\u904d\u5386\u201cX\u201d\u548c\u201cK\u201d\u7684\u7b2c0\u4e2a\u7ef4\u5ea6\uff08\u901a\u9053\u7ef4\u5ea6\uff09\uff0c\u518d\u628a\u5b83\u4eec\u52a0\u5728\u4e00\u8d77 return sum(d2l.corr2d(x, k) for x, k in zip(X, K)) \u591a\u8f93\u51fa\u901a\u9053: def corr2d_multi_in_out(X, K): # \u8fed\u4ee3\u201cK\u201d\u7684\u7b2c0\u4e2a\u7ef4\u5ea6\uff0c\u6bcf\u6b21\u90fd\u5bf9\u8f93\u5165\u201cX\u201d\u6267\u884c\u4e92\u76f8\u5173\u8fd0\u7b97\u3002 # \u6700\u540e\u5c06\u6240\u6709\u7ed3\u679c\u90fd\u53e0\u52a0\u5728\u4e00\u8d77 return torch.stack([corr2d_multi_in(X, k) for k in K], 0) \u8f93\u5165$\\mathbf{X} : c_i \\times n_h \\times n_w$ \u6838$\\mathbf{{W}} : c_0 \\times c_i \\times k_h \\times k_w $ \u8f93\u51fa$\\mathbf{Y} : c_o \\times m_h \\times m_w$ Pooling nn.MaxPool2d(3, padding=1, stride=2) pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1)) \u591a\u901a\u9053\u5bf9\u6bcf\u4e2a\u901a\u9053\u8fdb\u884cpooling\uff0c\u8f93\u51fa\u901a\u9053\u6570\u4e0e\u8f93\u5165\u76f8\u540c LeNet net = nn.Sequential( nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(), nn.Linear(120, 84), nn.Sigmoid(), nn.Linear(84, 10)) Conv2d output shape: torch.Size([1, 6, 28, 28]) Sigmoid output shape: torch.Size([1, 6, 28, 28]) AvgPool2d output shape: torch.Size([1, 6, 14, 14]) Conv2d output shape: torch.Size([1, 16, 10, 10]) Sigmoid output shape: torch.Size([1, 16, 10, 10]) AvgPool2d output shape: torch.Size([1, 16, 5, 5]) Flatten output shape: torch.Size([1, 400]) Linear output shape: torch.Size([1, 120]) Sigmoid output shape: torch.Size([1, 120]) Linear output shape: torch.Size([1, 84]) Sigmoid output shape: torch.Size([1, 84]) Linear output shape: torch.Size([1, 10]) AlexNet \u4f7f\u7528ReLU\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0cDropout\uff0c\u6570\u636e\u96c6\u9884\u5904\u7406\uff08\u88c1\u5207\u3001\u7ffb\u8f6c\u3001\u53d8\u8272\uff09 net = nn.Sequential( # \u8fd9\u91cc\u4f7f\u7528\u4e00\u4e2a11*11\u7684\u66f4\u5927\u7a97\u53e3\u6765\u6355\u6349\u5bf9\u8c61\u3002 # \u540c\u65f6\uff0c\u6b65\u5e45\u4e3a4\uff0c\u4ee5\u51cf\u5c11\u8f93\u51fa\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002 # \u53e6\u5916\uff0c\u8f93\u51fa\u901a\u9053\u7684\u6570\u76ee\u8fdc\u5927\u4e8eLeNet nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), # \u51cf\u5c0f\u5377\u79ef\u7a97\u53e3\uff0c\u4f7f\u7528\u586b\u5145\u4e3a2\u6765\u4f7f\u5f97\u8f93\u5165\u4e0e\u8f93\u51fa\u7684\u9ad8\u548c\u5bbd\u4e00\u81f4\uff0c\u4e14\u589e\u5927\u8f93\u51fa\u901a\u9053\u6570 nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), # \u4f7f\u7528\u4e09\u4e2a\u8fde\u7eed\u7684\u5377\u79ef\u5c42\u548c\u8f83\u5c0f\u7684\u5377\u79ef\u7a97\u53e3\u3002 # \u9664\u4e86\u6700\u540e\u7684\u5377\u79ef\u5c42\uff0c\u8f93\u51fa\u901a\u9053\u7684\u6570\u91cf\u8fdb\u4e00\u6b65\u589e\u52a0\u3002 # \u5728\u524d\u4e24\u4e2a\u5377\u79ef\u5c42\u4e4b\u540e\uff0c\u6c47\u805a\u5c42\u4e0d\u7528\u4e8e\u51cf\u5c11\u8f93\u5165\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6 nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(), # \u8fd9\u91cc\uff0c\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\u6570\u91cf\u662fLeNet\u4e2d\u7684\u597d\u51e0\u500d\u3002\u4f7f\u7528dropout\u5c42\u6765\u51cf\u8f7b\u8fc7\u62df\u5408 nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5), # \u6700\u540e\u662f\u8f93\u51fa\u5c42\u3002\u7531\u4e8e\u8fd9\u91cc\u4f7f\u7528Fashion-MNIST\uff0c\u6240\u4ee5\u7528\u7c7b\u522b\u6570\u4e3a10\uff0c\u800c\u975e\u8bba\u6587\u4e2d\u76841000 nn.Linear(4096, 10)) Conv2d output shape:torch.Size([1, 96, 54, 54]) ReLU output shape:torch.Size([1, 96, 54, 54]) MaxPool2d output shape:torch.Size([1, 96, 26, 26]) Conv2d output shape:torch.Size([1, 256, 26, 26]) ReLU output shape:torch.Size([1, 256, 26, 26]) MaxPool2d output shape:torch.Size([1, 256, 12, 12]) Conv2d output shape:torch.Size([1, 384, 12, 12]) ReLU output shape:torch.Size([1, 384, 12, 12]) Conv2d output shape:torch.Size([1, 384, 12, 12]) ReLU output shape:torch.Size([1, 384, 12, 12]) Conv2d output shape:torch.Size([1, 256, 12, 12]) ReLU output shape:torch.Size([1, 256, 12, 12]) MaxPool2d output shape:torch.Size([1, 256, 5, 5]) Flatten output shape:torch.Size([1, 6400]) Linear output shape:torch.Size([1, 4096]) ReLU output shape:torch.Size([1, 4096]) Dropout output shape:torch.Size([1, 4096]) Linear output shape:torch.Size([1, 4096]) ReLU output shape:torch.Size([1, 4096]) Dropout output shape:torch.Size([1, 4096]) Linear output shape:torch.Size([1, 10]) VGG VGG\u5757 kernel\u5927\u5c0f\u90fd\u4e3a3 $\\times$ 3 def vgg_block(num_convs, in_channels, out_channels): layers = [] for _ in range(num_convs): layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)) layers.append(nn.ReLU()) in_channels = out_channels layers.append(nn.MaxPool2d(kernel_size=2,stride=2)) return nn.Sequential(*layers) VGG\u7f51\u7edc VGG-11 conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512)) def vgg(conv_arch): conv_blks = [] in_channels = 1 # \u5377\u79ef\u5c42\u90e8\u5206 for (num_convs, out_channels) in conv_arch: conv_blks.append(vgg_block(num_convs, in_channels, out_channels)) in_channels = out_channels return nn.Sequential( *conv_blks, nn.Flatten(), # \u5168\u8fde\u63a5\u5c42\u90e8\u5206 nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 10)) net = vgg(conv_arch) NIN \u5728\u6bcf\u4e2a\u50cf\u7d20\u7684\u901a\u9053\u4e0a\u5206\u522b\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a def nin_block(in_channels, out_channels, kernel_size, strides, padding): return nn.Sequential( nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()) net = nn.Sequential( nin_block(1, 96, kernel_size=11, strides=4, padding=0), nn.MaxPool2d(3, stride=2), nin_block(96, 256, kernel_size=5, strides=1, padding=2), nn.MaxPool2d(3, stride=2), nin_block(256, 384, kernel_size=3, strides=1, padding=1), nn.MaxPool2d(3, stride=2), nn.Dropout(0.5), # \u6807\u7b7e\u7c7b\u522b\u6570\u662f10 nin_block(384, 10, kernel_size=3, strides=1, padding=1), nn.AdaptiveAvgPool2d((1, 1)), # \u5c06\u56db\u7ef4\u7684\u8f93\u51fa\u8f6c\u6210\u4e8c\u7ef4\u7684\u8f93\u51fa\uff0c\u5176\u5f62\u72b6\u4e3a(\u6279\u91cf\u5927\u5c0f,10) nn.Flatten()) nn.AdaptiveAvgPool2d() \u53c2\u6570\u4e3a\u6307\u5b9a\u8f93\u51fasize GoogLeNet class Inception(nn.Module): # c1--c4\u662f\u6bcf\u6761\u8def\u5f84\u7684\u8f93\u51fa\u901a\u9053\u6570 def __init__(self, in_channels, c1, c2, c3, c4, **kwargs): super(Inception, self).__init__(**kwargs) # \u7ebf\u8def1\uff0c\u53551x1\u5377\u79ef\u5c42 self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1) # \u7ebf\u8def2\uff0c1x1\u5377\u79ef\u5c42\u540e\u63a53x3\u5377\u79ef\u5c42 self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1) self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1) # \u7ebf\u8def3\uff0c1x1\u5377\u79ef\u5c42\u540e\u63a55x5\u5377\u79ef\u5c42 self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1) self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2) # \u7ebf\u8def4\uff0c3x3\u6700\u5927\u6c47\u805a\u5c42\u540e\u63a51x1\u5377\u79ef\u5c42 self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1) def forward(self, x): p1 = F.relu(self.p1_1(x)) p2 = F.relu(self.p2_2(F.relu(self.p2_1(x)))) p3 = F.relu(self.p3_2(F.relu(self.p3_1(x)))) p4 = F.relu(self.p4_2(self.p4_1(x))) # \u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u8fde\u7ed3\u8f93\u51fa return torch.cat((p1, p2, p3, p4), dim=1) \u8d85\u53c2\u6570\u4e3b\u8981\u4e3a\u901a\u9053\u6570,\u6bd4\u5982 Inception(192, 64, (96, 128), (16, 32), 32) \u8868\u793a\u4e00\u4e2aInception\u7684\u8f93\u5165\u901a\u9053\u6570\u4e3a192\uff0c\u8f93\u51fa\u901a\u9053\u6570\u4e3a 64 + 128 + 32 + 32 = 512 \u7f51\u7edc\u6574\u4f53\u7ed3\u6784\u7565 Batch-Norm $$\\mathrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}} \\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}} \\mathcal{B}} + \\boldsymbol{\\beta}.$$ \u5f52\u4e00\u5316\u540e\u7684\u65b9\u5dee\u4e3a$\\gamma$\uff0c\u5747\u503c\u4e3a$1 + \\beta$ $\\gamma,\\beta$ \u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570 \u5377\u79ef\u5c42\u6709\u591a\u4e2a\u901a\u9053\u65f6\uff0c\u5bf9\u6bcf\u4e2a\u901a\u9053\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u5047\u8bbebatch_size\u4e3am\uff0c\u5377\u79ef\u5c42\u8f93\u51fa\u7684\u5927\u5c0f\u4e3ap$\\times$q,\u5728\u6bcf\u4e2a\u8f93\u51fa\u901a\u9053\u4e0a\u7684mqp\u4e2a\u5143\u7d20\u4e0a\u6c42\u5747\u503c\u548c\u65b9\u5dee def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum): ''' moving_mean\u548cmoving_var\u8fd1\u4f3c\u8ba4\u4e3a\u5168\u5c40\u503c eps\u9632\u6b62\u65b9\u5dee\u4e3a0\uff0c\u56fa\u5b9a\u503c gmma,beta,moving_mean,moving_var\u7684\u5f62\u72b6\u548cX\u76f8\u540c ''' # \u901a\u8fc7is_grad_enabled\u6765\u5224\u65ad\u5f53\u524d\u6a21\u5f0f\u662f\u8bad\u7ec3\u6a21\u5f0f\u8fd8\u662f\u9884\u6d4b\u6a21\u5f0f if not torch.is_grad_enabled(): # \u5982\u679c\u662f\u5728\u9884\u6d4b\u6a21\u5f0f\u4e0b\uff0c\u76f4\u63a5\u4f7f\u7528\u4f20\u5165\u7684\u79fb\u52a8\u5e73\u5747\u6240\u5f97\u7684\u5747\u503c\u548c\u65b9\u5dee X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps) else: assert len(X.shape) in (2, 4) if len(X.shape) == 2: # \u4f7f\u7528\u5168\u8fde\u63a5\u5c42\u7684\u60c5\u51b5\uff0c\u8ba1\u7b97\u7279\u5f81\u7ef4\u4e0a\u7684\u5747\u503c\u548c\u65b9\u5dee mean = X.mean(dim=0) var = ((X - mean) ** 2).mean(dim=0) else: # \u4f7f\u7528\u4e8c\u7ef4\u5377\u79ef\u5c42\u7684\u60c5\u51b5\uff0c\u8ba1\u7b97\u901a\u9053\u7ef4\u4e0a\uff08axis=1\uff09\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002 # \u8fd9\u91cc\u6211\u4eec\u9700\u8981\u4fdd\u6301X\u7684\u5f62\u72b6\u4ee5\u4fbf\u540e\u9762\u53ef\u4ee5\u505a\u5e7f\u64ad\u8fd0\u7b97 mean = X.mean(dim=(0, 2, 3), keepdim=True) var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True) # \u8bad\u7ec3\u6a21\u5f0f\u4e0b\uff0c\u7528\u5f53\u524d\u7684\u5747\u503c\u548c\u65b9\u5dee\u505a\u6807\u51c6\u5316 X_hat = (X - mean) / torch.sqrt(var + eps) # \u66f4\u65b0\u79fb\u52a8\u5e73\u5747\u7684\u5747\u503c\u548c\u65b9\u5dee moving_mean = momentum * moving_mean + (1.0 - momentum) * mean moving_var = momentum * moving_var + (1.0 - momentum) * var Y = gamma * X_hat + beta # \u7f29\u653e\u548c\u79fb\u4f4d return Y, moving_mean.data, moving_var.data class BatchNorm(nn.Module): # num_features\uff1a\u5b8c\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\u6570\u91cf\u6216\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u3002 # num_dims\uff1a2\u8868\u793a\u5b8c\u5168\u8fde\u63a5\u5c42\uff0c4\u8868\u793a\u5377\u79ef\u5c42 def __init__(self, num_features, num_dims): super().__init__() if num_dims == 2: shape = (1, num_features) else: shape = (1, num_features, 1, 1) # \u53c2\u4e0e\u6c42\u68af\u5ea6\u548c\u8fed\u4ee3\u7684\u62c9\u4f38\u548c\u504f\u79fb\u53c2\u6570\uff0c\u5206\u522b\u521d\u59cb\u5316\u62101\u548c0 self.gamma = nn.Parameter(torch.ones(shape)) self.beta = nn.Parameter(torch.zeros(shape)) # \u975e\u6a21\u578b\u53c2\u6570\u7684\u53d8\u91cf\u521d\u59cb\u5316\u4e3a0\u548c1 self.moving_mean = torch.zeros(shape) self.moving_var = torch.ones(shape) def forward(self, X): # \u5982\u679cX\u4e0d\u5728\u5185\u5b58\u4e0a\uff0c\u5c06moving_mean\u548cmoving_var # \u590d\u5236\u5230X\u6240\u5728\u663e\u5b58\u4e0a if self.moving_mean.device != X.device: self.moving_mean = self.moving_mean.to(X.device) self.moving_var = self.moving_var.to(X.device) # \u4fdd\u5b58\u66f4\u65b0\u8fc7\u7684moving_mean\u548cmoving_var Y, self.moving_mean, self.moving_var = batch_norm( X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9) return Y pytorch\u4f7f\u7528batch_norm: nn.BatchNorm1d(),nn.BatchNorm2d() \uff0c\u53c2\u6570\u4e3a\u901a\u9053\u6570\uff0c\u5206\u522b\u8868\u793a\u5168\u8fde\u63a5\u5c42\u548c\u5377\u79ef\u5c42 ResNet \u8f93\u5165X\u548c\u8f93\u51faY\u7684\u5f62\u72b6\u8981\u76f8\u540c,3$\\times$3\u7684\u5377\u79ef\u6838\u6ca1\u6709\u6539\u53d8\u5f62\u72b6\uff0c\u4e3b\u8981\u662f\u901a\u9053\u6570\uff0c\u4e5f\u53ef\u4ee5\u75281$\\times$1\u7684\u5377\u79ef\u6838\u4fee\u6539X\u7684\u901a\u9053\u6570 class Residual(nn.Module): def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1): super().__init__() self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides) self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1) if use_1x1conv: self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides) else: self.conv3 = None self.bn1 = nn.BatchNorm2d(num_channels) self.bn2 = nn.BatchNorm2d(num_channels) def forward(self, X): Y = F.relu(self.bn1(self.conv1(X))) Y = self.bn2(self.conv2(Y)) if self.conv3: X = self.conv3(X) Y += X return F.relu(Y) DenseNet \u7f51\u7edc\u4e3b\u8981\u7531\u4e24\u90e8\u5206 \u7a20\u5bc6\u5c42\u548c\u8fc7\u6e21\u5c42 \u628a\u524d\u9762\u7f51\u7edc\u5c42\u7684\u8f93\u51fa\u90fd\u4f5c\u4e3a\u8f93\u51fa\uff0c\u6bcf\u4e2a\u7f51\u7edc\u7684\u8f93\u5165\u4e3a\u524d\u9762\u6240\u6709\u7f51\u7edc\u5c42\u7684\u8f93\u51fa\u52a0\u4e0ainput def conv_block(input_channels, num_channels): return nn.Sequential( nn.BatchNorm2d(input_channels), nn.ReLU(), nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1)) class DenseBlock(nn.Module): def __init__(self, num_convs, input_channels, num_channels): super(DenseBlock, self).__init__() layer = [] for i in range(num_convs): layer.append(conv_block( num_channels * i + input_channels, num_channels)) self.net = nn.Sequential(*layer) def forward(self, X): for blk in self.net: Y = blk(X) # \u8fde\u63a5\u901a\u9053\u7ef4\u5ea6\u4e0a\u6bcf\u4e2a\u5757\u7684\u8f93\u5165\u548c\u8f93\u51fa X = torch.cat((X, Y), dim=1) return X \u901a\u8fc7\u8fc7\u6e21\u5c42\u901a\u9053\u6570\u51cf\u5c11\uff0c\u9ad8\u548c\u5bbd\u51cf\u534a def transition_block(input_channels, num_channels): return nn.Sequential( nn.BatchNorm2d(input_channels), nn.ReLU(), nn.Conv2d(input_channels, num_channels, kernel_size=1), nn.AvgPool2d(kernel_size=2, stride=2)) RNN \u5e8f\u5217\u6a21\u578b \u901a\u8fc7$\\mathbf{x} t = [x {t-\\tau}, \\ldots, x_{t-1}]$ \u7684\u53d6\u503c\uff0c\u9884\u6d4b$y_t = x_t$\u3002 \u6587\u672c\u9884\u5904\u7406 \u8bfb\u53d6\u6570\u636e\u96c6\u5f97\u5230\u6240\u6709\u6587\u672c\u884c\uff08lines\uff09 the time machine by h g wells \u8bcd\u5143\u5316\uff08tokenize\uff09 \u628alines\u5206\u6210\u4e00\u4e2a\u4e00\u4e2a\u8bcd ['the', 'time', 'machine', 'by', 'h', 'g', 'wells'] \u5efa\u7acb\u8bcd\u8868\uff08vocabulary\uff09 class Vocab: \"\"\"\u6587\u672c\u8bcd\u8868\"\"\" def __init__(self, tokens=None, min_freq=0, reserved_tokens=None): if tokens is None: tokens = [] if reserved_tokens is None: reserved_tokens = [] # \u6309\u51fa\u73b0\u9891\u7387\u6392\u5e8f counter = count_corpus(tokens) self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # \u672a\u77e5\u8bcd\u5143\u7684\u7d22\u5f15\u4e3a0 self.idx_to_token = ['<unk>'] + reserved_tokens self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)} for token, freq in self._token_freqs: if freq < min_freq: break if token not in self.token_to_idx: self.idx_to_token.append(token) self.token_to_idx[token] = len(self.idx_to_token) - 1 def __len__(self): return len(self.idx_to_token) def __getitem__(self, tokens):#\u7ed9token\u8fd4\u56deindex if not isinstance(tokens, (list, tuple)): return self.token_to_idx.get(tokens, self.unk) return [self.__getitem__(token) for token in tokens] def to_tokens(self, indices):#\u7ed9index\u8fd4\u56detoken if not isinstance(indices, (list, tuple)): return self.idx_to_token[indices] return [self.idx_to_token[index] for index in indices] @property def unk(self): # \u672a\u77e5\u8bcd\u5143\u7684\u7d22\u5f15\u4e3a0 return 0 @property def token_freqs(self): return self._token_freqs def count_corpus(tokens): #@save \"\"\"\u7edf\u8ba1\u8bcd\u5143\u7684\u9891\u7387\"\"\" # \u8fd9\u91cc\u7684tokens\u662f1D\u5217\u8868\u62162D\u5217\u8868 if len(tokens) == 0 or isinstance(tokens[0], list): # \u5c06\u8bcd\u5143\u5217\u8868\u5c55\u5e73\u6210\u4e00\u4e2a\u5217\u8868 tokens = [token for line in tokens for token in line] return collections.Counter(tokens) \u8f6c\u6362\u7ed3\u679c\uff08\u8bcd\u9891\u9ad8\u4e0b\u6807\u5c0f\uff09 vocab = Vocab(tokens) print(list(vocab.token_to_idx.items())[:10]) [(' ', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)] Language Model n\u5143\u8bed\u6cd5 $$ \\begin{aligned} P(x_1, x_2, x_3, x_4) &= P(x_1) P(x_2) P(x_3) P(x_4),\\ P(x_1, x_2, x_3, x_4) &= P(x_1) P(x_2 \\mid x_1) P(x_3 \\mid x_2) P(x_4 \\mid x_3),\\ P(x_1, x_2, x_3, x_4) &= P(x_1) P(x_2 \\mid x_1) P(x_3 \\mid x_1, x_2) P(x_4 \\mid x_2, x_3). \\end{aligned} $$ \u5f97\u5230\u4e8c\u5143\u8bcd bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])] bigram_vocab = d2l.Vocab(bigram_tokens) \u7b2c$i$\u4e2a\u6700\u5e38\u7528\u7684\u9891\u7387$n_i$ $$\\log n_i = -\\alpha \\log i + c,$$ \u867d\u7136n\u5143\u8bcd\u7ec4\u5408\u65b9\u5f0f\u589e\u52a0\uff0c\u4f46\u662f\u7531\u4e8e\u5927\u81f4\u9075\u5faa\u4e0a\u8ff0\u89c4\u5f8b\uff0c\u5c06\u8bcd\u9891\u5c0f\u4e8e\u67d0\u4e2a\u503c\u7684\u7565\u53bb\uff0c\u8bcd\u8868\u4e2d\u8bb0\u5f55\u53cd\u800c\u66f4\u5c11 \u8bfb\u53d6\u957f\u5e8f\u5217\u6570\u636e \u968f\u673a\u91c7\u6837 def seq_data_iter_random(corpus, batch_size, num_steps): \"\"\"\u4f7f\u7528\u968f\u673a\u62bd\u6837\u751f\u6210\u4e00\u4e2a\u5c0f\u6279\u91cf\u5b50\u5e8f\u5217\"\"\" # \u4ece\u968f\u673a\u504f\u79fb\u91cf\u5f00\u59cb\u5bf9\u5e8f\u5217\u8fdb\u884c\u5206\u533a\uff0c\u968f\u673a\u8303\u56f4\u5305\u62ecnum_steps-1 corpus = corpus[random.randint(0, num_steps - 1):] # \u51cf\u53bb1\uff0c\u662f\u56e0\u4e3a\u6211\u4eec\u9700\u8981\u8003\u8651\u6807\u7b7e num_subseqs = (len(corpus) - 1) // num_steps # \u957f\u5ea6\u4e3anum_steps\u7684\u5b50\u5e8f\u5217\u7684\u8d77\u59cb\u7d22\u5f15 initial_indices = list(range(0, num_subseqs * num_steps, num_steps)) # \u5728\u968f\u673a\u62bd\u6837\u7684\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\uff0c # \u6765\u81ea\u4e24\u4e2a\u76f8\u90bb\u7684\u3001\u968f\u673a\u7684\u3001\u5c0f\u6279\u91cf\u4e2d\u7684\u5b50\u5e8f\u5217\u4e0d\u4e00\u5b9a\u5728\u539f\u59cb\u5e8f\u5217\u4e0a\u76f8\u90bb random.shuffle(initial_indices) def data(pos): # \u8fd4\u56de\u4ecepos\u4f4d\u7f6e\u5f00\u59cb\u7684\u957f\u5ea6\u4e3anum_steps\u7684\u5e8f\u5217 return corpus[pos: pos + num_steps] num_batches = num_subseqs // batch_size for i in range(0, batch_size * num_batches, batch_size): # \u5728\u8fd9\u91cc\uff0cinitial_indices\u5305\u542b\u5b50\u5e8f\u5217\u7684\u968f\u673a\u8d77\u59cb\u7d22\u5f15 initial_indices_per_batch = initial_indices[i: i + batch_size] X = [data(j) for j in initial_indices_per_batch] Y = [data(j + 1) for j in initial_indices_per_batch] yield torch.tensor(X), torch.tensor(Y) \u751f\u6210\u6837\u4f8b(0-34\u5e8f\u5217,batch_size=2,num_steps=5) X: tensor([ [13, 14, 15, 16, 17], [28, 29, 30, 31, 32] ]) Y: tensor([ [14, 15, 16, 17, 18], [29, 30, 31, 32, 33] ]) X: tensor([ [ 3, 4, 5, 6, 7], [18, 19, 20, 21, 22] ]) Y: tensor([ [ 4, 5, 6, 7, 8], [19, 20, 21, 22, 23] ]) X: tensor([ [ 8, 9, 10, 11, 12], [23, 24, 25, 26, 27] ]) Y: tensor([ [ 9, 10, 11, 12, 13], [24, 25, 26, 27, 28] ]) \u987a\u5e8f\u91c7\u6837 def seq_data_iter_sequential(corpus, batch_size, num_steps): \"\"\"\u4f7f\u7528\u987a\u5e8f\u5206\u533a\u751f\u6210\u4e00\u4e2a\u5c0f\u6279\u91cf\u5b50\u5e8f\u5217\"\"\" # \u4ece\u968f\u673a\u504f\u79fb\u91cf\u5f00\u59cb\u5212\u5206\u5e8f\u5217 offset = random.randint(0, num_steps) num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size Xs = torch.tensor(corpus[offset: offset + num_tokens]) Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens]) Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1) num_batches = Xs.shape[1] // num_steps for i in range(0, num_steps * num_batches, num_steps): X = Xs[:, i: i + num_steps] Y = Ys[:, i: i + num_steps] yield X, Y \u751f\u6210\u6837\u4f8b X: tensor([ [ 0, 1, 2, 3, 4], [17, 18, 19, 20, 21] ]) Y: tensor([ [ 1, 2, 3, 4, 5], [18, 19, 20, 21, 22] ]) X: tensor([ [ 5, 6, 7, 8, 9], [22, 23, 24, 25, 26] ]) Y: tensor([ [ 6, 7, 8, 9, 10], [23, 24, 25, 26, 27] ]) X: tensor([ [10, 11, 12, 13, 14], [27, 28, 29, 30, 31] ]) Y: tensor([ [11, 12, 13, 14, 15], [28, 29, 30, 31, 32] ]) RNN\u5b9e\u73b0 \u66f4\u65b0\u9690\u85cf\u72b6\u6001 $$ h_t = \\phi(W_{hh} h_{t-1} + W_{hx}\\mathbf{x} {t-1}+b_h) $$ \u8f93\u51fa $$ o_t = \\phi(W {ho} h_t + b_o) $$ \u56f0\u60d1\u5ea6(perplexity) \u8861\u91cf\u8bed\u8a00\u6a21\u578b\u7684\u597d\u574f\u53ef\u4ee5\u7528\u5e73\u5747\u4ea4\u53c9\u71b5 $$\\pi = \\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1),$$ \u56f0\u60d1\u5ea6\u4e3a$\\exp(\\pi)$ \u68af\u5ea6\u526a\u88c1 \u9884\u9632\u68af\u5ea6\u7206\u70b8 \u68af\u5ea6\u957f\u5ea6\u8d85\u8fc7$\\theta$\uff0c\u90a3\u4e48\u62d6\u56de\u957f\u5ea6$\\theta$ $$ g \\leftarrow \\min(1,\\frac{\\theta}{\\Vert \\mathbf{g} \\Vert}) \\mathbf{g} $$ \u53c2\u6570\u521d\u59cb\u5316 : def get_params(vocab_size, num_hiddens, device): num_inputs = num_outputs = vocab_size #\u56e0\u4e3a\u8fd9\u91cc\u7528\u7684onehot\u7801 def normal(shape): return torch.randn(size=shape, device=device) * 0.01 # \u9690\u85cf\u5c42\u53c2\u6570 W_xh = normal((num_inputs, num_hiddens)) W_hh = normal((num_hiddens, num_hiddens)) b_h = torch.zeros(num_hiddens, device=device) # \u8f93\u51fa\u5c42\u53c2\u6570 W_hq = normal((num_hiddens, num_outputs)) b_q = torch.zeros(num_outputs, device=device) # \u9644\u52a0\u68af\u5ea6 params = [W_xh, W_hh, b_h, W_hq, b_q] for param in params: param.requires_grad_(True) return params #\u521d\u59cb\u5316\u9690\u85cf\u72b6\u6001 def init_rnn_state(batch_size, num_hiddens, device): return (torch.zeros((batch_size, num_hiddens), device=device), ) $X$\u7684\u5f62\u72b6\u4e3a\uff08batch_size,vocab_size\uff09\uff0c\u6267\u884c\u64cd\u4f5c $$ H_{b \\times h} = \\phi(X_{b \\times x}W_{x \\times h} + H_{b \\times h}W_{h \\times h } + b_h(Broadcast)) $$ \u53ef\u4ee5\u770b\u51fa\u5bf9\u4e8e\u6bcf\u4e2a\u6279\u91cf\u7684\u9690\u72b6\u6001\u662f\u72ec\u7acb\u5b58\u50a8\u66f4\u65b0\u7684 \u8f93\u51fa$Y$\u7684\u5f62\u72b6\u4e3a(batch_size,vocab_size)\uff0ccat\u5b8c\u4e86return\u7684\u5f62\u72b6\u4e3a(time_steps$\\times$batch_size,vocab_size) def rnn(inputs, state, params): # inputs\u7684\u5f62\u72b6\uff1a(\u65f6\u95f4\u6b65\u6570\u91cf\uff0c\u6279\u91cf\u5927\u5c0f\uff0c\u8bcd\u8868\u5927\u5c0f) W_xh, W_hh, b_h, W_hq, b_q = params H, = state outputs = [] # X\u7684\u5f62\u72b6\uff1a(\u6279\u91cf\u5927\u5c0f\uff0c\u8bcd\u8868\u5927\u5c0f) for X in inputs: H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh)+b_h)#torch.mm\u8868\u793a\u77e9\u9635\u4e58\u8d77\u6765 Y = torch.mm(H, W_hq) + b_q outputs.append(Y) return torch.cat(outputs, dim=0), (H,) \u5b8c\u6574\u7684\u5982\u4e0b class RNNModelScratch: def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn): self.vocab_size, self.num_hiddens = vocab_size, num_hiddens self.params = get_params(vocab_size, num_hiddens, device) self.init_state, self.forward_fn = init_state, forward_fn def __call__(self, X, state): X = F.one_hot(X.T, self.vocab_size).type(torch.float32) return self.forward_fn(X, state, self.params) def begin_state(self, batch_size, device): return self.init_state(batch_size, self.num_hiddens, device) num_hiddens = 512 net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn) state = net.begin_state(X.shape[0], d2l.try_gpu()) Y, new_state = net(X.to(d2l.try_gpu()), state) \u4f7f\u7528\u7f51\u7edc\u8fdb\u884c\u9884\u6d4b\uff0c\u524d\u51e0\u4e2a\uff08prefix\uff09\u4e0d\u4ea7\u751f\u8f93\u51fa\u4f46\u662f\u66f4\u65b0\u9690\u72b6\u6001 def predict_ch8(prefix, num_preds, net, vocab, device): #@save \"\"\"\u5728prefix\u540e\u9762\u751f\u6210\u65b0\u5b57\u7b26\"\"\" state = net.begin_state(batch_size=1, device=device) outputs = [vocab[prefix[0]]] get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1)) #\u7528\u4e0a\u6b21\u8f93\u51fa\u7684\u6700\u540e\u7684\u5b57\u7b26 #\u4e00\u4e2a\u4e00\u4e2a\u8fdb\u53bb\u5f97\u5230\u8f93\u51fa for y in prefix[1:]: # \u9884\u70ed\u671f _, state = net(get_input(), state) outputs.append(vocab[y]) for _ in range(num_preds): # \u9884\u6d4bnum_preds\u6b65 y, state = net(get_input(), state) outputs.append(int(y.argmax(dim=1).reshape(1))) return ''.join([vocab.idx_to_token[i] for i in outputs]) \u8bad\u7ec3\u793a\u4f8b def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter): state, timer = None, d2l.Timer() metric = d2l.Accumulator(2) # \u8bad\u7ec3\u635f\u5931\u4e4b\u548c,\u8bcd\u5143\u6570\u91cf for X, Y in train_iter: if state is None or use_random_iter: # \u5728\u7b2c\u4e00\u6b21\u8fed\u4ee3\u6216\u4f7f\u7528\u968f\u673a\u62bd\u6837\u65f6\u8981\u91cd\u65b0\u521d\u59cb\u5316state state = net.begin_state(batch_size=X.shape[0], device=device) else: if isinstance(net, nn.Module) and not isinstance(state, tuple): # state\u5bf9\u4e8enn.GRU\u662f\u4e2a\u5f20\u91cf state.detach_() else: for s in state: s.detach_() y = Y.T.reshape(-1) X, y = X.to(device), y.to(device) y_hat, state = net(X, state) l = loss(y_hat, y.long()).mean() #\u8fd9\u91ccy_hat\u4e3a\u4e8c\u7ef4\u5f20\u91cf\uff0cy\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u7c7b\u4f3c\u4e8e\u591a\u5206\u7c7b\u95ee\u9898 if isinstance(updater, torch.optim.Optimizer): updater.zero_grad() l.backward() grad_clipping(net, 1)#\u68af\u5ea6\u526a\u88c1 updater.step() else: l.backward() grad_clipping(net, 1) # \u56e0\u4e3a\u5df2\u7ecf\u8c03\u7528\u4e86mean\u51fd\u6570 updater(batch_size=1) metric.add(l * y.numel(), y.numel())#numel\u51fd\u6570\u8fd4\u56de\u5f20\u91cf\u5143\u7d20\u603b\u6570\u91cf\uff0c return math.exp(metric[0] / metric[1]), metric[1] / timer.stop() \u4f7f\u7528pytorch API rnn_layer = nn.RNN(len(vocab), num_hiddens) state = torch.zeros((1, batch_size, num_hiddens))#(\u9690\u85cf\u5c42\u6570\uff0c\u6279\u91cf\u5927\u5c0f\uff0c\u9690\u53d8\u91cf\u957f\u5ea6) X = torch.rand(size=(num_steps, batch_size, len(vocab))) Y, state_new = rnn_layer(X, state) \u53cd\u5411\u4f20\u64ad \u4e3b\u8981\u662f\u5faa\u73af\u8ba1\u7b97\u68af\u5ea6\u7684\u65b9\u6cd5\u548c\u8fd1\u4f3c\u65b9\u6cd5\uff0ct\u5927\u65f6\u4ea7\u751f\u68af\u5ea6\u6d88\u5931\u6216\u8005\u7206\u70b8\u7684\u95ee\u9898 \u5177\u4f53\u53c2\u8003\u52a8\u624b\u6df1\u5ea6\u5b66\u4e60bptt\u7ae0\u8282 GRU \u91cd\u7f6e\u95e8\u548c\u66f4\u65b0\u95e8($\\mathbb{R} {b \\times h}$)\u66f4\u65b0 $$ \\begin{aligned} \\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W} {xr} + \\mathbf{H} {t-1} \\mathbf{W} {hr} + \\mathbf{b} r),\\ \\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W} {xz} + \\mathbf{H} {t-1} \\mathbf{W} {hz} + \\mathbf{b}_z), \\end{aligned} $$ ($\\sigma \u4e3a $sigmoid\u51fd\u6570) \u9690\u5019\u9009\u72b6\u6001 $$\\tilde{\\mathbf{H}} t = \\tanh(\\mathbf{X}_t \\mathbf{W} {xh} + \\left(\\mathbf{R} t \\odot \\mathbf{H} {t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h),$$ \u9690\u72b6\u6001\u66f4\u65b0 $$\\mathbf{H} t = \\mathbf{Z}_t \\odot \\mathbf{H} {t-1} + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.$$ \u76f8\u6bd4\u524d\u9762\u57fa\u672cRNN\u53ea\u662f\u9690\u72b6\u6001\u66f4\u65b0\u516c\u5f0f\u66f4\u4e3a\u590d\u6742 pytorch\u6846\u67b6 num_inputs = vocab_size gru_layer = nn.GRU(num_inputs, num_hiddens) model = d2l.RNNModel(gru_layer, len(vocab)) d2l.RNNModel class RNNModel(nn.Module): \"\"\"\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\"\"\" def __init__(self, rnn_layer, vocab_size, **kwargs): super(RNNModel, self).__init__(**kwargs) self.rnn = rnn_layer self.vocab_size = vocab_size self.num_hiddens = self.rnn.hidden_size # \u5982\u679cRNN\u662f\u53cc\u5411\u7684\uff08\u4e4b\u540e\u5c06\u4ecb\u7ecd\uff09\uff0cnum_directions\u5e94\u8be5\u662f2\uff0c\u5426\u5219\u5e94\u8be5\u662f1 if not self.rnn.bidirectional: self.num_directions = 1 self.linear = nn.Linear(self.num_hiddens, self.vocab_size) else: self.num_directions = 2 self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size) def forward(self, inputs, state): X = F.one_hot(inputs.T.long(), self.vocab_size) X = X.to(torch.float32) Y, state = self.rnn(X, state) # \u5168\u8fde\u63a5\u5c42\u9996\u5148\u5c06Y\u7684\u5f62\u72b6\u6539\u4e3a(\u65f6\u95f4\u6b65\u6570*\u6279\u91cf\u5927\u5c0f,\u9690\u85cf\u5355\u5143\u6570) # \u5b83\u7684\u8f93\u51fa\u5f62\u72b6\u662f(\u65f6\u95f4\u6b65\u6570*\u6279\u91cf\u5927\u5c0f,\u8bcd\u8868\u5927\u5c0f)\u3002 output = self.linear(Y.reshape((-1, Y.shape[-1]))) return output, state def begin_state(self, device, batch_size=1): if not isinstance(self.rnn, nn.LSTM): # nn.GRU\u4ee5\u5f20\u91cf\u4f5c\u4e3a\u9690\u72b6\u6001 return torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device) else: # nn.LSTM\u4ee5\u5143\u7ec4\u4f5c\u4e3a\u9690\u72b6\u6001 return (torch.zeros(( self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device), torch.zeros(( self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device)) LSTM \u8f93\u5165\u95e8\u662f$\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}$\uff0c \u9057\u5fd8\u95e8\u662f$\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}$\uff0c \u8f93\u51fa\u95e8\u662f$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}$\u3002 \u66f4\u65b0\u516c\u5f0f\u4e3a $$ \\begin{aligned} \\mathbf{I} t &= \\sigma(\\mathbf{X}_t \\mathbf{W} {xi} + \\mathbf{H} {t-1} \\mathbf{W} {hi} + \\mathbf{b} i),\\ \\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W} {xf} + \\mathbf{H} {t-1} \\mathbf{W} {hf} + \\mathbf{b} f),\\ \\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W} {xo} + \\mathbf{H} {t-1} \\mathbf{W} {ho} + \\mathbf{b}_o), \\end{aligned} $$ \u5019\u9009\u8bb0\u5fc6\u5143 $$\\tilde{\\mathbf{C}} t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W} {xc} + \\mathbf{H} {t-1} \\mathbf{W} {hc} + \\mathbf{b}_c),$$ \u8bb0\u5fc6\u5143 $$\\mathbf{C} t = \\mathbf{F}_t \\odot \\mathbf{C} {t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t.$$ \u9690\u72b6\u6001 $$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t).$$ pytorch\u6846\u67b6 num_inputs = vocab_size lstm_layer = nn.LSTM(num_inputs, num_hiddens) model = d2l.RNNModel(lstm_layer, len(vocab)) Deep RNN \u8bbe\u7f6e$\\mathbf{H}_t^{(0)} = \\mathbf{X}_t$\uff0c \u7b2c$l$\u5c42\u7684\u9690\u72b6\u6001\u66f4\u65b0 $$\\mathbf{H} t^{(l)} = \\phi_l(\\mathbf{H}_t^{(l-1)} \\mathbf{W} {xh}^{(l)} + \\mathbf{H} {t-1}^{(l)} \\mathbf{W} {hh}^{(l)} + \\mathbf{b}_h^{(l)}),$$ \u6700\u540e\uff0c\u8f93\u51fa\u5c42\u7684\u8ba1\u7b97\u4ec5\u57fa\u4e8e\u7b2c$l$\u4e2a\u9690\u85cf\u5c42\u6700\u7ec8\u7684\u9690\u72b6\u6001\uff1a $$\\mathbf{O} t = \\mathbf{H}_t^{(L)} \\mathbf{W} {hq} + \\mathbf{b}_q,$$ \u4e5f\u80fd\u7528GRU\u6216\u8005LSTM\u6765\u4ee3\u66ff\u8fd9\u91cc\u7684\u9690\u72b6\u6001 pytorch\u5b9e\u73b0\u6df1\u5ea6\u7684LSTM lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers) model = d2l.RNNModel(lstm_layer, len(vocab)) BRNN $$ \\begin{aligned} \\overrightarrow{\\mathbf{H}} t &= \\phi(\\mathbf{X}_t \\mathbf{W} {xh}^{(f)} + \\overrightarrow{\\mathbf{H}} {t-1} \\mathbf{W} {hh}^{(f)} + \\mathbf{b} h^{(f)}),\\ \\overleftarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W} {xh}^{(b)} + \\overleftarrow{\\mathbf{H}} {t+1} \\mathbf{W} {hh}^{(b)} + \\mathbf{b}_h^{(b)}), \\end{aligned} $$ \u6b63\u5411\u9690\u72b6\u6001$\\overrightarrow{\\mathbf{H}}_t$\u548c\u53cd\u5411\u9690\u72b6\u6001$\\overleftarrow{\\mathbf{H}}_t$\u8fde\u63a5\u8d77\u6765\uff0c\u83b7\u5f97\u9700\u8981\u9001\u5165\u8f93\u51fa\u5c42\u7684\u9690\u72b6\u6001$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}$\u3002 $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$\uff08$q$\u662f\u8f93\u51fa\u5355\u5143\u7684\u6570\u76ee\uff09: $$\\mathbf{O} t = \\mathbf{H}_t \\mathbf{W} {hq} + \\mathbf{b}_q.$$ \u8fd9\u91cc$\\mathbf{W}_{hq} \\in \\mathbb{R}^{2h \\times q}$ Seq2Seq \u673a\u5668\u7ffb\u8bd1 \u7f16\u7801\u5668\u662f\u4e00\u4e2aRNN(\u53ef\u4ee5\u662f\u53cc\u5411)\uff0c\u8bfb\u53d6\u8f93\u5165\u53e5\u5b50 \u89e3\u7801\u5668\u7528\u53e6\u4e00\u4e2aRNN\u6765\u8f93\u51fa \u7f16\u7801\u5668\u6700\u540e\u65f6\u95f4\u6b65\u7684\u9690\u72b6\u6001\u7528\u4f5c\u7f16\u7801\u5668\u7684\u521d\u59cb\u9690\u72b6\u6001 \u8bad\u7ec3\u65f6\u89e3\u7801\u5668\u4f7f\u7528\u76ee\u6807\u53e5\u5b50\u4f5c\u4e3a\u8f93\u5165\uff08\u5c31\u7b97\u67d0\u4e00\u6b65\u9884\u6d4b\u9519\u4e86\uff0c\u4e0b\u4e00\u6b65\u8f93\u5165\u7684\u8fd8\u662f\u6b63\u786e\u7684\u7ffb\u8bd1\u7ed3\u679c\uff09 \u9884\u6d4b\u5e8f\u5217\u8bc4\u4f30 \u8861\u91cf\u751f\u6210\u5e8f\u5217\u597d\u574f\u7684BLEU $$ \\exp\\left(\\min\\left(0, 1 - \\frac{\\mathrm{len} {\\text{label}}}{\\mathrm{len} {\\text{pred}}}\\right)\\right) \\prod_{n=1}^k p_n^{1/2^n},$$ $p_n$\u8868\u793a\u9884\u6d4b\u4e2d\u6240\u6709n_gram\uff08n\u5143\u8bed\u6cd5\uff09\u7684\u7cbe\u5ea6 Example \u6807\u7b7e\u5e8f\u5217ABCDEF\uff0c\u9884\u6d4b\u5e8f\u5217\u4e3aABBCD $p_1$ = 4/5 $p_2$ = 3/4 $p_3$ = 1/3 $p_4$ = 0 class Seq2SeqEncoder(d2l.Encoder): def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqEncoder, self).__init__(**kwargs) # \u5d4c\u5165\u5c42 self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout) def forward(self, X, *args): # \u8f93\u5165'X'\u5f62\u72b6(batch_size,num_steps) \u8f93\u51fa'X'\u7684\u5f62\u72b6\uff1a(batch_size,num_steps,embed_size) X = self.embedding(X) # \u5728\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\uff0c\u7b2c\u4e00\u4e2a\u8f74\u5bf9\u5e94\u4e8e\u65f6\u95f4\u6b65 X = X.permute(1, 0, 2) # \u5982\u679c\u672a\u63d0\u53ca\u72b6\u6001\uff0c\u5219\u9ed8\u8ba4\u4e3a0 output, state = self.rnn(X) # output\u7684\u5f62\u72b6:(num_steps,batch_size,num_hiddens) # state\u7684\u5f62\u72b6:(num_layers,batch_size,num_hiddens) return output, state Decoder\u7684embedding\u548cEncoder\u4e0d\u540c class Seq2SeqDecoder(d2l.Decoder): \"\"\"\u7528\u4e8e\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u89e3\u7801\u5668\"\"\" def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqDecoder, self).__init__(**kwargs) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, *args): #outputs:(output,state) return enc_outputs[1] def forward(self, X, state): # \u8f93\u51fa'X'\u7684\u5f62\u72b6\uff1a(batch_size,num_steps,embed_size) X = self.embedding(X).permute(1, 0, 2) # \u5e7f\u64adcontext\uff0c\u4f7f\u5176\u5177\u6709\u4e0eX\u76f8\u540c\u7684num_steps context = state[-1].repeat(X.shape[0], 1, 1)#\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u6700\u540e\u4e00\u5c42\u8f93\u51fa X_and_context = torch.cat((X, context), 2) output, state = self.rnn(X_and_context, state) output = self.dense(output).permute(1, 0, 2) # output\u7684\u5f62\u72b6:(batch_size,num_steps,vocab_size) # state\u7684\u5f62\u72b6:(num_layers,batch_size,num_hiddens) return output, state \u5728\u5e8f\u5217\u4e2d\u5c4f\u853d\u4e0d\u76f8\u5173\u7684\u9879 def sequence_mask(X, valid_len, value=0): maxlen = X.size(1) mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None] X[~mask] = value return X X = torch.tensor([[1, 2, 3], [4, 5, 6]]) sequence_mask(X, torch.tensor([1, 2])) tensor([ [1, 0, 0], [4, 5, 0] ]) class MaskedSoftmaxCELoss(nn.CrossEntropyLoss): \"\"\"\u5e26\u906e\u853d\u7684softmax\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\"\"\" # pred\u7684\u5f62\u72b6\uff1a(batch_size,num_steps,vocab_size) # label\u7684\u5f62\u72b6\uff1a(batch_size,num_steps) # valid_len\u7684\u5f62\u72b6\uff1a(batch_size,) def forward(self, pred, label, valid_len): weights = torch.ones_like(label) weights = sequence_mask(weights, valid_len) self.reduction='none' unweighted_loss = super(MaskedSoftmaxCELoss, self).forward( pred.permute(0, 2, 1), label) weighted_loss = (unweighted_loss * weights).mean(dim=1) #\u65e0\u6548\u7684\u5168\u90e8\u7f6e\u4e3a0\uff0c\u5bf9\u6bcf\u4e2a\u6837\u672c\u8fd4\u56deloss return weighted_loss \uff08\u5177\u4f53\u6ca1\u770b\u5f88\u61c2\u600e\u4e48\u7ec3\u7684\uff0c\u5148\u8fc7\u4e86\u540e\u9762\u8865 Attention \u6bcf\u4e2a\u503c\uff08Value\uff09\u548c\u4e00\u4e2a\u952e\uff08key\uff09\u4e00\u4e00\u5bf9\u5e94\uff0c\u901a\u8fc7\u67e5\u8be2\uff08query\uff09\u4e0e\u952e\u8fdb\u884c\u5339\u914d\uff0c\u5f97\u5230\u6700\u5339\u914d\u7684\u503c\u3002 Nadaraya-Waston \u6838\u56de\u5f52 $$f(x) = \\sum_{i=1}^n \\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)} y_i$$ \u66f4\u4e00\u822c\u7684\u8868\u793a $$f(x) = \\sum_{i=1}^n \\alpha(x, x_i) y_i,$$ \u7528\u9ad8\u65af\u6838$K(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{u^2}{2}).$\u5e26\u5165\u7b2c\u4e00\u4e2a\u5f0f\u5b50 \u5f97\u5230 $$\\begin{aligned} f(x) &= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}(x - x_i)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}(x - x_j)^2\\right)} y_i \\&= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}(x - x_i)^2\\right) y_i. \\end{aligned}$$ \u5e26\u5b66\u4e60\u53c2\u6570w $$\\begin{aligned}f(x) &= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}((x - x_i)w)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}((x - x_j)w)^2\\right)} y_i \\&= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}((x - x_i)w)^2\\right) y_i.\\end{aligned}$$ \u6ce8\u610f\u529b\u8bc4\u5206\u51fd\u6570 \u7528\u6570\u5b66\u8bed\u8a00\u63cf\u8ff0\uff0c\u5047\u8bbe\u6709\u4e00\u4e2a\u67e5\u8be2$\\mathbf{q} \\in \\mathbb{R}^q$\u548c$m$\u4e2a\u201c\u952e\uff0d\u503c\u201d\u5bf9$(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)$\uff0c \u5176\u4e2d$\\mathbf{k}_i \\in \\mathbb{R}^k$\uff0c$\\mathbf{v}_i \\in \\mathbb{R}^v$\u3002 \u6ce8\u610f\u529b\u6c47\u805a\u51fd\u6570$f$\u5c31\u88ab\u8868\u793a\u6210\u503c\u7684\u52a0\u6743\u548c\uff1a $$f(\\mathbf{q}, (\\mathbf{k} 1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)) = \\sum {i=1}^m \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i \\in \\mathbb{R}^v,$$ \u5176\u4e2d\u67e5\u8be2$\\mathbf{q}$\u548c\u952e$\\mathbf{k}_i$\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff08\u6807\u91cf\uff09\u662f\u901a\u8fc7\u6ce8\u610f\u529b\u8bc4\u5206\u51fd\u6570$a$\u5c06\u4e24\u4e2a\u5411\u91cf\u6620\u5c04\u6210\u6807\u91cf\uff0c\u518d\u7ecf\u8fc7softmax\u8fd0\u7b97\u5f97\u5230\u7684\uff1a $$\\alpha(\\mathbf{q}, \\mathbf{k} i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}_i))}{\\sum {j=1}^m \\exp(a(\\mathbf{q}, \\mathbf{k}_j))} \\in \\mathbb{R}.$$ Additive Attention $$a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R},$$ \u5176\u4e2d\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u662f$\\mathbf W_q\\in\\mathbb R^{h\\times q}$\u3001$\\mathbf W_k\\in\\mathbb R^{h\\times k}$\u548c$\\mathbf w_v\\in\\mathbb R^{h}$\u3002 class AdditiveAttention(nn.Module): def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs): super(AdditiveAttention, self).__init__(**kwargs) self.W_k = nn.Linear(key_size, num_hiddens, bias=False) self.W_q = nn.Linear(query_size, num_hiddens, bias=False) self.w_v = nn.Linear(num_hiddens, 1, bias=False) self.dropout = nn.Dropout(dropout) def forward(self, queries, keys, values, valid_lens): queries, keys = self.W_q(queries), self.W_k(keys) # \u5728\u7ef4\u5ea6\u6269\u5c55\u540e\uff0c # queries\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570\uff0c1\uff0cnum_hidden) # key\u7684\u5f62\u72b6\uff1a(batch_size\uff0c1\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0cnum_hiddens) # \u4f7f\u7528\u5e7f\u64ad\u65b9\u5f0f\u8fdb\u884c\u6c42\u548c features = queries.unsqueeze(2) + keys.unsqueeze(1) features = torch.tanh(features) # self.w_v\u4ec5\u6709\u4e00\u4e2a\u8f93\u51fa\uff0c\u56e0\u6b64\u4ece\u5f62\u72b6\u4e2d\u79fb\u9664\u6700\u540e\u90a3\u4e2a\u7ef4\u5ea6\u3002 # scores\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570\uff0c\u201c\u952e-\u503c\u201d\u5bf9\u7684\u4e2a\u6570) scores = self.w_v(features).squeeze(-1) self.attention_weights = masked_softmax(scores, valid_lens) # values\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0c\u503c\u7684\u7ef4\u5ea6) # \u8f93\u51fa\u7684\u5f62\u72b6\uff1a(batch_size,\u67e5\u8be2\u7684\u4e2a\u6570,\u503c\u7684\u7ef4\u5ea6) return torch.bmm(self.dropout(self.attention_weights), values) Scaled dot-product attention \u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b \uff08scaled dot-product attention\uff09\u8981\u6c42query\u548cKey\u957f\u5ea6\u76f8\u540c\uff0c\u8bc4\u5206\u51fd\u6570\u4e3a\uff1a $$a(\\mathbf q, \\mathbf k) = \\mathbf{q}^\\top \\mathbf{k} /\\sqrt{d}.$$ \u57fa\u4e8e$n$\u4e2a\u67e5\u8be2\u548c$m$\u4e2a\u952e\uff0d\u503c\u5bf9\u8ba1\u7b97\u6ce8\u610f\u529b\uff0c\u5176\u4e2d\u67e5\u8be2\u548c\u952e\u7684\u957f\u5ea6\u5747\u4e3a$d$\uff0c\u503c\u7684\u957f\u5ea6\u4e3a$v$\u3002\u67e5\u8be2$\\mathbf Q\\in\\mathbb R^{n\\times d}$\u3001\u952e$\\mathbf K\\in\\mathbb R^{m\\times d}$\u548c\u503c$\\mathbf V\\in\\mathbb R^{m\\times v}$\u7684\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u662f\uff1a $$ \\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}.$$ class DotProductAttention(nn.Module): \"\"\"\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\"\"\" def __init__(self, dropout, **kwargs): super(DotProductAttention, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) # queries\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570\uff0cd) # keys\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0cd) # values\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0c\u503c\u7684\u7ef4\u5ea6) # valid_lens\u7684\u5f62\u72b6:(batch_size\uff0c)\u6216\u8005(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570) def forward(self, queries, keys, values, valid_lens=None): d = queries.shape[-1] # \u8bbe\u7f6etranspose_b=True\u4e3a\u4e86\u4ea4\u6362keys\u7684\u6700\u540e\u4e24\u4e2a\u7ef4\u5ea6 scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d) self.attention_weights = masked_softmax(scores, valid_lens) return torch.bmm(self.dropout(self.attention_weights), values) Attention Decoder \u4e0e\u4e4b\u524dSeq2Seq\u76f8\u6bd4\uff0c\u4e0a\u4e0b\u6587\u53d8\u91cf$\\mathbf{c}$\u5728\u4efb\u4f55\u89e3\u7801\u65f6\u95f4\u6b65$t'$\u90fd\u4f1a\u88ab$\\mathbf{c}_{t'}$\u66ff\u6362\u3002\u5047\u8bbe\u8f93\u5165\u5e8f\u5217\u4e2d\u6709$T$\u4e2a\u8bcd\u5143\uff0c\u89e3\u7801\u65f6\u95f4\u6b65$t'$\u7684\u4e0a\u4e0b\u6587\u53d8\u91cf\u662f\u6ce8\u610f\u529b\u96c6\u4e2d\u7684\u8f93\u51fa\uff1a $$\\mathbf{c} {t'} = \\sum {t=1}^T \\alpha(\\mathbf{s}_{t' - 1}, \\mathbf{h}_t) \\mathbf{h}_t$$ \u5176\u4e2d\uff0c\u65f6\u95f4\u6b65$t' - 1$\u65f6\u7684\u89e3\u7801\u5668\u9690\u72b6\u6001$\\mathbf{s}_{t' - 1}$\u662f\u67e5\u8be2\uff0c\u7f16\u7801\u5668\u9690\u72b6\u6001$\\mathbf{h}_t$\u65e2\u662f\u952e\uff0c\u4e5f\u662f\u503c\u3002 class Seq2SeqAttentionDecoder(AttentionDecoder): def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqAttentionDecoder, self).__init__(**kwargs) self.attention = d2l.AdditiveAttention( num_hiddens, num_hiddens, num_hiddens, dropout) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU( embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, enc_valid_lens, *args): # outputs\u7684\u5f62\u72b6\u4e3a(batch_size\uff0cnum_steps\uff0cnum_hiddens). # hidden_state\u7684\u5f62\u72b6\u4e3a(num_layers\uff0cbatch_size\uff0cnum_hiddens) outputs, hidden_state = enc_outputs return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens) def forward(self, X, state): # enc_outputs\u7684\u5f62\u72b6\u4e3a(batch_size,num_steps,num_hiddens). # hidden_state\u7684\u5f62\u72b6\u4e3a(num_layers,batch_size, # num_hiddens) enc_outputs, hidden_state, enc_valid_lens = state # \u8f93\u51faX\u7684\u5f62\u72b6\u4e3a(num_steps,batch_size,embed_size) X = self.embedding(X).permute(1, 0, 2) outputs, self._attention_weights = [], [] for x in X: # query\u7684\u5f62\u72b6\u4e3a(batch_size,1,num_hiddens) query = torch.unsqueeze(hidden_state[-1], dim=1) # context\u7684\u5f62\u72b6\u4e3a(batch_size,1,num_hiddens) context = self.attention( query, enc_outputs, enc_outputs, enc_valid_lens) # \u5728\u7279\u5f81\u7ef4\u5ea6\u4e0a\u8fde\u7ed3 x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1) # \u5c06x\u53d8\u5f62\u4e3a(1,batch_size,embed_size+num_hiddens) out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state) outputs.append(out) self._attention_weights.append(self.attention.attention_weights) # \u5168\u8fde\u63a5\u5c42\u53d8\u6362\u540e\uff0coutputs\u7684\u5f62\u72b6\u4e3a # (num_steps,batch_size,vocab_size) outputs = self.dense(torch.cat(outputs, dim=0)) return outputs.permute(1, 0, 2), [enc_outputs, hidden_state, enc_valid_lens] @property def attention_weights(self): return self._attention_weights Mutihead-Attention \u7ed9\u5b9a\u67e5\u8be2$\\mathbf{q} \\in \\mathbb{R}^{d_q}$\u3001\u952e$\\mathbf{k} \\in \\mathbb{R}^{d_k}$\u548c\u503c$\\mathbf{v} \\in \\mathbb{R}^{d_v}$\uff0c\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934$\\mathbf{h}_i$\uff08$i = 1, \\ldots, h$\uff09\u7684\u8ba1\u7b97\u65b9\u6cd5\u4e3a\uff1a $$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},$$ \u5176\u4e2d\uff0c\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u5305\u62ec$\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}$\u3001$\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}$\u548c $\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}$\uff0c\u4ee5\u53ca\u4ee3\u8868\u6ce8\u610f\u529b\u6c47\u805a\u7684\u51fd\u6570$f$\u3002 \u591a\u5934\u6ce8\u610f\u529b\u7684\u8f93\u51fa\u9700\u8981\u7ecf\u8fc7\u53e6\u4e00\u4e2a\u7ebf\u6027\u8f6c\u6362\uff0c\u5b83\u5bf9\u5e94\u7740$h$\u4e2a\u5934\u8fde\u7ed3\u540e\u7684\u7ed3\u679c\uff0c\u53ef\u5b66\u4e60\u53c2\u6570\u662f$\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$\uff1a $$\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\vdots\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.$$ \u5728\u5b9e\u73b0\u8fc7\u7a0b\u4e2d\u901a\u5e38\u9009\u62e9\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u4f5c\u4e3a\u6bcf\u4e00\u4e2a\u6ce8\u610f\u529b\u5934\u3002 \u8bbe\u5b9a$p_q = p_k = p_v = p_o / h$\u3002\u5982\u679c\u5c06\u67e5\u8be2\u3001\u952e\u548c\u503c\u7684\u7ebf\u6027\u53d8\u6362\u7684\u8f93\u51fa\u6570\u91cf\u8bbe\u7f6e\u4e3a$p_q h = p_k h = p_v h = p_o$\uff0c\u5219\u53ef\u4ee5\u5e76\u884c\u8ba1\u7b97$h$\u4e2a\u5934\u3002 Self-Attention \u7ed9\u5b9a\u4e00\u4e2a\u7531\u8bcd\u5143\u7ec4\u6210\u7684\u8f93\u5165\u5e8f\u5217$\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$\uff0c\u5176\u4e2d\u4efb\u610f$\\mathbf{x}_i \\in \\mathbb{R}^d$\uff08$1 \\leq i \\leq n$\uff09\u3002\u8be5\u5e8f\u5217\u7684\u81ea\u6ce8\u610f\u529b\u8f93\u51fa\u4e3a\u4e00\u4e2a\u957f\u5ea6\u76f8\u540c\u7684\u5e8f\u5217 $\\mathbf{y}_1, \\ldots, \\mathbf{y}_n$\uff0c\u5176\u4e2d\uff1a $$\\mathbf{y}_i = f(\\mathbf{x}_i, (\\mathbf{x}_1, \\mathbf{x}_1), \\ldots, (\\mathbf{x}_n, \\mathbf{x}_n)) \\in \\mathbb{R}^d$$ Positional-Encoding \u7531\u4e8eSelf Attention \u6ca1\u6709\u8bb0\u5f55\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5047\u8bbe\u957f\u5ea6\u4e3an\u7684\u5e8f\u5217$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$,\u7528\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635$\\mathbf{P} \\in \\mathbb{R}^{n \\times d}$ \u6765\u8ba1\u7b97$\\mathbf{X} + \\mathbf{P}$ \u4f5c\u4e3aSelf Attention \u7684\u8f93\u5165\u3002 \u77e9\u9635$\\mathbf{P}$\u7b2c$i$\u884c\u3001\u7b2c$2j$\u5217\u548c$2j+1$\u5217\u4e0a\u7684\u5143\u7d20\u4e3a\uff1a $$\\begin{aligned} p_{i, 2j} &= \\sin\\left(\\frac{i}{10000^{2j/d}}\\right),\\p_{i, 2j+1} &= \\cos\\left(\\frac{i}{10000^{2j/d}}\\right).\\end{aligned}$$ \u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f $(p_{i, 2j}, p_{i, 2j+1})$\u90fd\u53ef\u4ee5\u7ebf\u6027\u6295\u5f71\u5230$(p_{i+\\delta, 2j}, p_{i+\\delta, 2j+1})$\uff1a $$\\begin{aligned} &\\begin{bmatrix} \\cos(\\delta \\omega_j) & \\sin(\\delta \\omega_j) \\ -\\sin(\\delta \\omega_j) & \\cos(\\delta \\omega_j) \\ \\end{bmatrix} \\begin{bmatrix} p_{i, 2j} \\ p_{i, 2j+1} \\ \\end{bmatrix}\\ =&\\begin{bmatrix} \\cos(\\delta \\omega_j) \\sin(i \\omega_j) + \\sin(\\delta \\omega_j) \\cos(i \\omega_j) \\ -\\sin(\\delta \\omega_j) \\sin(i \\omega_j) + \\cos(\\delta \\omega_j) \\cos(i \\omega_j) \\ \\end{bmatrix}\\ =&\\begin{bmatrix} \\sin\\left((i+\\delta) \\omega_j\\right) \\ \\cos\\left((i+\\delta) \\omega_j\\right) \\ \\end{bmatrix}\\ =& \\begin{bmatrix} p_{i+\\delta, 2j} \\ p_{i+\\delta, 2j+1} \\ \\end{bmatrix}, \\end{aligned}$$ \u56e0\u6b64\u8fd9\u4e2a\u77e9\u9635\u4e0d\u4f9d\u8d56\u4e8e\u4efb\u4f55\u4f4d\u7f6e\u7684\u7d22\u5f15\u3002 Transformer Add&Norm \u4f7f\u7528LayerNorm \uff08\u4e0d\u6539\u53d8\u8bed\u4e49\u5411\u91cf\u7684\u65b9\u5411\uff0c\u6539\u53d8\u6a21\u957f\uff09 class AddNorm(nn.Module): def __init__(self, normalized_shape, dropout, **kwargs): super(AddNorm, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) self.ln = nn.LayerNorm(normalized_shape) def forward(self, X, Y): return self.ln(self.dropout(Y) + X) Encoder class EncoderBlock(nn.Module): def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs): super(EncoderBlock, self).__init__(**kwargs) self.attention = d2l.MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias) self.addnorm1 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN( ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm2 = AddNorm(norm_shape, dropout) def forward(self, X, valid_lens): Y = self.addnorm1(X, self.attention(X, X, X, valid_lens)) return self.addnorm2(Y, self.ffn(Y)) Encoder\u5c42\u90fd\u4e0d\u4f1a\u6539\u53d8\u8f93\u5165\u7684\u5f62\u72b6\u3002 class TransformerEncoder(d2l.Encoder): def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs): super(TransformerEncoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(\"block\"+str(i), EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias)) def forward(self, X, valid_lens, *args): # \u56e0\u4e3a\u4f4d\u7f6e\u7f16\u7801\u503c\u5728-1\u548c1\u4e4b\u95f4\uff0c # \u56e0\u6b64\u5d4c\u5165\u503c\u4e58\u4ee5\u5d4c\u5165\u7ef4\u5ea6\u7684\u5e73\u65b9\u6839\u8fdb\u884c\u7f29\u653e\uff0c # \u7136\u540e\u518d\u4e0e\u4f4d\u7f6e\u7f16\u7801\u76f8\u52a0\u3002 X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self.attention_weights = [None] * len(self.blks) for i, blk in enumerate(self.blks): X = blk(X, valid_lens) self.attention_weights[ i] = blk.attention.attention.attention_weights return X Decoder \u5728\u63a9\u853d\u591a\u5934\u89e3\u7801\u5668\u81ea\u6ce8\u610f\u529b\u5c42\uff08\u7b2c\u4e00\u4e2a\u5b50\u5c42\uff09\u4e2d\uff0c\u67e5\u8be2\u3001\u952e\u548c\u503c\u90fd\u6765\u81ea\u4e0a\u4e00\u4e2a\u89e3\u7801\u5668\u5c42\u7684\u8f93\u51fa\u3002\u4e3a\u4e86\u5728\u89e3\u7801\u5668\u4e2d\u4fdd\u7559\u81ea\u56de\u5f52\u7684\u5c5e\u6027\uff0c\u5176\u63a9\u853d\u81ea\u6ce8\u610f\u529b\u8bbe\u5b9a\u4e86\u53c2\u6570\uff0c\u4ee5\u4fbf\u4efb\u4f55\u67e5\u8be2\u90fd\u53ea\u4f1a\u4e0e\u89e3\u7801\u5668\u4e2d\u6240\u6709\u5df2\u7ecf\u751f\u6210\u8bcd\u5143\u7684\u4f4d\u7f6e\uff08\u5373\u76f4\u5230\u8be5\u67e5\u8be2\u4f4d\u7f6e\u4e3a\u6b62\uff09\u8fdb\u884c\u6ce8\u610f\u529b\u8ba1\u7b97\u3002 class DecoderBlock(nn.Module): def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs): super(DecoderBlock, self).__init__(**kwargs) self.i = i self.attention1 = d2l.MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm1 = AddNorm(norm_shape, dropout) self.attention2 = d2l.MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm2 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm3 = AddNorm(norm_shape, dropout) def forward(self, X, state): enc_outputs, enc_valid_lens = state[0], state[1] # \u8bad\u7ec3\u9636\u6bb5\uff0c\u8f93\u51fa\u5e8f\u5217\u7684\u6240\u6709\u8bcd\u5143\u90fd\u5728\u540c\u4e00\u65f6\u95f4\u5904\u7406\uff0c # \u56e0\u6b64state[2][self.i]\u521d\u59cb\u5316\u4e3aNone\u3002 # \u9884\u6d4b\u9636\u6bb5\uff0c\u8f93\u51fa\u5e8f\u5217\u662f\u901a\u8fc7\u8bcd\u5143\u4e00\u4e2a\u63a5\u7740\u4e00\u4e2a\u89e3\u7801\u7684\uff0c # \u56e0\u6b64state[2][self.i]\u5305\u542b\u7740\u76f4\u5230\u5f53\u524d\u65f6\u95f4\u6b65\u7b2ci\u4e2a\u5757\u89e3\u7801\u7684\u8f93\u51fa\u8868\u793a if state[2][self.i] is None: key_values = X else: key_values = torch.cat((state[2][self.i], X), axis=1) state[2][self.i] = key_values if self.training: batch_size, num_steps, _ = X.shape # dec_valid_lens\u7684\u5f00\u5934:(batch_size,num_steps), # \u5176\u4e2d\u6bcf\u4e00\u884c\u662f[1,2,...,num_steps] dec_valid_lens = torch.arange( 1, num_steps + 1, device=X.device).repeat(batch_size, 1) else: dec_valid_lens = None # \u81ea\u6ce8\u610f\u529b X2 = self.attention1(X, key_values, key_values, dec_valid_lens) Y = self.addnorm1(X, X2) # \u7f16\u7801\u5668\uff0d\u89e3\u7801\u5668\u6ce8\u610f\u529b\u3002 # enc_outputs\u7684\u5f00\u5934:(batch_size,num_steps,num_hiddens) Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens) Z = self.addnorm2(Y, Y2) return self.addnorm3(Z, self.ffn(Z)), state class TransformerDecoder(d2l.AttentionDecoder): def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs): super(TransformerDecoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.num_layers = num_layers self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(\"block\"+str(i), DecoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i)) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, enc_valid_lens, *args): return [enc_outputs, enc_valid_lens, [None] * self.num_layers] def forward(self, X, state): X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self._attention_weights = [[None] * len(self.blks) for _ in range (2)] for i, blk in enumerate(self.blks): X, state = blk(X, state) # \u89e3\u7801\u5668\u81ea\u6ce8\u610f\u529b\u6743\u91cd self._attention_weights[0][ i] = blk.attention1.attention.attention_weights # \u201c\u7f16\u7801\u5668\uff0d\u89e3\u7801\u5668\u201d\u81ea\u6ce8\u610f\u529b\u6743\u91cd self._attention_weights[1][ i] = blk.attention2.attention.attention_weights return self.dense(X), state @property def attention_weights(self): return self._attention_weights Multiple GPUs \u62c6\u5206\u6570\u636e \u8fd9\u79cd\u65b9\u5f0f\u4e0b\uff0c\u6240\u6709GPU\u5c3d\u7ba1\u6709\u4e0d\u540c\u7684\u89c2\u6d4b\u7ed3\u679c\uff0c\u4f46\u662f\u6267\u884c\u7740\u76f8\u540c\u7c7b\u578b\u7684\u5de5\u4f5c\u3002\u5728\u5b8c\u6210\u6bcf\u4e2a\u5c0f\u6279\u91cf\u6570\u636e\u7684\u8bad\u7ec3\u4e4b\u540e\uff0c\u68af\u5ea6\u5728GPU\u4e0a\u805a\u5408\u3002 GPU\u7684\u6570\u91cf\u8d8a\u591a\uff0c\u5c0f\u6279\u91cf\u5305\u542b\u7684\u6570\u636e\u91cf\u5c31\u8d8a\u5927\uff0c\u4ece\u800c\u5c31\u80fd\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002 \u7f3a\u70b9\uff1a\u4e0d\u80fd\u591f\u8bad\u7ec3\u66f4\u5927\u7684\u6a21\u578b $k$\u4e2aGPU\u5e76\u884c\u8bad\u7ec3\u8fc7\u7a0b\u5982\u4e0b\uff1a \u5728\u4efb\u4f55\u4e00\u6b21\u8bad\u7ec3\u8fed\u4ee3\u4e2d\uff0c\u7ed9\u5b9a\u7684\u968f\u673a\u7684\u5c0f\u6279\u91cf\u6837\u672c\u90fd\u5c06\u88ab\u5206\u6210$k$\u4e2a\u90e8\u5206\uff0c\u5e76\u5747\u5300\u5730\u5206\u914d\u5230GPU\u4e0a\uff1b \u6bcf\u4e2aGPU\u6839\u636e\u5206\u914d\u7ed9\u5b83\u7684\u5c0f\u6279\u91cf\u5b50\u96c6\uff0c\u8ba1\u7b97\u6a21\u578b\u53c2\u6570\u7684\u635f\u5931\u548c\u68af\u5ea6\uff1b \u5c06$k$\u4e2aGPU\u4e2d\u7684\u5c40\u90e8\u68af\u5ea6\u805a\u5408\uff0c\u4ee5\u83b7\u5f97\u5f53\u524d\u5c0f\u6279\u91cf\u7684\u968f\u673a\u68af\u5ea6\uff1b \u805a\u5408\u68af\u5ea6\u88ab\u91cd\u65b0\u5206\u53d1\u5230\u6bcf\u4e2aGPU\u4e2d\uff1b *\u6bcf\u4e2aGPU\u4f7f\u7528\u8fd9\u4e2a\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\uff0c\u6765\u66f4\u65b0\u5b83\u6240\u7ef4\u62a4\u7684\u5b8c\u6574\u7684\u6a21\u578b\u53c2\u6570\u96c6\u3002 \u5411\u591a\u4e2a\u8bbe\u5907\u590d\u5236\u53c2\u6570 def get_params(params, device): new_params = [p.to(device) for p in params] for p in new_params: p.requires_grad_() return new_params \u80fd\u591f\u5c06\u6240\u6709\u8bbe\u5907\u4e0a\u7684\u68af\u5ea6\u8fdb\u884c\u76f8\u52a0 def allreduce(data): for i in range(1, len(data)): data[0][:] += data[i].to(data[0].device) for i in range(1, len(data)): data[i][:] = data[0].to(data[i].device) \u5206\u53d1\u6570\u636e nn.parallel.scatter(data, devices) data = torch.arange(20).reshape(4, 5) devices = [torch.device('cuda:0'), torch.device('cuda:1')] split = nn.parallel.scatter(data, devices) print('input :', data) print('load into', devices) print('output:', split) input : tensor([ [ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19] ]) load into [device(type='cuda', index=0), device(type='cuda', index=1)] output: (tensor([ [0, 1, 2, 3, 4], [5, 6, 7, 8, 9] ], device='cuda:0'), tensor([ [10, 11, 12, 13, 14], [15, 16, 17, 18, 19] ], device='cuda:1')) \u5206\u53d1\u6570\u636e\u548c\u6807\u7b7e def split_batch(X, y, devices): assert X.shape[0] == y.shape[0] return (nn.parallel.scatter(X, devices), nn.parallel.scatter(y, devices)) \u5b9e\u73b0\u591aGPU\u8bad\u7ec3 def train_batch(X, y, device_params, devices, lr): X_shards, y_shards = split_batch(X, y, devices) # \u5728\u6bcf\u4e2aGPU\u4e0a\u5206\u522b\u8ba1\u7b97\u635f\u5931 ls = [loss(lenet(X_shard, device_W), y_shard).sum() for X_shard, y_shard, device_W in zip( X_shards, y_shards, device_params)] for l in ls: # \u53cd\u5411\u4f20\u64ad\u5728\u6bcf\u4e2aGPU\u4e0a\u5206\u522b\u6267\u884c l.backward() # \u5c06\u6bcf\u4e2aGPU\u7684\u6240\u6709\u68af\u5ea6\u76f8\u52a0\uff0c\u5e76\u5c06\u5176\u5e7f\u64ad\u5230\u6240\u6709GPU with torch.no_grad(): for i in range(len(device_params[0])): allreduce([device_params[c][i].grad for c in range(len(devices))]) # \u5728\u6bcf\u4e2aGPU\u4e0a\u5206\u522b\u66f4\u65b0\u6a21\u578b\u53c2\u6570 for param in device_params: d2l.sgd(param, lr, X.shape[0]) # \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4f7f\u7528\u5168\u5c3a\u5bf8\u7684\u5c0f\u6279\u91cf \u8bc4\u4f30\u6a21\u578b\u7684\u65f6\u5019\u53ea\u5728\u4e00\u4e2aGPU\u4e0a def train(num_gpus, batch_size, lr): train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) devices = [d2l.try_gpu(i) for i in range(num_gpus)] # \u5c06\u6a21\u578b\u53c2\u6570\u590d\u5236\u5230num_gpus\u4e2aGPU device_params = [get_params(params, d) for d in devices] num_epochs = 10 animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs]) timer = d2l.Timer() for epoch in range(num_epochs): timer.start() for X, y in train_iter: # \u4e3a\u5355\u4e2a\u5c0f\u6279\u91cf\u6267\u884c\u591aGPU\u8bad\u7ec3 train_batch(X, y, device_params, devices, lr) torch.cuda.synchronize() timer.stop() # \u5728GPU0\u4e0a\u8bc4\u4f30\u6a21\u578b animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu( lambda x: lenet(x, device_params[0]), test_iter, devices[0]),)) print(f'\u6d4b\u8bd5\u7cbe\u5ea6\uff1a{animator.Y[0][-1]:.2f}\uff0c{timer.avg():.1f}\u79d2/\u8f6e\uff0c' f'\u5728{str(devices)}') \u7b80\u6d01\u5b9e\u73b0 net = nn.DataParallel(net, device_ids=devices) \uff0c\u548c\u4e4b\u524d\u51e0\u4e4e\u6ca1\u4ec0\u4e48\u533a\u522b def train(net, num_gpus, batch_size, lr): train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) devices = [d2l.try_gpu(i) for i in range(num_gpus)] def init_weights(m): if type(m) in [nn.Linear, nn.Conv2d]: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights) # \u5728\u591a\u4e2aGPU\u4e0a\u8bbe\u7f6e\u6a21\u578b net = nn.DataParallel(net, device_ids=devices) trainer = torch.optim.SGD(net.parameters(), lr) loss = nn.CrossEntropyLoss() timer, num_epochs = d2l.Timer(), 10 animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs]) for epoch in range(num_epochs): net.train() timer.start() for X, y in train_iter: trainer.zero_grad() X, y = X.to(devices[0]), y.to(devices[0]) l = loss(net(X), y) l.backward() trainer.step() timer.stop() animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),)) print(f'\u6d4b\u8bd5\u7cbe\u5ea6\uff1a{animator.Y[0][-1]:.2f}\uff0c{timer.avg():.1f}\u79d2/\u8f6e\uff0c' f'\u5728{str(devices)}') Computer Vision Image Augmentation \u7ffb\u8f6c\uff08\u4e0a\u4e0b\u7ffb\u8f6c\u3001\u5de6\u53f3\u7ffb\u8f6c\uff09 torchvision.transforms.RandomHorizontalFlip() torchvision.transforms.RandomVerticalFlip() \u968f\u673a\u526a\u88c1 torchvision.transforms.RandomResizedCrop((200, 200), scale=(0.1, 1), ratio=(0.5, 2)) \u6539\u53d8\u989c\u8272\uff08\u4eae\u5ea6\u3001\u5bf9\u6bd4\u5ea6\u3001\u9971\u548c\u5ea6\u3001\u8272\u8c03\uff09 torchvision.transforms.ColorJitter(brightness=0.5, contrast=0, saturation=0, hue=0) \u76ee\u6807\u68c0\u6d4b Bounding Box \u4e24\u79cd\u8868\u793a - \u5de6\u4e0a\u89d2\u5750\u6807\u548c\u53f3\u4e0b\u89d2\u5750\u6807 - \u4e2d\u5fc3\u5750\u6807\u548cw\uff0ch \u951a\u6846 \u751f\u6210\u951a\u6846 \u8f93\u5165\u56fe\u50cf\u7684\u9ad8\u5ea6\u4e3a$h$\uff0c\u5bbd\u5ea6\u4e3a$w$\u3002\u4ee5\u56fe\u50cf\u7684\u6bcf\u4e2a\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u751f\u6210\u4e0d\u540c\u5f62\u72b6\u7684\u951a\u6846\uff1a \u7f29\u653e\u6bd4 \u4e3a$s\\in (0, 1]$\uff0c \u5bbd\u9ad8\u6bd4 \u4e3a$r > 0$\u3002 \u90a3\u4e48 \u951a\u6846\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u5206\u522b\u662f$hs\\sqrt{r}$\u548c$hs/\\sqrt{r}$\u3002 \u8bf7\u6ce8\u610f\uff0c\u5f53\u4e2d\u5fc3\u4f4d\u7f6e\u7ed9\u5b9a\u65f6\uff0c\u5df2\u77e5\u5bbd\u548c\u9ad8\u7684\u951a\u6846\u662f\u786e\u5b9a\u7684\u3002 \u7f29\u653e\u6bd4\uff08scale\uff09\u53d6\u503c$s_1,\\ldots, s_n$\u548c\u5bbd\u9ad8\u6bd4\uff08aspect ratio\uff09\u53d6\u503c$r_1,\\ldots, r_m$\u3002\u4f7f\u7528\u8fd9\u4e9b\u6bd4\u4f8b\u548c\u957f\u5bbd\u6bd4\u7684\u6240\u6709\u7ec4\u5408\u4ee5\u6bcf\u4e2a\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u65f6\uff0c\u8f93\u5165\u56fe\u50cf\u5c06\u603b\u5171\u6709$whnm$\u4e2a\u951a\u6846\u3002 \u5728\u5b9e\u8df5\u4e2d\uff0c\u8003\u8651\u5305\u542b$s_1$\u6216$r_1$\u7684\u7ec4\u5408\uff1a $$(s_1, r_1), (s_1, r_2), \\ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \\ldots, (s_n, r_1).$$ \u4e5f\u5c31\u662f\u8bf4\uff0c\u4ee5\u540c\u4e00\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u7684\u951a\u6846\u7684\u6570\u91cf\u662f$n+m-1$\u3002\u5bf9\u4e8e\u6574\u4e2a\u8f93\u5165\u56fe\u50cf\uff0c\u5c06\u5171\u751f\u6210$wh(n+m-1)$\u4e2a\u951a\u6846\u3002 \u4ea4\u5e76\u6bd4(IoU) $$J(\\mathcal{A},\\mathcal{B}) = \\frac{\\left|\\mathcal{A} \\cap \\mathcal{B}\\right|}{\\left| \\mathcal{A} \\cup \\mathcal{B}\\right|}.$$ \u951a\u6846\u5206\u914d \u6bcf\u6b21\u53d6\u6700\u5927IoU\u7684\u951a\u6846\u548c\u771f\u5b9e\u6846\uff0c\u53bb\u6389\u4e4b\u540e\u91cd\u590d\uff0c\u6700\u540e\u6839\u636e\u9608\u503c\u786e\u5b9a\u662f\u5426\u4e3a\u951a\u6846\u5206\u914d\u771f\u5b9e\u6846 \u6807\u8bb0\u7c7b\u522b \u7ed9\u5b9a\u6846$A$\u548c$B$\uff0c\u4e2d\u5fc3\u5750\u6807\u5206\u522b\u4e3a$(x_a, y_a)$\u548c$(x_b, y_b)$\uff0c\u5bbd\u5ea6\u5206\u522b\u4e3a$w_a$\u548c$w_b$\uff0c\u9ad8\u5ea6\u5206\u522b\u4e3a$h_a$\u548c$h_b$\uff0c\u53ef\u4ee5\u5c06$A$\u7684\u504f\u79fb\u91cf\u6807\u8bb0\u4e3a\uff1a $$\\left( \\frac{ \\frac{x_b - x_a}{w_a} - \\mu_x }{\\sigma_x}, \\frac{ \\frac{y_b - y_a}{h_a} - \\mu_y }{\\sigma_y}, \\frac{ \\log \\frac{w_b}{w_a} - \\mu_w }{\\sigma_w}, \\frac{ \\log \\frac{h_b}{h_a} - \\mu_h }{\\sigma_h}\\right),$$ \u975e\u6781\u5927\u503c\u6291\u5236 \u5728\u540c\u4e00\u5f20\u56fe\u50cf\u4e2d\uff0c\u6240\u6709\u9884\u6d4b\u7684\u975e\u80cc\u666f\u8fb9\u754c\u6846\u90fd\u6309\u7f6e\u4fe1\u5ea6\u964d\u5e8f\u6392\u5e8f\uff0c\u4ee5\u751f\u6210\u5217\u8868$L$\u3002 \u4ece$L$\u4e2d\u9009\u53d6\u7f6e\u4fe1\u5ea6\u6700\u9ad8\u7684\u9884\u6d4b\u8fb9\u754c\u6846$B_1$\u4f5c\u4e3a\u57fa\u51c6\uff0c\u7136\u540e\u5c06\u6240\u6709\u4e0e$B_1$\u7684IoU\u8d85\u8fc7\u9884\u5b9a\u9608\u503c$\\epsilon$\u7684\u975e\u57fa\u51c6\u9884\u6d4b\u8fb9\u754c\u6846\u4ece$L$\u4e2d\u79fb\u9664\u3002 \u4ece$L$\u4e2d\u9009\u53d6\u7f6e\u4fe1\u5ea6\u7b2c\u4e8c\u9ad8\u7684\u9884\u6d4b\u8fb9\u754c\u6846$B_2$\u4f5c\u4e3a\u53c8\u4e00\u4e2a\u57fa\u51c6\uff0c\u7136\u540e\u5c06\u6240\u6709\u4e0e$B_2$\u7684IoU\u5927\u4e8e$\\epsilon$\u7684\u975e\u57fa\u51c6\u9884\u6d4b\u8fb9\u754c\u6846\u4ece$L$\u4e2d\u79fb\u9664\u3002 \u91cd\u590d\u4e0a\u8ff0\u8fc7\u7a0b\uff0c\u76f4\u5230$L$\u4e2d\u7684\u6240\u6709\u9884\u6d4b\u8fb9\u754c\u6846\u90fd\u66fe\u88ab\u7528\u4f5c\u57fa\u51c6\u3002\u6b64\u65f6\uff0c$L$\u4e2d\u4efb\u610f\u4e00\u5bf9\u9884\u6d4b\u8fb9\u754c\u6846\u7684IoU\u90fd\u5c0f\u4e8e\u9608\u503c$\\epsilon$\uff1b\u56e0\u6b64\uff0c\u6ca1\u6709\u4e00\u5bf9\u8fb9\u754c\u6846\u8fc7\u4e8e\u76f8\u4f3c\u3002 \u8f93\u51fa\u5217\u8868$L$\u4e2d\u7684\u6240\u6709\u9884\u6d4b\u8fb9\u754c\u6846\u3002 R-CNN Region-based CNN \u4f7f\u7528\u542f\u53d1\u5f0f\u641c\u7d22\u9009\u62e9\u951a\u6846 \u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u6bcf\u4e2a\u951a\u6846\u62bd\u53d6\u7279\u5f81 \u8bad\u7ec3\u4e00\u4e2aSVM\u6765\u5bf9\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b \u8bad\u7ec3\u4e00\u4e2a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u6765\u9884\u6d4b\u8fb9\u7f18\u6846\u504f\u79fb Rol Pooling \u7ed9\u5b9a\u4e00\u4e2a\u951a\u6846\uff0c\u5747\u5300\u5206\u6210$n \\times m$\u5757\uff0c\u8f93\u51fa\u6bcf\u5757\u91cc\u9762\u7684\u6700\u5927\u503c\uff0c\u4e0d\u7ba1\u951a\u6846\u5927\u5c0f\u4e3a\u591a\u5c11\uff0c\u90fd\u662fnm Fast RCNN \u4f7f\u7528CNN\u5bf9\u56fe\u7247\u62bd\u53d6\u7279\u5f81\uff08\u6574\u5f20\u56fe\u7247\uff09\uff0c\u4f7f\u7528Rol Pooling\u5c42\u5bf9\u6bcf\u4e2a\u951a\u6846\u751f\u6210\u56fa\u5b9a\u957f\u5ea6\u7684\u7279\u5f81 Faster RCNN \u4f7f\u7528\u4e00\u4e2a\u7f51\u7edc\u6765\u66ff\u4ee3\u542f\u53d1\u5f0f\u641c\u7d22\u6765\u83b7\u5f97\u66f4\u597d\u7684\u951a\u6846 Mask RCNN \u5982\u679c\u6709\u50cf\u7d20\u7ea7\u522b\u7684\u6807\u53f7\uff0c\u7528FCN\u6765\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f YOLO \u53ea\u770b\u4e00\u6b21 - SSD\u951a\u6846\u5927\u91cf\u91cd\u53e0\u6d6a\u8d39\u8ba1\u7b97 - YOLO\u5c06\u56fe\u7247\u5747\u5300\u5206\u6210$S \\times S$ \u951a\u6846 - \u6bcf\u4e2a\u951a\u6846\u9884\u6d4b$B$\u4e2a\u8fb9\u7f18\u6846 \u5355\u53d1\u591a\u6846\u68c0\u6d4b\uff08SSD\uff09 \u8bed\u4e49\u5206\u5272 \u56fe\u50cf\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272 \u8f6c\u7f6e\u5377\u79ef \u8f6c\u7f6e\u5377\u79ef\u53ef\u4ee5\u7528\u6765\u589e\u52a0\u9ad8\u5bbd $Y[i: i + h, j: j + w] += X[i, j] * K$ \u8f6c\u7f6e? $Y = X * W$ \u53ef\u4ee5\u5bf9W\u6784\u9020\u4e00\u4e2aV\uff0c\u4f7f\u5f97\u5377\u79ef\u7b49\u4ef7\u4e8e\u77e9\u9635\u4e58\u6cd5$Y^ \\prime = V X^ \\prime$, \u8fd9\u91cc$X^ \\prime Y^\\prime$\u662f $XY$\u5bf9\u5e94\u7684\u5411\u91cf\u7248\u672c \u8f6c\u7f6e\u5377\u79ef\u7b49\u4ef7\u4e8e $Y^\\prime = V^T X^\\prime $ \u57fa\u672c\u64cd\u4f5c def trans_conv(X, K): h, w = K.shape Y = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1)) for i in range(X.shape[0]): for j in range(X.shape[1]): Y[i: i + h, j: j + w] += X[i, j] * K return Y torch API X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2) tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False) tensor([ [ [ [ 0., 0., 1.], [ 0., 4., 6.], [ 4., 12., 9.] ] ] ]) tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False) padding\u5c06\u5220\u9664\u7b2c\u4e00\u548c\u6700\u540e\u4e00\u884c\u548c\u5217 tensor([ [ [ [4.] ] ] ]) tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False) \u6b65\u5e45\u4e3a2\u589e\u5927\u8f93\u51fa tensor([ [ [ [0., 0., 0., 1.], [0., 0., 2., 3.], [0., 2., 0., 3.], [4., 6., 6., 9.] ] ] ]) \u5168\u5377\u79ef\u7f51\u7edc(FCN) \u7528\u8f6c\u7f6e\u5377\u79ef\u5c42\u6765\u66ff\u6362CNN\u6700\u540e\u7684\u5168\u8fde\u63a5\u5c42\uff0c\u4ece\u800c\u5b9e\u73b0\u6bcf\u4e2a\u50cf\u7d20\u7684\u9884\u6d4b CNN --> 1x1 Conv --> \u8f6c\u7f6e\u5377\u79ef --> output \u5148\u7528Resnet18\u63d0\u53d6\u7279\u5f81 net = nn.Sequential(*list(pretrained_net.children())[:-2]) \u7136\u540e\u52a01x1\u7684\u5377\u79ef\u5c42\u548c\u8f6c\u7f6e\u5377\u79ef\u5c42\uff0c\u4f7f\u5f97\u8f93\u51fa\u5927\u5c0f\u548c\u539f\u56fe\u50cf\u5927\u5c0f\u76f8\u540c num_classes = 21 net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1)) net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, padding=16, stride=32))","title":"\u6df1\u5ea6\u5b66\u4e60"},{"location":"DeepLearning/#pytorch","text":"deep-learning-computation\u7ae0\u8282","title":"Pytorch\u57fa\u7840"},{"location":"DeepLearning/#pytorch_1","text":"\u6df1\u5165\u6d45\u51faPytroch \u7b2c\u4e8c\u7ae0","title":"Pytorch\u81ea\u52a8\u6c42\u5bfc"},{"location":"DeepLearning/#_1","text":"\u5c06\u4ee3\u7801\u5206\u89e3\u6210\u64cd\u4f5c\u5b50\uff0c\u5c06\u8ba1\u7b97\u8868\u793a\u4e3a\u4e00\u4e2a\u65e0\u73af\u56fe \u4e00\u822c\u53ea\u6709leaf node\u6709grad \u6700\u540e\u7684\u8f93\u51fa\u770b\u6210\u5173\u4e8e\u7f51\u7edc\u6743\u91cd\u7684\u51fd\u6570\uff0cbackward\u51fd\u6570\u8ba1\u7b97\u51fa\u6743\u91cd\u7684\u68af\u5ea6\uff08\u5168\u5fae\u5206\uff09 \u5bf9\u4e8eleaf\u548crequire_grad\u7684\u8282\u70b9\u4e0d\u80fd\u591f\u8fdb\u884cinplace operation retain_graph \u9632\u6b62backward\u4e4b\u540e\u91ca\u653e\u76f8\u5173\u5185\u5b58 .detach return a new tensor ,detached from the current graph,the result will never require gradient \u5c06\u8ba1\u7b97\u79fb\u52a8\u5230\u8ba1\u7b97\u56fe\u4e4b\u5916\uff0c\u5f53\u4f5c\u5e38\u6570","title":"\u8ba1\u7b97\u56fe"},{"location":"DeepLearning/#_2","text":"","title":"\u6570\u636e\u8bfb\u53d6"},{"location":"DeepLearning/#_3","text":"\u968f\u673a\u91c7\u6837b\u4e2a\u6837\u672c $i_1,i_2,...,i_b$\u6765\u8fd1\u4f3c\u635f\u5931 $$ \\frac{1}{b}\\sum_{i \\in I_b}l(\\hat y_i,y_i) $$ \u4e00\u6b21\u8fed\u4ee3\u7528b\u4e2a\u6570\u636e\u8ba1\u7b97\u540e\u66f4\u65b0\u53c2\u6570 \u4e00\u4e2aepoch\u5c06\u6570\u636e\u96c6\u7684\u6570\u636e\u90fd\u7528\u4e00\u904d","title":"\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d"},{"location":"DeepLearning/#data-iterator","text":"\u4e00\u6b21\u968f\u673a\u8bfb\u53d6batch_size\u4e2a\u6570\u636e def data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) for i in range(0, num_examples, batch_size): batch_indices = torch.tensor( indices[i: min(i + batch_size, num_examples)]) yield features[batch_indices], labels[batch_indices]","title":"data iterator"},{"location":"DeepLearning/#_4","text":"cat \u548c stack \u53c2\u8003 \u535a\u5ba2 \u6279\u91cf\u77e9\u9635\u4e58\u6cd5 torch.bmm(X,Y),X\u7684shape\u4e3a(n,a,b)\uff0cY\u7684shape\u4e3a(n,b,c)\uff0c\u8f93\u51fa\u5f62\u72b6(n,a,c) torch.unsqueeze() \u5728\u6307\u5b9a\u7eac\u5ea6\u63d2\u5165\u7eac\u5ea61 X.shape = (2,3) X.unsqueeze(0).shape = (1\uff0c2\uff0c3) X.unsqueeze(1).shape = (2,1,3)","title":"\u8865\u5145"},{"location":"DeepLearning/#mlp","text":"dive into deep learning \u591a\u5c42\u611f\u77e5\u673a\u7ae0\u8282 num_inputs, num_outputs, num_hiddens = 784, 10, 256 W1 = nn.Parameter(torch.randn( num_inputs, num_hiddens, requires_grad=True) * 0.01) b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True)) W2 = nn.Parameter(torch.randn( num_hiddens, num_outputs, requires_grad=True) * 0.01) b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True)) params = [W1, b1, W2, b2] def relu(X): a = torch.zeros_like(X) return torch.max(X, a) def net(X): X = X.reshape((-1, num_inputs)) H = relu(X@W1 + b1) # \u8fd9\u91cc\u201c@\u201d\u4ee3\u8868\u77e9\u9635\u4e58\u6cd5 return (H@W2 + b2) \u7528 torch.nn net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10)) def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights); X = X.reshape((-1, num_inputs)) -- nn.Flatten() H = relu(X@W1 + b1) -- nn.Linear(784, 256) \u548c nn.ReLu() (H@W2 + b2) -- nn.Linear(256, 10)","title":"MLP"},{"location":"DeepLearning/#_5","text":"","title":"\u8fc7\u62df\u5408"},{"location":"DeepLearning/#_6","text":"$$ \\begin{aligned} L(\\mathbf{w}, b) + \\frac{\\lambda}{2} |\\mathbf{w}|^2 \\end{aligned} $$ $$ \\begin{aligned} \\mathbf{w} & \\leftarrow \\left(1- \\eta\\lambda \\right) \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned} $$","title":"\u6b63\u5219\u5316"},{"location":"DeepLearning/#drop-out","text":"$$ \\begin{aligned} h' = \\begin{cases} 0 & \\text{ \u6982\u7387\u4e3a } p \\ \\frac{h}{1-p} & \\text{ \u5176\u4ed6\u60c5\u51b5} \\end{cases} \\end{aligned} $$ \u671f\u671b\u503c\u4fdd\u6301\u4e0d\u53d8\uff0c\u5373$E[h'] = h$\u3002 def dropout_layer(X, dropout): assert 0 <= dropout <= 1 # \u5728\u672c\u60c5\u51b5\u4e2d\uff0c\u6240\u6709\u5143\u7d20\u90fd\u88ab\u4e22\u5f03 if dropout == 1: return torch.zeros_like(X) # \u5728\u672c\u60c5\u51b5\u4e2d\uff0c\u6240\u6709\u5143\u7d20\u90fd\u88ab\u4fdd\u7559 if dropout == 0: return X mask = (torch.rand(X.shape) > dropout).float() return mask * X / (1.0 - dropout) dropout1, dropout2 = 0.2, 0.5 class Net(nn.Module): def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2, is_training = True): super(Net, self).__init__() self.num_inputs = num_inputs self.training = is_training self.lin1 = nn.Linear(num_inputs, num_hiddens1) self.lin2 = nn.Linear(num_hiddens1, num_hiddens2) self.lin3 = nn.Linear(num_hiddens2, num_outputs) self.relu = nn.ReLU() def forward(self, X): H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs)))) # \u53ea\u6709\u5728\u8bad\u7ec3\u6a21\u578b\u65f6\u624d\u4f7f\u7528dropout if self.training == True: # \u5728\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42 H1 = dropout_layer(H1, dropout1) H2 = self.relu(self.lin2(H1)) if self.training == True: # \u5728\u7b2c\u4e8c\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42 H2 = dropout_layer(H2, dropout2) out = self.lin3(H2) return out net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2) \u76f4\u63a5\u7528 nn.Dropout(p) net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), # \u5728\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42 nn.Dropout(dropout1), nn.Linear(256, 256), nn.ReLU(), # \u5728\u7b2c\u4e8c\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42 nn.Dropout(dropout2), nn.Linear(256, 10)) def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights);","title":"Drop out"},{"location":"DeepLearning/#_7","text":"","title":"\u53c2\u6570\u521d\u59cb\u5316"},{"location":"DeepLearning/#xavier","text":"$$o_{i} = \\sum_{j=1}^{n_\\mathrm{in}} w_{ij} x_j.$$ $$ \\begin{aligned} E[o_i] & = \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij} x_j] \\&= \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij}] E[x_j] \\&= 0, \\ \\mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\ & = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\ & = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\ & = n_\\mathrm{in} \\sigma^2 \\gamma^2. \\end{aligned} $$ \u4f7f\u65b9\u5dee\u6ee1\u8db3 $$ \\begin{aligned} \\frac{1}{2} (n_\\mathrm{in} + n_\\mathrm{out}) \\sigma^2 = 1 \\text{ or } \\sigma = \\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}. \\end{aligned} $$ Xavier\u521d\u59cb\u5316\u901a\u5e38\u4ece\u65b9\u5dee\u6ee1\u8db3\u4e0a\u8ff0\u7684\u5747\u5300\u5206\u5e03\u6216\u9ad8\u65af\u5206\u5e03\u4e2d\u91c7\u6837\u6743\u91cd","title":"Xavier\u521d\u59cb\u5316"},{"location":"DeepLearning/#cnn","text":"\u5377\u79ef\u64cd\u4f5c\u5b9e\u73b0 def corr2d(X, K): h, w = K.shape Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] = (X[i:i + h, j:j + w] * K).sum() return Y \u5377\u79ef\u5c42 class Conv2D(nn.Module): def __init__(self, kernel_size): super().__init__() self.weight = nn.Parameter(torch.rand(kernel_size)) self.bias = nn.Parameter(torch.zeros(1)) def forward(self, x): return corr2d(x, self.weight) + self.bias backprop\u8bad\u7ec3\u5377\u79ef\u6838 nn.Conv2d()\u8f93\u5165\u548c\u8f93\u51fa:\u6279\u91cf\u5927\u5c0f\u3001\u901a\u9053\u3001\u9ad8\u5ea6\u3001\u5bbd\u5ea6","title":"CNN"},{"location":"DeepLearning/#_8","text":"\u591a\u8f93\u5165\u901a\u9053: def corr2d_multi_in(X, K): # \u5148\u904d\u5386\u201cX\u201d\u548c\u201cK\u201d\u7684\u7b2c0\u4e2a\u7ef4\u5ea6\uff08\u901a\u9053\u7ef4\u5ea6\uff09\uff0c\u518d\u628a\u5b83\u4eec\u52a0\u5728\u4e00\u8d77 return sum(d2l.corr2d(x, k) for x, k in zip(X, K)) \u591a\u8f93\u51fa\u901a\u9053: def corr2d_multi_in_out(X, K): # \u8fed\u4ee3\u201cK\u201d\u7684\u7b2c0\u4e2a\u7ef4\u5ea6\uff0c\u6bcf\u6b21\u90fd\u5bf9\u8f93\u5165\u201cX\u201d\u6267\u884c\u4e92\u76f8\u5173\u8fd0\u7b97\u3002 # \u6700\u540e\u5c06\u6240\u6709\u7ed3\u679c\u90fd\u53e0\u52a0\u5728\u4e00\u8d77 return torch.stack([corr2d_multi_in(X, k) for k in K], 0) \u8f93\u5165$\\mathbf{X} : c_i \\times n_h \\times n_w$ \u6838$\\mathbf{{W}} : c_0 \\times c_i \\times k_h \\times k_w $ \u8f93\u51fa$\\mathbf{Y} : c_o \\times m_h \\times m_w$","title":"\u591a\u8f93\u5165\u8f93\u51fa\u901a\u9053"},{"location":"DeepLearning/#pooling","text":"nn.MaxPool2d(3, padding=1, stride=2) pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1)) \u591a\u901a\u9053\u5bf9\u6bcf\u4e2a\u901a\u9053\u8fdb\u884cpooling\uff0c\u8f93\u51fa\u901a\u9053\u6570\u4e0e\u8f93\u5165\u76f8\u540c","title":"Pooling"},{"location":"DeepLearning/#lenet","text":"net = nn.Sequential( nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(), nn.Linear(120, 84), nn.Sigmoid(), nn.Linear(84, 10)) Conv2d output shape: torch.Size([1, 6, 28, 28]) Sigmoid output shape: torch.Size([1, 6, 28, 28]) AvgPool2d output shape: torch.Size([1, 6, 14, 14]) Conv2d output shape: torch.Size([1, 16, 10, 10]) Sigmoid output shape: torch.Size([1, 16, 10, 10]) AvgPool2d output shape: torch.Size([1, 16, 5, 5]) Flatten output shape: torch.Size([1, 400]) Linear output shape: torch.Size([1, 120]) Sigmoid output shape: torch.Size([1, 120]) Linear output shape: torch.Size([1, 84]) Sigmoid output shape: torch.Size([1, 84]) Linear output shape: torch.Size([1, 10])","title":"LeNet"},{"location":"DeepLearning/#alexnet","text":"\u4f7f\u7528ReLU\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0cDropout\uff0c\u6570\u636e\u96c6\u9884\u5904\u7406\uff08\u88c1\u5207\u3001\u7ffb\u8f6c\u3001\u53d8\u8272\uff09 net = nn.Sequential( # \u8fd9\u91cc\u4f7f\u7528\u4e00\u4e2a11*11\u7684\u66f4\u5927\u7a97\u53e3\u6765\u6355\u6349\u5bf9\u8c61\u3002 # \u540c\u65f6\uff0c\u6b65\u5e45\u4e3a4\uff0c\u4ee5\u51cf\u5c11\u8f93\u51fa\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002 # \u53e6\u5916\uff0c\u8f93\u51fa\u901a\u9053\u7684\u6570\u76ee\u8fdc\u5927\u4e8eLeNet nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), # \u51cf\u5c0f\u5377\u79ef\u7a97\u53e3\uff0c\u4f7f\u7528\u586b\u5145\u4e3a2\u6765\u4f7f\u5f97\u8f93\u5165\u4e0e\u8f93\u51fa\u7684\u9ad8\u548c\u5bbd\u4e00\u81f4\uff0c\u4e14\u589e\u5927\u8f93\u51fa\u901a\u9053\u6570 nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), # \u4f7f\u7528\u4e09\u4e2a\u8fde\u7eed\u7684\u5377\u79ef\u5c42\u548c\u8f83\u5c0f\u7684\u5377\u79ef\u7a97\u53e3\u3002 # \u9664\u4e86\u6700\u540e\u7684\u5377\u79ef\u5c42\uff0c\u8f93\u51fa\u901a\u9053\u7684\u6570\u91cf\u8fdb\u4e00\u6b65\u589e\u52a0\u3002 # \u5728\u524d\u4e24\u4e2a\u5377\u79ef\u5c42\u4e4b\u540e\uff0c\u6c47\u805a\u5c42\u4e0d\u7528\u4e8e\u51cf\u5c11\u8f93\u5165\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6 nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(), # \u8fd9\u91cc\uff0c\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\u6570\u91cf\u662fLeNet\u4e2d\u7684\u597d\u51e0\u500d\u3002\u4f7f\u7528dropout\u5c42\u6765\u51cf\u8f7b\u8fc7\u62df\u5408 nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5), # \u6700\u540e\u662f\u8f93\u51fa\u5c42\u3002\u7531\u4e8e\u8fd9\u91cc\u4f7f\u7528Fashion-MNIST\uff0c\u6240\u4ee5\u7528\u7c7b\u522b\u6570\u4e3a10\uff0c\u800c\u975e\u8bba\u6587\u4e2d\u76841000 nn.Linear(4096, 10)) Conv2d output shape:torch.Size([1, 96, 54, 54]) ReLU output shape:torch.Size([1, 96, 54, 54]) MaxPool2d output shape:torch.Size([1, 96, 26, 26]) Conv2d output shape:torch.Size([1, 256, 26, 26]) ReLU output shape:torch.Size([1, 256, 26, 26]) MaxPool2d output shape:torch.Size([1, 256, 12, 12]) Conv2d output shape:torch.Size([1, 384, 12, 12]) ReLU output shape:torch.Size([1, 384, 12, 12]) Conv2d output shape:torch.Size([1, 384, 12, 12]) ReLU output shape:torch.Size([1, 384, 12, 12]) Conv2d output shape:torch.Size([1, 256, 12, 12]) ReLU output shape:torch.Size([1, 256, 12, 12]) MaxPool2d output shape:torch.Size([1, 256, 5, 5]) Flatten output shape:torch.Size([1, 6400]) Linear output shape:torch.Size([1, 4096]) ReLU output shape:torch.Size([1, 4096]) Dropout output shape:torch.Size([1, 4096]) Linear output shape:torch.Size([1, 4096]) ReLU output shape:torch.Size([1, 4096]) Dropout output shape:torch.Size([1, 4096]) Linear output shape:torch.Size([1, 10])","title":"AlexNet"},{"location":"DeepLearning/#vgg","text":"VGG\u5757 kernel\u5927\u5c0f\u90fd\u4e3a3 $\\times$ 3 def vgg_block(num_convs, in_channels, out_channels): layers = [] for _ in range(num_convs): layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)) layers.append(nn.ReLU()) in_channels = out_channels layers.append(nn.MaxPool2d(kernel_size=2,stride=2)) return nn.Sequential(*layers) VGG\u7f51\u7edc VGG-11 conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512)) def vgg(conv_arch): conv_blks = [] in_channels = 1 # \u5377\u79ef\u5c42\u90e8\u5206 for (num_convs, out_channels) in conv_arch: conv_blks.append(vgg_block(num_convs, in_channels, out_channels)) in_channels = out_channels return nn.Sequential( *conv_blks, nn.Flatten(), # \u5168\u8fde\u63a5\u5c42\u90e8\u5206 nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 10)) net = vgg(conv_arch)","title":"VGG"},{"location":"DeepLearning/#nin","text":"\u5728\u6bcf\u4e2a\u50cf\u7d20\u7684\u901a\u9053\u4e0a\u5206\u522b\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a def nin_block(in_channels, out_channels, kernel_size, strides, padding): return nn.Sequential( nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()) net = nn.Sequential( nin_block(1, 96, kernel_size=11, strides=4, padding=0), nn.MaxPool2d(3, stride=2), nin_block(96, 256, kernel_size=5, strides=1, padding=2), nn.MaxPool2d(3, stride=2), nin_block(256, 384, kernel_size=3, strides=1, padding=1), nn.MaxPool2d(3, stride=2), nn.Dropout(0.5), # \u6807\u7b7e\u7c7b\u522b\u6570\u662f10 nin_block(384, 10, kernel_size=3, strides=1, padding=1), nn.AdaptiveAvgPool2d((1, 1)), # \u5c06\u56db\u7ef4\u7684\u8f93\u51fa\u8f6c\u6210\u4e8c\u7ef4\u7684\u8f93\u51fa\uff0c\u5176\u5f62\u72b6\u4e3a(\u6279\u91cf\u5927\u5c0f,10) nn.Flatten()) nn.AdaptiveAvgPool2d() \u53c2\u6570\u4e3a\u6307\u5b9a\u8f93\u51fasize","title":"NIN"},{"location":"DeepLearning/#googlenet","text":"class Inception(nn.Module): # c1--c4\u662f\u6bcf\u6761\u8def\u5f84\u7684\u8f93\u51fa\u901a\u9053\u6570 def __init__(self, in_channels, c1, c2, c3, c4, **kwargs): super(Inception, self).__init__(**kwargs) # \u7ebf\u8def1\uff0c\u53551x1\u5377\u79ef\u5c42 self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1) # \u7ebf\u8def2\uff0c1x1\u5377\u79ef\u5c42\u540e\u63a53x3\u5377\u79ef\u5c42 self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1) self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1) # \u7ebf\u8def3\uff0c1x1\u5377\u79ef\u5c42\u540e\u63a55x5\u5377\u79ef\u5c42 self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1) self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2) # \u7ebf\u8def4\uff0c3x3\u6700\u5927\u6c47\u805a\u5c42\u540e\u63a51x1\u5377\u79ef\u5c42 self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1) def forward(self, x): p1 = F.relu(self.p1_1(x)) p2 = F.relu(self.p2_2(F.relu(self.p2_1(x)))) p3 = F.relu(self.p3_2(F.relu(self.p3_1(x)))) p4 = F.relu(self.p4_2(self.p4_1(x))) # \u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u8fde\u7ed3\u8f93\u51fa return torch.cat((p1, p2, p3, p4), dim=1) \u8d85\u53c2\u6570\u4e3b\u8981\u4e3a\u901a\u9053\u6570,\u6bd4\u5982 Inception(192, 64, (96, 128), (16, 32), 32) \u8868\u793a\u4e00\u4e2aInception\u7684\u8f93\u5165\u901a\u9053\u6570\u4e3a192\uff0c\u8f93\u51fa\u901a\u9053\u6570\u4e3a 64 + 128 + 32 + 32 = 512 \u7f51\u7edc\u6574\u4f53\u7ed3\u6784\u7565","title":"GoogLeNet"},{"location":"DeepLearning/#batch-norm","text":"$$\\mathrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}} \\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}} \\mathcal{B}} + \\boldsymbol{\\beta}.$$ \u5f52\u4e00\u5316\u540e\u7684\u65b9\u5dee\u4e3a$\\gamma$\uff0c\u5747\u503c\u4e3a$1 + \\beta$ $\\gamma,\\beta$ \u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570 \u5377\u79ef\u5c42\u6709\u591a\u4e2a\u901a\u9053\u65f6\uff0c\u5bf9\u6bcf\u4e2a\u901a\u9053\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u5047\u8bbebatch_size\u4e3am\uff0c\u5377\u79ef\u5c42\u8f93\u51fa\u7684\u5927\u5c0f\u4e3ap$\\times$q,\u5728\u6bcf\u4e2a\u8f93\u51fa\u901a\u9053\u4e0a\u7684mqp\u4e2a\u5143\u7d20\u4e0a\u6c42\u5747\u503c\u548c\u65b9\u5dee def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum): ''' moving_mean\u548cmoving_var\u8fd1\u4f3c\u8ba4\u4e3a\u5168\u5c40\u503c eps\u9632\u6b62\u65b9\u5dee\u4e3a0\uff0c\u56fa\u5b9a\u503c gmma,beta,moving_mean,moving_var\u7684\u5f62\u72b6\u548cX\u76f8\u540c ''' # \u901a\u8fc7is_grad_enabled\u6765\u5224\u65ad\u5f53\u524d\u6a21\u5f0f\u662f\u8bad\u7ec3\u6a21\u5f0f\u8fd8\u662f\u9884\u6d4b\u6a21\u5f0f if not torch.is_grad_enabled(): # \u5982\u679c\u662f\u5728\u9884\u6d4b\u6a21\u5f0f\u4e0b\uff0c\u76f4\u63a5\u4f7f\u7528\u4f20\u5165\u7684\u79fb\u52a8\u5e73\u5747\u6240\u5f97\u7684\u5747\u503c\u548c\u65b9\u5dee X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps) else: assert len(X.shape) in (2, 4) if len(X.shape) == 2: # \u4f7f\u7528\u5168\u8fde\u63a5\u5c42\u7684\u60c5\u51b5\uff0c\u8ba1\u7b97\u7279\u5f81\u7ef4\u4e0a\u7684\u5747\u503c\u548c\u65b9\u5dee mean = X.mean(dim=0) var = ((X - mean) ** 2).mean(dim=0) else: # \u4f7f\u7528\u4e8c\u7ef4\u5377\u79ef\u5c42\u7684\u60c5\u51b5\uff0c\u8ba1\u7b97\u901a\u9053\u7ef4\u4e0a\uff08axis=1\uff09\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002 # \u8fd9\u91cc\u6211\u4eec\u9700\u8981\u4fdd\u6301X\u7684\u5f62\u72b6\u4ee5\u4fbf\u540e\u9762\u53ef\u4ee5\u505a\u5e7f\u64ad\u8fd0\u7b97 mean = X.mean(dim=(0, 2, 3), keepdim=True) var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True) # \u8bad\u7ec3\u6a21\u5f0f\u4e0b\uff0c\u7528\u5f53\u524d\u7684\u5747\u503c\u548c\u65b9\u5dee\u505a\u6807\u51c6\u5316 X_hat = (X - mean) / torch.sqrt(var + eps) # \u66f4\u65b0\u79fb\u52a8\u5e73\u5747\u7684\u5747\u503c\u548c\u65b9\u5dee moving_mean = momentum * moving_mean + (1.0 - momentum) * mean moving_var = momentum * moving_var + (1.0 - momentum) * var Y = gamma * X_hat + beta # \u7f29\u653e\u548c\u79fb\u4f4d return Y, moving_mean.data, moving_var.data class BatchNorm(nn.Module): # num_features\uff1a\u5b8c\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\u6570\u91cf\u6216\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u3002 # num_dims\uff1a2\u8868\u793a\u5b8c\u5168\u8fde\u63a5\u5c42\uff0c4\u8868\u793a\u5377\u79ef\u5c42 def __init__(self, num_features, num_dims): super().__init__() if num_dims == 2: shape = (1, num_features) else: shape = (1, num_features, 1, 1) # \u53c2\u4e0e\u6c42\u68af\u5ea6\u548c\u8fed\u4ee3\u7684\u62c9\u4f38\u548c\u504f\u79fb\u53c2\u6570\uff0c\u5206\u522b\u521d\u59cb\u5316\u62101\u548c0 self.gamma = nn.Parameter(torch.ones(shape)) self.beta = nn.Parameter(torch.zeros(shape)) # \u975e\u6a21\u578b\u53c2\u6570\u7684\u53d8\u91cf\u521d\u59cb\u5316\u4e3a0\u548c1 self.moving_mean = torch.zeros(shape) self.moving_var = torch.ones(shape) def forward(self, X): # \u5982\u679cX\u4e0d\u5728\u5185\u5b58\u4e0a\uff0c\u5c06moving_mean\u548cmoving_var # \u590d\u5236\u5230X\u6240\u5728\u663e\u5b58\u4e0a if self.moving_mean.device != X.device: self.moving_mean = self.moving_mean.to(X.device) self.moving_var = self.moving_var.to(X.device) # \u4fdd\u5b58\u66f4\u65b0\u8fc7\u7684moving_mean\u548cmoving_var Y, self.moving_mean, self.moving_var = batch_norm( X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9) return Y pytorch\u4f7f\u7528batch_norm: nn.BatchNorm1d(),nn.BatchNorm2d() \uff0c\u53c2\u6570\u4e3a\u901a\u9053\u6570\uff0c\u5206\u522b\u8868\u793a\u5168\u8fde\u63a5\u5c42\u548c\u5377\u79ef\u5c42","title":"Batch-Norm"},{"location":"DeepLearning/#resnet","text":"\u8f93\u5165X\u548c\u8f93\u51faY\u7684\u5f62\u72b6\u8981\u76f8\u540c,3$\\times$3\u7684\u5377\u79ef\u6838\u6ca1\u6709\u6539\u53d8\u5f62\u72b6\uff0c\u4e3b\u8981\u662f\u901a\u9053\u6570\uff0c\u4e5f\u53ef\u4ee5\u75281$\\times$1\u7684\u5377\u79ef\u6838\u4fee\u6539X\u7684\u901a\u9053\u6570 class Residual(nn.Module): def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1): super().__init__() self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides) self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1) if use_1x1conv: self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides) else: self.conv3 = None self.bn1 = nn.BatchNorm2d(num_channels) self.bn2 = nn.BatchNorm2d(num_channels) def forward(self, X): Y = F.relu(self.bn1(self.conv1(X))) Y = self.bn2(self.conv2(Y)) if self.conv3: X = self.conv3(X) Y += X return F.relu(Y)","title":"ResNet"},{"location":"DeepLearning/#densenet","text":"\u7f51\u7edc\u4e3b\u8981\u7531\u4e24\u90e8\u5206 \u7a20\u5bc6\u5c42\u548c\u8fc7\u6e21\u5c42 \u628a\u524d\u9762\u7f51\u7edc\u5c42\u7684\u8f93\u51fa\u90fd\u4f5c\u4e3a\u8f93\u51fa\uff0c\u6bcf\u4e2a\u7f51\u7edc\u7684\u8f93\u5165\u4e3a\u524d\u9762\u6240\u6709\u7f51\u7edc\u5c42\u7684\u8f93\u51fa\u52a0\u4e0ainput def conv_block(input_channels, num_channels): return nn.Sequential( nn.BatchNorm2d(input_channels), nn.ReLU(), nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1)) class DenseBlock(nn.Module): def __init__(self, num_convs, input_channels, num_channels): super(DenseBlock, self).__init__() layer = [] for i in range(num_convs): layer.append(conv_block( num_channels * i + input_channels, num_channels)) self.net = nn.Sequential(*layer) def forward(self, X): for blk in self.net: Y = blk(X) # \u8fde\u63a5\u901a\u9053\u7ef4\u5ea6\u4e0a\u6bcf\u4e2a\u5757\u7684\u8f93\u5165\u548c\u8f93\u51fa X = torch.cat((X, Y), dim=1) return X \u901a\u8fc7\u8fc7\u6e21\u5c42\u901a\u9053\u6570\u51cf\u5c11\uff0c\u9ad8\u548c\u5bbd\u51cf\u534a def transition_block(input_channels, num_channels): return nn.Sequential( nn.BatchNorm2d(input_channels), nn.ReLU(), nn.Conv2d(input_channels, num_channels, kernel_size=1), nn.AvgPool2d(kernel_size=2, stride=2))","title":"DenseNet"},{"location":"DeepLearning/#rnn","text":"","title":"RNN"},{"location":"DeepLearning/#_9","text":"\u901a\u8fc7$\\mathbf{x} t = [x {t-\\tau}, \\ldots, x_{t-1}]$ \u7684\u53d6\u503c\uff0c\u9884\u6d4b$y_t = x_t$\u3002","title":"\u5e8f\u5217\u6a21\u578b"},{"location":"DeepLearning/#_10","text":"\u8bfb\u53d6\u6570\u636e\u96c6\u5f97\u5230\u6240\u6709\u6587\u672c\u884c\uff08lines\uff09 the time machine by h g wells \u8bcd\u5143\u5316\uff08tokenize\uff09 \u628alines\u5206\u6210\u4e00\u4e2a\u4e00\u4e2a\u8bcd ['the', 'time', 'machine', 'by', 'h', 'g', 'wells'] \u5efa\u7acb\u8bcd\u8868\uff08vocabulary\uff09 class Vocab: \"\"\"\u6587\u672c\u8bcd\u8868\"\"\" def __init__(self, tokens=None, min_freq=0, reserved_tokens=None): if tokens is None: tokens = [] if reserved_tokens is None: reserved_tokens = [] # \u6309\u51fa\u73b0\u9891\u7387\u6392\u5e8f counter = count_corpus(tokens) self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # \u672a\u77e5\u8bcd\u5143\u7684\u7d22\u5f15\u4e3a0 self.idx_to_token = ['<unk>'] + reserved_tokens self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)} for token, freq in self._token_freqs: if freq < min_freq: break if token not in self.token_to_idx: self.idx_to_token.append(token) self.token_to_idx[token] = len(self.idx_to_token) - 1 def __len__(self): return len(self.idx_to_token) def __getitem__(self, tokens):#\u7ed9token\u8fd4\u56deindex if not isinstance(tokens, (list, tuple)): return self.token_to_idx.get(tokens, self.unk) return [self.__getitem__(token) for token in tokens] def to_tokens(self, indices):#\u7ed9index\u8fd4\u56detoken if not isinstance(indices, (list, tuple)): return self.idx_to_token[indices] return [self.idx_to_token[index] for index in indices] @property def unk(self): # \u672a\u77e5\u8bcd\u5143\u7684\u7d22\u5f15\u4e3a0 return 0 @property def token_freqs(self): return self._token_freqs def count_corpus(tokens): #@save \"\"\"\u7edf\u8ba1\u8bcd\u5143\u7684\u9891\u7387\"\"\" # \u8fd9\u91cc\u7684tokens\u662f1D\u5217\u8868\u62162D\u5217\u8868 if len(tokens) == 0 or isinstance(tokens[0], list): # \u5c06\u8bcd\u5143\u5217\u8868\u5c55\u5e73\u6210\u4e00\u4e2a\u5217\u8868 tokens = [token for line in tokens for token in line] return collections.Counter(tokens) \u8f6c\u6362\u7ed3\u679c\uff08\u8bcd\u9891\u9ad8\u4e0b\u6807\u5c0f\uff09 vocab = Vocab(tokens) print(list(vocab.token_to_idx.items())[:10]) [(' ', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]","title":"\u6587\u672c\u9884\u5904\u7406"},{"location":"DeepLearning/#language-model","text":"n\u5143\u8bed\u6cd5 $$ \\begin{aligned} P(x_1, x_2, x_3, x_4) &= P(x_1) P(x_2) P(x_3) P(x_4),\\ P(x_1, x_2, x_3, x_4) &= P(x_1) P(x_2 \\mid x_1) P(x_3 \\mid x_2) P(x_4 \\mid x_3),\\ P(x_1, x_2, x_3, x_4) &= P(x_1) P(x_2 \\mid x_1) P(x_3 \\mid x_1, x_2) P(x_4 \\mid x_2, x_3). \\end{aligned} $$ \u5f97\u5230\u4e8c\u5143\u8bcd bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])] bigram_vocab = d2l.Vocab(bigram_tokens) \u7b2c$i$\u4e2a\u6700\u5e38\u7528\u7684\u9891\u7387$n_i$ $$\\log n_i = -\\alpha \\log i + c,$$ \u867d\u7136n\u5143\u8bcd\u7ec4\u5408\u65b9\u5f0f\u589e\u52a0\uff0c\u4f46\u662f\u7531\u4e8e\u5927\u81f4\u9075\u5faa\u4e0a\u8ff0\u89c4\u5f8b\uff0c\u5c06\u8bcd\u9891\u5c0f\u4e8e\u67d0\u4e2a\u503c\u7684\u7565\u53bb\uff0c\u8bcd\u8868\u4e2d\u8bb0\u5f55\u53cd\u800c\u66f4\u5c11","title":"Language Model"},{"location":"DeepLearning/#_11","text":"","title":"\u8bfb\u53d6\u957f\u5e8f\u5217\u6570\u636e"},{"location":"DeepLearning/#_12","text":"def seq_data_iter_random(corpus, batch_size, num_steps): \"\"\"\u4f7f\u7528\u968f\u673a\u62bd\u6837\u751f\u6210\u4e00\u4e2a\u5c0f\u6279\u91cf\u5b50\u5e8f\u5217\"\"\" # \u4ece\u968f\u673a\u504f\u79fb\u91cf\u5f00\u59cb\u5bf9\u5e8f\u5217\u8fdb\u884c\u5206\u533a\uff0c\u968f\u673a\u8303\u56f4\u5305\u62ecnum_steps-1 corpus = corpus[random.randint(0, num_steps - 1):] # \u51cf\u53bb1\uff0c\u662f\u56e0\u4e3a\u6211\u4eec\u9700\u8981\u8003\u8651\u6807\u7b7e num_subseqs = (len(corpus) - 1) // num_steps # \u957f\u5ea6\u4e3anum_steps\u7684\u5b50\u5e8f\u5217\u7684\u8d77\u59cb\u7d22\u5f15 initial_indices = list(range(0, num_subseqs * num_steps, num_steps)) # \u5728\u968f\u673a\u62bd\u6837\u7684\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\uff0c # \u6765\u81ea\u4e24\u4e2a\u76f8\u90bb\u7684\u3001\u968f\u673a\u7684\u3001\u5c0f\u6279\u91cf\u4e2d\u7684\u5b50\u5e8f\u5217\u4e0d\u4e00\u5b9a\u5728\u539f\u59cb\u5e8f\u5217\u4e0a\u76f8\u90bb random.shuffle(initial_indices) def data(pos): # \u8fd4\u56de\u4ecepos\u4f4d\u7f6e\u5f00\u59cb\u7684\u957f\u5ea6\u4e3anum_steps\u7684\u5e8f\u5217 return corpus[pos: pos + num_steps] num_batches = num_subseqs // batch_size for i in range(0, batch_size * num_batches, batch_size): # \u5728\u8fd9\u91cc\uff0cinitial_indices\u5305\u542b\u5b50\u5e8f\u5217\u7684\u968f\u673a\u8d77\u59cb\u7d22\u5f15 initial_indices_per_batch = initial_indices[i: i + batch_size] X = [data(j) for j in initial_indices_per_batch] Y = [data(j + 1) for j in initial_indices_per_batch] yield torch.tensor(X), torch.tensor(Y) \u751f\u6210\u6837\u4f8b(0-34\u5e8f\u5217,batch_size=2,num_steps=5) X: tensor([ [13, 14, 15, 16, 17], [28, 29, 30, 31, 32] ]) Y: tensor([ [14, 15, 16, 17, 18], [29, 30, 31, 32, 33] ]) X: tensor([ [ 3, 4, 5, 6, 7], [18, 19, 20, 21, 22] ]) Y: tensor([ [ 4, 5, 6, 7, 8], [19, 20, 21, 22, 23] ]) X: tensor([ [ 8, 9, 10, 11, 12], [23, 24, 25, 26, 27] ]) Y: tensor([ [ 9, 10, 11, 12, 13], [24, 25, 26, 27, 28] ])","title":"\u968f\u673a\u91c7\u6837"},{"location":"DeepLearning/#_13","text":"def seq_data_iter_sequential(corpus, batch_size, num_steps): \"\"\"\u4f7f\u7528\u987a\u5e8f\u5206\u533a\u751f\u6210\u4e00\u4e2a\u5c0f\u6279\u91cf\u5b50\u5e8f\u5217\"\"\" # \u4ece\u968f\u673a\u504f\u79fb\u91cf\u5f00\u59cb\u5212\u5206\u5e8f\u5217 offset = random.randint(0, num_steps) num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size Xs = torch.tensor(corpus[offset: offset + num_tokens]) Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens]) Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1) num_batches = Xs.shape[1] // num_steps for i in range(0, num_steps * num_batches, num_steps): X = Xs[:, i: i + num_steps] Y = Ys[:, i: i + num_steps] yield X, Y \u751f\u6210\u6837\u4f8b X: tensor([ [ 0, 1, 2, 3, 4], [17, 18, 19, 20, 21] ]) Y: tensor([ [ 1, 2, 3, 4, 5], [18, 19, 20, 21, 22] ]) X: tensor([ [ 5, 6, 7, 8, 9], [22, 23, 24, 25, 26] ]) Y: tensor([ [ 6, 7, 8, 9, 10], [23, 24, 25, 26, 27] ]) X: tensor([ [10, 11, 12, 13, 14], [27, 28, 29, 30, 31] ]) Y: tensor([ [11, 12, 13, 14, 15], [28, 29, 30, 31, 32] ])","title":"\u987a\u5e8f\u91c7\u6837"},{"location":"DeepLearning/#rnn_1","text":"\u66f4\u65b0\u9690\u85cf\u72b6\u6001 $$ h_t = \\phi(W_{hh} h_{t-1} + W_{hx}\\mathbf{x} {t-1}+b_h) $$ \u8f93\u51fa $$ o_t = \\phi(W {ho} h_t + b_o) $$ \u56f0\u60d1\u5ea6(perplexity) \u8861\u91cf\u8bed\u8a00\u6a21\u578b\u7684\u597d\u574f\u53ef\u4ee5\u7528\u5e73\u5747\u4ea4\u53c9\u71b5 $$\\pi = \\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1),$$ \u56f0\u60d1\u5ea6\u4e3a$\\exp(\\pi)$ \u68af\u5ea6\u526a\u88c1 \u9884\u9632\u68af\u5ea6\u7206\u70b8 \u68af\u5ea6\u957f\u5ea6\u8d85\u8fc7$\\theta$\uff0c\u90a3\u4e48\u62d6\u56de\u957f\u5ea6$\\theta$ $$ g \\leftarrow \\min(1,\\frac{\\theta}{\\Vert \\mathbf{g} \\Vert}) \\mathbf{g} $$ \u53c2\u6570\u521d\u59cb\u5316 : def get_params(vocab_size, num_hiddens, device): num_inputs = num_outputs = vocab_size #\u56e0\u4e3a\u8fd9\u91cc\u7528\u7684onehot\u7801 def normal(shape): return torch.randn(size=shape, device=device) * 0.01 # \u9690\u85cf\u5c42\u53c2\u6570 W_xh = normal((num_inputs, num_hiddens)) W_hh = normal((num_hiddens, num_hiddens)) b_h = torch.zeros(num_hiddens, device=device) # \u8f93\u51fa\u5c42\u53c2\u6570 W_hq = normal((num_hiddens, num_outputs)) b_q = torch.zeros(num_outputs, device=device) # \u9644\u52a0\u68af\u5ea6 params = [W_xh, W_hh, b_h, W_hq, b_q] for param in params: param.requires_grad_(True) return params #\u521d\u59cb\u5316\u9690\u85cf\u72b6\u6001 def init_rnn_state(batch_size, num_hiddens, device): return (torch.zeros((batch_size, num_hiddens), device=device), ) $X$\u7684\u5f62\u72b6\u4e3a\uff08batch_size,vocab_size\uff09\uff0c\u6267\u884c\u64cd\u4f5c $$ H_{b \\times h} = \\phi(X_{b \\times x}W_{x \\times h} + H_{b \\times h}W_{h \\times h } + b_h(Broadcast)) $$ \u53ef\u4ee5\u770b\u51fa\u5bf9\u4e8e\u6bcf\u4e2a\u6279\u91cf\u7684\u9690\u72b6\u6001\u662f\u72ec\u7acb\u5b58\u50a8\u66f4\u65b0\u7684 \u8f93\u51fa$Y$\u7684\u5f62\u72b6\u4e3a(batch_size,vocab_size)\uff0ccat\u5b8c\u4e86return\u7684\u5f62\u72b6\u4e3a(time_steps$\\times$batch_size,vocab_size) def rnn(inputs, state, params): # inputs\u7684\u5f62\u72b6\uff1a(\u65f6\u95f4\u6b65\u6570\u91cf\uff0c\u6279\u91cf\u5927\u5c0f\uff0c\u8bcd\u8868\u5927\u5c0f) W_xh, W_hh, b_h, W_hq, b_q = params H, = state outputs = [] # X\u7684\u5f62\u72b6\uff1a(\u6279\u91cf\u5927\u5c0f\uff0c\u8bcd\u8868\u5927\u5c0f) for X in inputs: H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh)+b_h)#torch.mm\u8868\u793a\u77e9\u9635\u4e58\u8d77\u6765 Y = torch.mm(H, W_hq) + b_q outputs.append(Y) return torch.cat(outputs, dim=0), (H,) \u5b8c\u6574\u7684\u5982\u4e0b class RNNModelScratch: def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn): self.vocab_size, self.num_hiddens = vocab_size, num_hiddens self.params = get_params(vocab_size, num_hiddens, device) self.init_state, self.forward_fn = init_state, forward_fn def __call__(self, X, state): X = F.one_hot(X.T, self.vocab_size).type(torch.float32) return self.forward_fn(X, state, self.params) def begin_state(self, batch_size, device): return self.init_state(batch_size, self.num_hiddens, device) num_hiddens = 512 net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn) state = net.begin_state(X.shape[0], d2l.try_gpu()) Y, new_state = net(X.to(d2l.try_gpu()), state) \u4f7f\u7528\u7f51\u7edc\u8fdb\u884c\u9884\u6d4b\uff0c\u524d\u51e0\u4e2a\uff08prefix\uff09\u4e0d\u4ea7\u751f\u8f93\u51fa\u4f46\u662f\u66f4\u65b0\u9690\u72b6\u6001 def predict_ch8(prefix, num_preds, net, vocab, device): #@save \"\"\"\u5728prefix\u540e\u9762\u751f\u6210\u65b0\u5b57\u7b26\"\"\" state = net.begin_state(batch_size=1, device=device) outputs = [vocab[prefix[0]]] get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1)) #\u7528\u4e0a\u6b21\u8f93\u51fa\u7684\u6700\u540e\u7684\u5b57\u7b26 #\u4e00\u4e2a\u4e00\u4e2a\u8fdb\u53bb\u5f97\u5230\u8f93\u51fa for y in prefix[1:]: # \u9884\u70ed\u671f _, state = net(get_input(), state) outputs.append(vocab[y]) for _ in range(num_preds): # \u9884\u6d4bnum_preds\u6b65 y, state = net(get_input(), state) outputs.append(int(y.argmax(dim=1).reshape(1))) return ''.join([vocab.idx_to_token[i] for i in outputs]) \u8bad\u7ec3\u793a\u4f8b def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter): state, timer = None, d2l.Timer() metric = d2l.Accumulator(2) # \u8bad\u7ec3\u635f\u5931\u4e4b\u548c,\u8bcd\u5143\u6570\u91cf for X, Y in train_iter: if state is None or use_random_iter: # \u5728\u7b2c\u4e00\u6b21\u8fed\u4ee3\u6216\u4f7f\u7528\u968f\u673a\u62bd\u6837\u65f6\u8981\u91cd\u65b0\u521d\u59cb\u5316state state = net.begin_state(batch_size=X.shape[0], device=device) else: if isinstance(net, nn.Module) and not isinstance(state, tuple): # state\u5bf9\u4e8enn.GRU\u662f\u4e2a\u5f20\u91cf state.detach_() else: for s in state: s.detach_() y = Y.T.reshape(-1) X, y = X.to(device), y.to(device) y_hat, state = net(X, state) l = loss(y_hat, y.long()).mean() #\u8fd9\u91ccy_hat\u4e3a\u4e8c\u7ef4\u5f20\u91cf\uff0cy\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u7c7b\u4f3c\u4e8e\u591a\u5206\u7c7b\u95ee\u9898 if isinstance(updater, torch.optim.Optimizer): updater.zero_grad() l.backward() grad_clipping(net, 1)#\u68af\u5ea6\u526a\u88c1 updater.step() else: l.backward() grad_clipping(net, 1) # \u56e0\u4e3a\u5df2\u7ecf\u8c03\u7528\u4e86mean\u51fd\u6570 updater(batch_size=1) metric.add(l * y.numel(), y.numel())#numel\u51fd\u6570\u8fd4\u56de\u5f20\u91cf\u5143\u7d20\u603b\u6570\u91cf\uff0c return math.exp(metric[0] / metric[1]), metric[1] / timer.stop() \u4f7f\u7528pytorch API rnn_layer = nn.RNN(len(vocab), num_hiddens) state = torch.zeros((1, batch_size, num_hiddens))#(\u9690\u85cf\u5c42\u6570\uff0c\u6279\u91cf\u5927\u5c0f\uff0c\u9690\u53d8\u91cf\u957f\u5ea6) X = torch.rand(size=(num_steps, batch_size, len(vocab))) Y, state_new = rnn_layer(X, state)","title":"RNN\u5b9e\u73b0"},{"location":"DeepLearning/#_14","text":"\u4e3b\u8981\u662f\u5faa\u73af\u8ba1\u7b97\u68af\u5ea6\u7684\u65b9\u6cd5\u548c\u8fd1\u4f3c\u65b9\u6cd5\uff0ct\u5927\u65f6\u4ea7\u751f\u68af\u5ea6\u6d88\u5931\u6216\u8005\u7206\u70b8\u7684\u95ee\u9898 \u5177\u4f53\u53c2\u8003\u52a8\u624b\u6df1\u5ea6\u5b66\u4e60bptt\u7ae0\u8282","title":"\u53cd\u5411\u4f20\u64ad"},{"location":"DeepLearning/#gru","text":"\u91cd\u7f6e\u95e8\u548c\u66f4\u65b0\u95e8($\\mathbb{R} {b \\times h}$)\u66f4\u65b0 $$ \\begin{aligned} \\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W} {xr} + \\mathbf{H} {t-1} \\mathbf{W} {hr} + \\mathbf{b} r),\\ \\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W} {xz} + \\mathbf{H} {t-1} \\mathbf{W} {hz} + \\mathbf{b}_z), \\end{aligned} $$ ($\\sigma \u4e3a $sigmoid\u51fd\u6570) \u9690\u5019\u9009\u72b6\u6001 $$\\tilde{\\mathbf{H}} t = \\tanh(\\mathbf{X}_t \\mathbf{W} {xh} + \\left(\\mathbf{R} t \\odot \\mathbf{H} {t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h),$$ \u9690\u72b6\u6001\u66f4\u65b0 $$\\mathbf{H} t = \\mathbf{Z}_t \\odot \\mathbf{H} {t-1} + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.$$ \u76f8\u6bd4\u524d\u9762\u57fa\u672cRNN\u53ea\u662f\u9690\u72b6\u6001\u66f4\u65b0\u516c\u5f0f\u66f4\u4e3a\u590d\u6742 pytorch\u6846\u67b6 num_inputs = vocab_size gru_layer = nn.GRU(num_inputs, num_hiddens) model = d2l.RNNModel(gru_layer, len(vocab)) d2l.RNNModel class RNNModel(nn.Module): \"\"\"\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\"\"\" def __init__(self, rnn_layer, vocab_size, **kwargs): super(RNNModel, self).__init__(**kwargs) self.rnn = rnn_layer self.vocab_size = vocab_size self.num_hiddens = self.rnn.hidden_size # \u5982\u679cRNN\u662f\u53cc\u5411\u7684\uff08\u4e4b\u540e\u5c06\u4ecb\u7ecd\uff09\uff0cnum_directions\u5e94\u8be5\u662f2\uff0c\u5426\u5219\u5e94\u8be5\u662f1 if not self.rnn.bidirectional: self.num_directions = 1 self.linear = nn.Linear(self.num_hiddens, self.vocab_size) else: self.num_directions = 2 self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size) def forward(self, inputs, state): X = F.one_hot(inputs.T.long(), self.vocab_size) X = X.to(torch.float32) Y, state = self.rnn(X, state) # \u5168\u8fde\u63a5\u5c42\u9996\u5148\u5c06Y\u7684\u5f62\u72b6\u6539\u4e3a(\u65f6\u95f4\u6b65\u6570*\u6279\u91cf\u5927\u5c0f,\u9690\u85cf\u5355\u5143\u6570) # \u5b83\u7684\u8f93\u51fa\u5f62\u72b6\u662f(\u65f6\u95f4\u6b65\u6570*\u6279\u91cf\u5927\u5c0f,\u8bcd\u8868\u5927\u5c0f)\u3002 output = self.linear(Y.reshape((-1, Y.shape[-1]))) return output, state def begin_state(self, device, batch_size=1): if not isinstance(self.rnn, nn.LSTM): # nn.GRU\u4ee5\u5f20\u91cf\u4f5c\u4e3a\u9690\u72b6\u6001 return torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device) else: # nn.LSTM\u4ee5\u5143\u7ec4\u4f5c\u4e3a\u9690\u72b6\u6001 return (torch.zeros(( self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device), torch.zeros(( self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device))","title":"GRU"},{"location":"DeepLearning/#lstm","text":"\u8f93\u5165\u95e8\u662f$\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}$\uff0c \u9057\u5fd8\u95e8\u662f$\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}$\uff0c \u8f93\u51fa\u95e8\u662f$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}$\u3002 \u66f4\u65b0\u516c\u5f0f\u4e3a $$ \\begin{aligned} \\mathbf{I} t &= \\sigma(\\mathbf{X}_t \\mathbf{W} {xi} + \\mathbf{H} {t-1} \\mathbf{W} {hi} + \\mathbf{b} i),\\ \\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W} {xf} + \\mathbf{H} {t-1} \\mathbf{W} {hf} + \\mathbf{b} f),\\ \\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W} {xo} + \\mathbf{H} {t-1} \\mathbf{W} {ho} + \\mathbf{b}_o), \\end{aligned} $$ \u5019\u9009\u8bb0\u5fc6\u5143 $$\\tilde{\\mathbf{C}} t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W} {xc} + \\mathbf{H} {t-1} \\mathbf{W} {hc} + \\mathbf{b}_c),$$ \u8bb0\u5fc6\u5143 $$\\mathbf{C} t = \\mathbf{F}_t \\odot \\mathbf{C} {t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t.$$ \u9690\u72b6\u6001 $$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t).$$ pytorch\u6846\u67b6 num_inputs = vocab_size lstm_layer = nn.LSTM(num_inputs, num_hiddens) model = d2l.RNNModel(lstm_layer, len(vocab))","title":"LSTM"},{"location":"DeepLearning/#deep-rnn","text":"\u8bbe\u7f6e$\\mathbf{H}_t^{(0)} = \\mathbf{X}_t$\uff0c \u7b2c$l$\u5c42\u7684\u9690\u72b6\u6001\u66f4\u65b0 $$\\mathbf{H} t^{(l)} = \\phi_l(\\mathbf{H}_t^{(l-1)} \\mathbf{W} {xh}^{(l)} + \\mathbf{H} {t-1}^{(l)} \\mathbf{W} {hh}^{(l)} + \\mathbf{b}_h^{(l)}),$$ \u6700\u540e\uff0c\u8f93\u51fa\u5c42\u7684\u8ba1\u7b97\u4ec5\u57fa\u4e8e\u7b2c$l$\u4e2a\u9690\u85cf\u5c42\u6700\u7ec8\u7684\u9690\u72b6\u6001\uff1a $$\\mathbf{O} t = \\mathbf{H}_t^{(L)} \\mathbf{W} {hq} + \\mathbf{b}_q,$$ \u4e5f\u80fd\u7528GRU\u6216\u8005LSTM\u6765\u4ee3\u66ff\u8fd9\u91cc\u7684\u9690\u72b6\u6001 pytorch\u5b9e\u73b0\u6df1\u5ea6\u7684LSTM lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers) model = d2l.RNNModel(lstm_layer, len(vocab))","title":"Deep RNN"},{"location":"DeepLearning/#brnn","text":"$$ \\begin{aligned} \\overrightarrow{\\mathbf{H}} t &= \\phi(\\mathbf{X}_t \\mathbf{W} {xh}^{(f)} + \\overrightarrow{\\mathbf{H}} {t-1} \\mathbf{W} {hh}^{(f)} + \\mathbf{b} h^{(f)}),\\ \\overleftarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W} {xh}^{(b)} + \\overleftarrow{\\mathbf{H}} {t+1} \\mathbf{W} {hh}^{(b)} + \\mathbf{b}_h^{(b)}), \\end{aligned} $$ \u6b63\u5411\u9690\u72b6\u6001$\\overrightarrow{\\mathbf{H}}_t$\u548c\u53cd\u5411\u9690\u72b6\u6001$\\overleftarrow{\\mathbf{H}}_t$\u8fde\u63a5\u8d77\u6765\uff0c\u83b7\u5f97\u9700\u8981\u9001\u5165\u8f93\u51fa\u5c42\u7684\u9690\u72b6\u6001$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}$\u3002 $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$\uff08$q$\u662f\u8f93\u51fa\u5355\u5143\u7684\u6570\u76ee\uff09: $$\\mathbf{O} t = \\mathbf{H}_t \\mathbf{W} {hq} + \\mathbf{b}_q.$$ \u8fd9\u91cc$\\mathbf{W}_{hq} \\in \\mathbb{R}^{2h \\times q}$","title":"BRNN"},{"location":"DeepLearning/#seq2seq","text":"\u673a\u5668\u7ffb\u8bd1 \u7f16\u7801\u5668\u662f\u4e00\u4e2aRNN(\u53ef\u4ee5\u662f\u53cc\u5411)\uff0c\u8bfb\u53d6\u8f93\u5165\u53e5\u5b50 \u89e3\u7801\u5668\u7528\u53e6\u4e00\u4e2aRNN\u6765\u8f93\u51fa \u7f16\u7801\u5668\u6700\u540e\u65f6\u95f4\u6b65\u7684\u9690\u72b6\u6001\u7528\u4f5c\u7f16\u7801\u5668\u7684\u521d\u59cb\u9690\u72b6\u6001 \u8bad\u7ec3\u65f6\u89e3\u7801\u5668\u4f7f\u7528\u76ee\u6807\u53e5\u5b50\u4f5c\u4e3a\u8f93\u5165\uff08\u5c31\u7b97\u67d0\u4e00\u6b65\u9884\u6d4b\u9519\u4e86\uff0c\u4e0b\u4e00\u6b65\u8f93\u5165\u7684\u8fd8\u662f\u6b63\u786e\u7684\u7ffb\u8bd1\u7ed3\u679c\uff09","title":"Seq2Seq"},{"location":"DeepLearning/#_15","text":"\u8861\u91cf\u751f\u6210\u5e8f\u5217\u597d\u574f\u7684BLEU $$ \\exp\\left(\\min\\left(0, 1 - \\frac{\\mathrm{len} {\\text{label}}}{\\mathrm{len} {\\text{pred}}}\\right)\\right) \\prod_{n=1}^k p_n^{1/2^n},$$ $p_n$\u8868\u793a\u9884\u6d4b\u4e2d\u6240\u6709n_gram\uff08n\u5143\u8bed\u6cd5\uff09\u7684\u7cbe\u5ea6 Example \u6807\u7b7e\u5e8f\u5217ABCDEF\uff0c\u9884\u6d4b\u5e8f\u5217\u4e3aABBCD $p_1$ = 4/5 $p_2$ = 3/4 $p_3$ = 1/3 $p_4$ = 0 class Seq2SeqEncoder(d2l.Encoder): def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqEncoder, self).__init__(**kwargs) # \u5d4c\u5165\u5c42 self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout) def forward(self, X, *args): # \u8f93\u5165'X'\u5f62\u72b6(batch_size,num_steps) \u8f93\u51fa'X'\u7684\u5f62\u72b6\uff1a(batch_size,num_steps,embed_size) X = self.embedding(X) # \u5728\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\uff0c\u7b2c\u4e00\u4e2a\u8f74\u5bf9\u5e94\u4e8e\u65f6\u95f4\u6b65 X = X.permute(1, 0, 2) # \u5982\u679c\u672a\u63d0\u53ca\u72b6\u6001\uff0c\u5219\u9ed8\u8ba4\u4e3a0 output, state = self.rnn(X) # output\u7684\u5f62\u72b6:(num_steps,batch_size,num_hiddens) # state\u7684\u5f62\u72b6:(num_layers,batch_size,num_hiddens) return output, state Decoder\u7684embedding\u548cEncoder\u4e0d\u540c class Seq2SeqDecoder(d2l.Decoder): \"\"\"\u7528\u4e8e\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u89e3\u7801\u5668\"\"\" def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqDecoder, self).__init__(**kwargs) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, *args): #outputs:(output,state) return enc_outputs[1] def forward(self, X, state): # \u8f93\u51fa'X'\u7684\u5f62\u72b6\uff1a(batch_size,num_steps,embed_size) X = self.embedding(X).permute(1, 0, 2) # \u5e7f\u64adcontext\uff0c\u4f7f\u5176\u5177\u6709\u4e0eX\u76f8\u540c\u7684num_steps context = state[-1].repeat(X.shape[0], 1, 1)#\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u6700\u540e\u4e00\u5c42\u8f93\u51fa X_and_context = torch.cat((X, context), 2) output, state = self.rnn(X_and_context, state) output = self.dense(output).permute(1, 0, 2) # output\u7684\u5f62\u72b6:(batch_size,num_steps,vocab_size) # state\u7684\u5f62\u72b6:(num_layers,batch_size,num_hiddens) return output, state \u5728\u5e8f\u5217\u4e2d\u5c4f\u853d\u4e0d\u76f8\u5173\u7684\u9879 def sequence_mask(X, valid_len, value=0): maxlen = X.size(1) mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None] X[~mask] = value return X X = torch.tensor([[1, 2, 3], [4, 5, 6]]) sequence_mask(X, torch.tensor([1, 2])) tensor([ [1, 0, 0], [4, 5, 0] ]) class MaskedSoftmaxCELoss(nn.CrossEntropyLoss): \"\"\"\u5e26\u906e\u853d\u7684softmax\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\"\"\" # pred\u7684\u5f62\u72b6\uff1a(batch_size,num_steps,vocab_size) # label\u7684\u5f62\u72b6\uff1a(batch_size,num_steps) # valid_len\u7684\u5f62\u72b6\uff1a(batch_size,) def forward(self, pred, label, valid_len): weights = torch.ones_like(label) weights = sequence_mask(weights, valid_len) self.reduction='none' unweighted_loss = super(MaskedSoftmaxCELoss, self).forward( pred.permute(0, 2, 1), label) weighted_loss = (unweighted_loss * weights).mean(dim=1) #\u65e0\u6548\u7684\u5168\u90e8\u7f6e\u4e3a0\uff0c\u5bf9\u6bcf\u4e2a\u6837\u672c\u8fd4\u56deloss return weighted_loss \uff08\u5177\u4f53\u6ca1\u770b\u5f88\u61c2\u600e\u4e48\u7ec3\u7684\uff0c\u5148\u8fc7\u4e86\u540e\u9762\u8865","title":"\u9884\u6d4b\u5e8f\u5217\u8bc4\u4f30"},{"location":"DeepLearning/#attention","text":"\u6bcf\u4e2a\u503c\uff08Value\uff09\u548c\u4e00\u4e2a\u952e\uff08key\uff09\u4e00\u4e00\u5bf9\u5e94\uff0c\u901a\u8fc7\u67e5\u8be2\uff08query\uff09\u4e0e\u952e\u8fdb\u884c\u5339\u914d\uff0c\u5f97\u5230\u6700\u5339\u914d\u7684\u503c\u3002","title":"Attention"},{"location":"DeepLearning/#nadaraya-waston","text":"$$f(x) = \\sum_{i=1}^n \\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)} y_i$$ \u66f4\u4e00\u822c\u7684\u8868\u793a $$f(x) = \\sum_{i=1}^n \\alpha(x, x_i) y_i,$$ \u7528\u9ad8\u65af\u6838$K(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{u^2}{2}).$\u5e26\u5165\u7b2c\u4e00\u4e2a\u5f0f\u5b50 \u5f97\u5230 $$\\begin{aligned} f(x) &= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}(x - x_i)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}(x - x_j)^2\\right)} y_i \\&= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}(x - x_i)^2\\right) y_i. \\end{aligned}$$ \u5e26\u5b66\u4e60\u53c2\u6570w $$\\begin{aligned}f(x) &= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}((x - x_i)w)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}((x - x_j)w)^2\\right)} y_i \\&= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}((x - x_i)w)^2\\right) y_i.\\end{aligned}$$","title":"Nadaraya-Waston \u6838\u56de\u5f52"},{"location":"DeepLearning/#_16","text":"\u7528\u6570\u5b66\u8bed\u8a00\u63cf\u8ff0\uff0c\u5047\u8bbe\u6709\u4e00\u4e2a\u67e5\u8be2$\\mathbf{q} \\in \\mathbb{R}^q$\u548c$m$\u4e2a\u201c\u952e\uff0d\u503c\u201d\u5bf9$(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)$\uff0c \u5176\u4e2d$\\mathbf{k}_i \\in \\mathbb{R}^k$\uff0c$\\mathbf{v}_i \\in \\mathbb{R}^v$\u3002 \u6ce8\u610f\u529b\u6c47\u805a\u51fd\u6570$f$\u5c31\u88ab\u8868\u793a\u6210\u503c\u7684\u52a0\u6743\u548c\uff1a $$f(\\mathbf{q}, (\\mathbf{k} 1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)) = \\sum {i=1}^m \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i \\in \\mathbb{R}^v,$$ \u5176\u4e2d\u67e5\u8be2$\\mathbf{q}$\u548c\u952e$\\mathbf{k}_i$\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff08\u6807\u91cf\uff09\u662f\u901a\u8fc7\u6ce8\u610f\u529b\u8bc4\u5206\u51fd\u6570$a$\u5c06\u4e24\u4e2a\u5411\u91cf\u6620\u5c04\u6210\u6807\u91cf\uff0c\u518d\u7ecf\u8fc7softmax\u8fd0\u7b97\u5f97\u5230\u7684\uff1a $$\\alpha(\\mathbf{q}, \\mathbf{k} i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}_i))}{\\sum {j=1}^m \\exp(a(\\mathbf{q}, \\mathbf{k}_j))} \\in \\mathbb{R}.$$","title":"\u6ce8\u610f\u529b\u8bc4\u5206\u51fd\u6570"},{"location":"DeepLearning/#additive-attention","text":"$$a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R},$$ \u5176\u4e2d\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u662f$\\mathbf W_q\\in\\mathbb R^{h\\times q}$\u3001$\\mathbf W_k\\in\\mathbb R^{h\\times k}$\u548c$\\mathbf w_v\\in\\mathbb R^{h}$\u3002 class AdditiveAttention(nn.Module): def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs): super(AdditiveAttention, self).__init__(**kwargs) self.W_k = nn.Linear(key_size, num_hiddens, bias=False) self.W_q = nn.Linear(query_size, num_hiddens, bias=False) self.w_v = nn.Linear(num_hiddens, 1, bias=False) self.dropout = nn.Dropout(dropout) def forward(self, queries, keys, values, valid_lens): queries, keys = self.W_q(queries), self.W_k(keys) # \u5728\u7ef4\u5ea6\u6269\u5c55\u540e\uff0c # queries\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570\uff0c1\uff0cnum_hidden) # key\u7684\u5f62\u72b6\uff1a(batch_size\uff0c1\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0cnum_hiddens) # \u4f7f\u7528\u5e7f\u64ad\u65b9\u5f0f\u8fdb\u884c\u6c42\u548c features = queries.unsqueeze(2) + keys.unsqueeze(1) features = torch.tanh(features) # self.w_v\u4ec5\u6709\u4e00\u4e2a\u8f93\u51fa\uff0c\u56e0\u6b64\u4ece\u5f62\u72b6\u4e2d\u79fb\u9664\u6700\u540e\u90a3\u4e2a\u7ef4\u5ea6\u3002 # scores\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570\uff0c\u201c\u952e-\u503c\u201d\u5bf9\u7684\u4e2a\u6570) scores = self.w_v(features).squeeze(-1) self.attention_weights = masked_softmax(scores, valid_lens) # values\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0c\u503c\u7684\u7ef4\u5ea6) # \u8f93\u51fa\u7684\u5f62\u72b6\uff1a(batch_size,\u67e5\u8be2\u7684\u4e2a\u6570,\u503c\u7684\u7ef4\u5ea6) return torch.bmm(self.dropout(self.attention_weights), values)","title":"Additive Attention"},{"location":"DeepLearning/#scaled-dot-product-attention","text":"\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b \uff08scaled dot-product attention\uff09\u8981\u6c42query\u548cKey\u957f\u5ea6\u76f8\u540c\uff0c\u8bc4\u5206\u51fd\u6570\u4e3a\uff1a $$a(\\mathbf q, \\mathbf k) = \\mathbf{q}^\\top \\mathbf{k} /\\sqrt{d}.$$ \u57fa\u4e8e$n$\u4e2a\u67e5\u8be2\u548c$m$\u4e2a\u952e\uff0d\u503c\u5bf9\u8ba1\u7b97\u6ce8\u610f\u529b\uff0c\u5176\u4e2d\u67e5\u8be2\u548c\u952e\u7684\u957f\u5ea6\u5747\u4e3a$d$\uff0c\u503c\u7684\u957f\u5ea6\u4e3a$v$\u3002\u67e5\u8be2$\\mathbf Q\\in\\mathbb R^{n\\times d}$\u3001\u952e$\\mathbf K\\in\\mathbb R^{m\\times d}$\u548c\u503c$\\mathbf V\\in\\mathbb R^{m\\times v}$\u7684\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u662f\uff1a $$ \\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}.$$ class DotProductAttention(nn.Module): \"\"\"\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\"\"\" def __init__(self, dropout, **kwargs): super(DotProductAttention, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) # queries\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570\uff0cd) # keys\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0cd) # values\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0c\u503c\u7684\u7ef4\u5ea6) # valid_lens\u7684\u5f62\u72b6:(batch_size\uff0c)\u6216\u8005(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570) def forward(self, queries, keys, values, valid_lens=None): d = queries.shape[-1] # \u8bbe\u7f6etranspose_b=True\u4e3a\u4e86\u4ea4\u6362keys\u7684\u6700\u540e\u4e24\u4e2a\u7ef4\u5ea6 scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d) self.attention_weights = masked_softmax(scores, valid_lens) return torch.bmm(self.dropout(self.attention_weights), values)","title":"Scaled dot-product attention"},{"location":"DeepLearning/#attention-decoder","text":"\u4e0e\u4e4b\u524dSeq2Seq\u76f8\u6bd4\uff0c\u4e0a\u4e0b\u6587\u53d8\u91cf$\\mathbf{c}$\u5728\u4efb\u4f55\u89e3\u7801\u65f6\u95f4\u6b65$t'$\u90fd\u4f1a\u88ab$\\mathbf{c}_{t'}$\u66ff\u6362\u3002\u5047\u8bbe\u8f93\u5165\u5e8f\u5217\u4e2d\u6709$T$\u4e2a\u8bcd\u5143\uff0c\u89e3\u7801\u65f6\u95f4\u6b65$t'$\u7684\u4e0a\u4e0b\u6587\u53d8\u91cf\u662f\u6ce8\u610f\u529b\u96c6\u4e2d\u7684\u8f93\u51fa\uff1a $$\\mathbf{c} {t'} = \\sum {t=1}^T \\alpha(\\mathbf{s}_{t' - 1}, \\mathbf{h}_t) \\mathbf{h}_t$$ \u5176\u4e2d\uff0c\u65f6\u95f4\u6b65$t' - 1$\u65f6\u7684\u89e3\u7801\u5668\u9690\u72b6\u6001$\\mathbf{s}_{t' - 1}$\u662f\u67e5\u8be2\uff0c\u7f16\u7801\u5668\u9690\u72b6\u6001$\\mathbf{h}_t$\u65e2\u662f\u952e\uff0c\u4e5f\u662f\u503c\u3002 class Seq2SeqAttentionDecoder(AttentionDecoder): def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqAttentionDecoder, self).__init__(**kwargs) self.attention = d2l.AdditiveAttention( num_hiddens, num_hiddens, num_hiddens, dropout) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU( embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, enc_valid_lens, *args): # outputs\u7684\u5f62\u72b6\u4e3a(batch_size\uff0cnum_steps\uff0cnum_hiddens). # hidden_state\u7684\u5f62\u72b6\u4e3a(num_layers\uff0cbatch_size\uff0cnum_hiddens) outputs, hidden_state = enc_outputs return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens) def forward(self, X, state): # enc_outputs\u7684\u5f62\u72b6\u4e3a(batch_size,num_steps,num_hiddens). # hidden_state\u7684\u5f62\u72b6\u4e3a(num_layers,batch_size, # num_hiddens) enc_outputs, hidden_state, enc_valid_lens = state # \u8f93\u51faX\u7684\u5f62\u72b6\u4e3a(num_steps,batch_size,embed_size) X = self.embedding(X).permute(1, 0, 2) outputs, self._attention_weights = [], [] for x in X: # query\u7684\u5f62\u72b6\u4e3a(batch_size,1,num_hiddens) query = torch.unsqueeze(hidden_state[-1], dim=1) # context\u7684\u5f62\u72b6\u4e3a(batch_size,1,num_hiddens) context = self.attention( query, enc_outputs, enc_outputs, enc_valid_lens) # \u5728\u7279\u5f81\u7ef4\u5ea6\u4e0a\u8fde\u7ed3 x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1) # \u5c06x\u53d8\u5f62\u4e3a(1,batch_size,embed_size+num_hiddens) out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state) outputs.append(out) self._attention_weights.append(self.attention.attention_weights) # \u5168\u8fde\u63a5\u5c42\u53d8\u6362\u540e\uff0coutputs\u7684\u5f62\u72b6\u4e3a # (num_steps,batch_size,vocab_size) outputs = self.dense(torch.cat(outputs, dim=0)) return outputs.permute(1, 0, 2), [enc_outputs, hidden_state, enc_valid_lens] @property def attention_weights(self): return self._attention_weights","title":"Attention Decoder"},{"location":"DeepLearning/#mutihead-attention","text":"\u7ed9\u5b9a\u67e5\u8be2$\\mathbf{q} \\in \\mathbb{R}^{d_q}$\u3001\u952e$\\mathbf{k} \\in \\mathbb{R}^{d_k}$\u548c\u503c$\\mathbf{v} \\in \\mathbb{R}^{d_v}$\uff0c\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934$\\mathbf{h}_i$\uff08$i = 1, \\ldots, h$\uff09\u7684\u8ba1\u7b97\u65b9\u6cd5\u4e3a\uff1a $$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},$$ \u5176\u4e2d\uff0c\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u5305\u62ec$\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}$\u3001$\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}$\u548c $\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}$\uff0c\u4ee5\u53ca\u4ee3\u8868\u6ce8\u610f\u529b\u6c47\u805a\u7684\u51fd\u6570$f$\u3002 \u591a\u5934\u6ce8\u610f\u529b\u7684\u8f93\u51fa\u9700\u8981\u7ecf\u8fc7\u53e6\u4e00\u4e2a\u7ebf\u6027\u8f6c\u6362\uff0c\u5b83\u5bf9\u5e94\u7740$h$\u4e2a\u5934\u8fde\u7ed3\u540e\u7684\u7ed3\u679c\uff0c\u53ef\u5b66\u4e60\u53c2\u6570\u662f$\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$\uff1a $$\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\vdots\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.$$ \u5728\u5b9e\u73b0\u8fc7\u7a0b\u4e2d\u901a\u5e38\u9009\u62e9\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u4f5c\u4e3a\u6bcf\u4e00\u4e2a\u6ce8\u610f\u529b\u5934\u3002 \u8bbe\u5b9a$p_q = p_k = p_v = p_o / h$\u3002\u5982\u679c\u5c06\u67e5\u8be2\u3001\u952e\u548c\u503c\u7684\u7ebf\u6027\u53d8\u6362\u7684\u8f93\u51fa\u6570\u91cf\u8bbe\u7f6e\u4e3a$p_q h = p_k h = p_v h = p_o$\uff0c\u5219\u53ef\u4ee5\u5e76\u884c\u8ba1\u7b97$h$\u4e2a\u5934\u3002","title":"Mutihead-Attention"},{"location":"DeepLearning/#self-attention","text":"\u7ed9\u5b9a\u4e00\u4e2a\u7531\u8bcd\u5143\u7ec4\u6210\u7684\u8f93\u5165\u5e8f\u5217$\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$\uff0c\u5176\u4e2d\u4efb\u610f$\\mathbf{x}_i \\in \\mathbb{R}^d$\uff08$1 \\leq i \\leq n$\uff09\u3002\u8be5\u5e8f\u5217\u7684\u81ea\u6ce8\u610f\u529b\u8f93\u51fa\u4e3a\u4e00\u4e2a\u957f\u5ea6\u76f8\u540c\u7684\u5e8f\u5217 $\\mathbf{y}_1, \\ldots, \\mathbf{y}_n$\uff0c\u5176\u4e2d\uff1a $$\\mathbf{y}_i = f(\\mathbf{x}_i, (\\mathbf{x}_1, \\mathbf{x}_1), \\ldots, (\\mathbf{x}_n, \\mathbf{x}_n)) \\in \\mathbb{R}^d$$","title":"Self-Attention"},{"location":"DeepLearning/#positional-encoding","text":"\u7531\u4e8eSelf Attention \u6ca1\u6709\u8bb0\u5f55\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5047\u8bbe\u957f\u5ea6\u4e3an\u7684\u5e8f\u5217$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$,\u7528\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635$\\mathbf{P} \\in \\mathbb{R}^{n \\times d}$ \u6765\u8ba1\u7b97$\\mathbf{X} + \\mathbf{P}$ \u4f5c\u4e3aSelf Attention \u7684\u8f93\u5165\u3002 \u77e9\u9635$\\mathbf{P}$\u7b2c$i$\u884c\u3001\u7b2c$2j$\u5217\u548c$2j+1$\u5217\u4e0a\u7684\u5143\u7d20\u4e3a\uff1a $$\\begin{aligned} p_{i, 2j} &= \\sin\\left(\\frac{i}{10000^{2j/d}}\\right),\\p_{i, 2j+1} &= \\cos\\left(\\frac{i}{10000^{2j/d}}\\right).\\end{aligned}$$ \u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f $(p_{i, 2j}, p_{i, 2j+1})$\u90fd\u53ef\u4ee5\u7ebf\u6027\u6295\u5f71\u5230$(p_{i+\\delta, 2j}, p_{i+\\delta, 2j+1})$\uff1a $$\\begin{aligned} &\\begin{bmatrix} \\cos(\\delta \\omega_j) & \\sin(\\delta \\omega_j) \\ -\\sin(\\delta \\omega_j) & \\cos(\\delta \\omega_j) \\ \\end{bmatrix} \\begin{bmatrix} p_{i, 2j} \\ p_{i, 2j+1} \\ \\end{bmatrix}\\ =&\\begin{bmatrix} \\cos(\\delta \\omega_j) \\sin(i \\omega_j) + \\sin(\\delta \\omega_j) \\cos(i \\omega_j) \\ -\\sin(\\delta \\omega_j) \\sin(i \\omega_j) + \\cos(\\delta \\omega_j) \\cos(i \\omega_j) \\ \\end{bmatrix}\\ =&\\begin{bmatrix} \\sin\\left((i+\\delta) \\omega_j\\right) \\ \\cos\\left((i+\\delta) \\omega_j\\right) \\ \\end{bmatrix}\\ =& \\begin{bmatrix} p_{i+\\delta, 2j} \\ p_{i+\\delta, 2j+1} \\ \\end{bmatrix}, \\end{aligned}$$ \u56e0\u6b64\u8fd9\u4e2a\u77e9\u9635\u4e0d\u4f9d\u8d56\u4e8e\u4efb\u4f55\u4f4d\u7f6e\u7684\u7d22\u5f15\u3002","title":"Positional-Encoding"},{"location":"DeepLearning/#transformer","text":"","title":"Transformer"},{"location":"DeepLearning/#addnorm","text":"\u4f7f\u7528LayerNorm \uff08\u4e0d\u6539\u53d8\u8bed\u4e49\u5411\u91cf\u7684\u65b9\u5411\uff0c\u6539\u53d8\u6a21\u957f\uff09 class AddNorm(nn.Module): def __init__(self, normalized_shape, dropout, **kwargs): super(AddNorm, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) self.ln = nn.LayerNorm(normalized_shape) def forward(self, X, Y): return self.ln(self.dropout(Y) + X)","title":"Add&amp;Norm"},{"location":"DeepLearning/#encoder","text":"class EncoderBlock(nn.Module): def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs): super(EncoderBlock, self).__init__(**kwargs) self.attention = d2l.MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias) self.addnorm1 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN( ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm2 = AddNorm(norm_shape, dropout) def forward(self, X, valid_lens): Y = self.addnorm1(X, self.attention(X, X, X, valid_lens)) return self.addnorm2(Y, self.ffn(Y)) Encoder\u5c42\u90fd\u4e0d\u4f1a\u6539\u53d8\u8f93\u5165\u7684\u5f62\u72b6\u3002 class TransformerEncoder(d2l.Encoder): def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs): super(TransformerEncoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(\"block\"+str(i), EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias)) def forward(self, X, valid_lens, *args): # \u56e0\u4e3a\u4f4d\u7f6e\u7f16\u7801\u503c\u5728-1\u548c1\u4e4b\u95f4\uff0c # \u56e0\u6b64\u5d4c\u5165\u503c\u4e58\u4ee5\u5d4c\u5165\u7ef4\u5ea6\u7684\u5e73\u65b9\u6839\u8fdb\u884c\u7f29\u653e\uff0c # \u7136\u540e\u518d\u4e0e\u4f4d\u7f6e\u7f16\u7801\u76f8\u52a0\u3002 X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self.attention_weights = [None] * len(self.blks) for i, blk in enumerate(self.blks): X = blk(X, valid_lens) self.attention_weights[ i] = blk.attention.attention.attention_weights return X","title":"Encoder"},{"location":"DeepLearning/#decoder","text":"\u5728\u63a9\u853d\u591a\u5934\u89e3\u7801\u5668\u81ea\u6ce8\u610f\u529b\u5c42\uff08\u7b2c\u4e00\u4e2a\u5b50\u5c42\uff09\u4e2d\uff0c\u67e5\u8be2\u3001\u952e\u548c\u503c\u90fd\u6765\u81ea\u4e0a\u4e00\u4e2a\u89e3\u7801\u5668\u5c42\u7684\u8f93\u51fa\u3002\u4e3a\u4e86\u5728\u89e3\u7801\u5668\u4e2d\u4fdd\u7559\u81ea\u56de\u5f52\u7684\u5c5e\u6027\uff0c\u5176\u63a9\u853d\u81ea\u6ce8\u610f\u529b\u8bbe\u5b9a\u4e86\u53c2\u6570\uff0c\u4ee5\u4fbf\u4efb\u4f55\u67e5\u8be2\u90fd\u53ea\u4f1a\u4e0e\u89e3\u7801\u5668\u4e2d\u6240\u6709\u5df2\u7ecf\u751f\u6210\u8bcd\u5143\u7684\u4f4d\u7f6e\uff08\u5373\u76f4\u5230\u8be5\u67e5\u8be2\u4f4d\u7f6e\u4e3a\u6b62\uff09\u8fdb\u884c\u6ce8\u610f\u529b\u8ba1\u7b97\u3002 class DecoderBlock(nn.Module): def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs): super(DecoderBlock, self).__init__(**kwargs) self.i = i self.attention1 = d2l.MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm1 = AddNorm(norm_shape, dropout) self.attention2 = d2l.MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm2 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm3 = AddNorm(norm_shape, dropout) def forward(self, X, state): enc_outputs, enc_valid_lens = state[0], state[1] # \u8bad\u7ec3\u9636\u6bb5\uff0c\u8f93\u51fa\u5e8f\u5217\u7684\u6240\u6709\u8bcd\u5143\u90fd\u5728\u540c\u4e00\u65f6\u95f4\u5904\u7406\uff0c # \u56e0\u6b64state[2][self.i]\u521d\u59cb\u5316\u4e3aNone\u3002 # \u9884\u6d4b\u9636\u6bb5\uff0c\u8f93\u51fa\u5e8f\u5217\u662f\u901a\u8fc7\u8bcd\u5143\u4e00\u4e2a\u63a5\u7740\u4e00\u4e2a\u89e3\u7801\u7684\uff0c # \u56e0\u6b64state[2][self.i]\u5305\u542b\u7740\u76f4\u5230\u5f53\u524d\u65f6\u95f4\u6b65\u7b2ci\u4e2a\u5757\u89e3\u7801\u7684\u8f93\u51fa\u8868\u793a if state[2][self.i] is None: key_values = X else: key_values = torch.cat((state[2][self.i], X), axis=1) state[2][self.i] = key_values if self.training: batch_size, num_steps, _ = X.shape # dec_valid_lens\u7684\u5f00\u5934:(batch_size,num_steps), # \u5176\u4e2d\u6bcf\u4e00\u884c\u662f[1,2,...,num_steps] dec_valid_lens = torch.arange( 1, num_steps + 1, device=X.device).repeat(batch_size, 1) else: dec_valid_lens = None # \u81ea\u6ce8\u610f\u529b X2 = self.attention1(X, key_values, key_values, dec_valid_lens) Y = self.addnorm1(X, X2) # \u7f16\u7801\u5668\uff0d\u89e3\u7801\u5668\u6ce8\u610f\u529b\u3002 # enc_outputs\u7684\u5f00\u5934:(batch_size,num_steps,num_hiddens) Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens) Z = self.addnorm2(Y, Y2) return self.addnorm3(Z, self.ffn(Z)), state class TransformerDecoder(d2l.AttentionDecoder): def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs): super(TransformerDecoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.num_layers = num_layers self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(\"block\"+str(i), DecoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i)) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, enc_valid_lens, *args): return [enc_outputs, enc_valid_lens, [None] * self.num_layers] def forward(self, X, state): X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self._attention_weights = [[None] * len(self.blks) for _ in range (2)] for i, blk in enumerate(self.blks): X, state = blk(X, state) # \u89e3\u7801\u5668\u81ea\u6ce8\u610f\u529b\u6743\u91cd self._attention_weights[0][ i] = blk.attention1.attention.attention_weights # \u201c\u7f16\u7801\u5668\uff0d\u89e3\u7801\u5668\u201d\u81ea\u6ce8\u610f\u529b\u6743\u91cd self._attention_weights[1][ i] = blk.attention2.attention.attention_weights return self.dense(X), state @property def attention_weights(self): return self._attention_weights","title":"Decoder"},{"location":"DeepLearning/#multiple-gpus","text":"","title":"Multiple GPUs"},{"location":"DeepLearning/#_17","text":"\u8fd9\u79cd\u65b9\u5f0f\u4e0b\uff0c\u6240\u6709GPU\u5c3d\u7ba1\u6709\u4e0d\u540c\u7684\u89c2\u6d4b\u7ed3\u679c\uff0c\u4f46\u662f\u6267\u884c\u7740\u76f8\u540c\u7c7b\u578b\u7684\u5de5\u4f5c\u3002\u5728\u5b8c\u6210\u6bcf\u4e2a\u5c0f\u6279\u91cf\u6570\u636e\u7684\u8bad\u7ec3\u4e4b\u540e\uff0c\u68af\u5ea6\u5728GPU\u4e0a\u805a\u5408\u3002 GPU\u7684\u6570\u91cf\u8d8a\u591a\uff0c\u5c0f\u6279\u91cf\u5305\u542b\u7684\u6570\u636e\u91cf\u5c31\u8d8a\u5927\uff0c\u4ece\u800c\u5c31\u80fd\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002 \u7f3a\u70b9\uff1a\u4e0d\u80fd\u591f\u8bad\u7ec3\u66f4\u5927\u7684\u6a21\u578b $k$\u4e2aGPU\u5e76\u884c\u8bad\u7ec3\u8fc7\u7a0b\u5982\u4e0b\uff1a \u5728\u4efb\u4f55\u4e00\u6b21\u8bad\u7ec3\u8fed\u4ee3\u4e2d\uff0c\u7ed9\u5b9a\u7684\u968f\u673a\u7684\u5c0f\u6279\u91cf\u6837\u672c\u90fd\u5c06\u88ab\u5206\u6210$k$\u4e2a\u90e8\u5206\uff0c\u5e76\u5747\u5300\u5730\u5206\u914d\u5230GPU\u4e0a\uff1b \u6bcf\u4e2aGPU\u6839\u636e\u5206\u914d\u7ed9\u5b83\u7684\u5c0f\u6279\u91cf\u5b50\u96c6\uff0c\u8ba1\u7b97\u6a21\u578b\u53c2\u6570\u7684\u635f\u5931\u548c\u68af\u5ea6\uff1b \u5c06$k$\u4e2aGPU\u4e2d\u7684\u5c40\u90e8\u68af\u5ea6\u805a\u5408\uff0c\u4ee5\u83b7\u5f97\u5f53\u524d\u5c0f\u6279\u91cf\u7684\u968f\u673a\u68af\u5ea6\uff1b \u805a\u5408\u68af\u5ea6\u88ab\u91cd\u65b0\u5206\u53d1\u5230\u6bcf\u4e2aGPU\u4e2d\uff1b *\u6bcf\u4e2aGPU\u4f7f\u7528\u8fd9\u4e2a\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\uff0c\u6765\u66f4\u65b0\u5b83\u6240\u7ef4\u62a4\u7684\u5b8c\u6574\u7684\u6a21\u578b\u53c2\u6570\u96c6\u3002 \u5411\u591a\u4e2a\u8bbe\u5907\u590d\u5236\u53c2\u6570 def get_params(params, device): new_params = [p.to(device) for p in params] for p in new_params: p.requires_grad_() return new_params \u80fd\u591f\u5c06\u6240\u6709\u8bbe\u5907\u4e0a\u7684\u68af\u5ea6\u8fdb\u884c\u76f8\u52a0 def allreduce(data): for i in range(1, len(data)): data[0][:] += data[i].to(data[0].device) for i in range(1, len(data)): data[i][:] = data[0].to(data[i].device) \u5206\u53d1\u6570\u636e nn.parallel.scatter(data, devices) data = torch.arange(20).reshape(4, 5) devices = [torch.device('cuda:0'), torch.device('cuda:1')] split = nn.parallel.scatter(data, devices) print('input :', data) print('load into', devices) print('output:', split) input : tensor([ [ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19] ]) load into [device(type='cuda', index=0), device(type='cuda', index=1)] output: (tensor([ [0, 1, 2, 3, 4], [5, 6, 7, 8, 9] ], device='cuda:0'), tensor([ [10, 11, 12, 13, 14], [15, 16, 17, 18, 19] ], device='cuda:1')) \u5206\u53d1\u6570\u636e\u548c\u6807\u7b7e def split_batch(X, y, devices): assert X.shape[0] == y.shape[0] return (nn.parallel.scatter(X, devices), nn.parallel.scatter(y, devices)) \u5b9e\u73b0\u591aGPU\u8bad\u7ec3 def train_batch(X, y, device_params, devices, lr): X_shards, y_shards = split_batch(X, y, devices) # \u5728\u6bcf\u4e2aGPU\u4e0a\u5206\u522b\u8ba1\u7b97\u635f\u5931 ls = [loss(lenet(X_shard, device_W), y_shard).sum() for X_shard, y_shard, device_W in zip( X_shards, y_shards, device_params)] for l in ls: # \u53cd\u5411\u4f20\u64ad\u5728\u6bcf\u4e2aGPU\u4e0a\u5206\u522b\u6267\u884c l.backward() # \u5c06\u6bcf\u4e2aGPU\u7684\u6240\u6709\u68af\u5ea6\u76f8\u52a0\uff0c\u5e76\u5c06\u5176\u5e7f\u64ad\u5230\u6240\u6709GPU with torch.no_grad(): for i in range(len(device_params[0])): allreduce([device_params[c][i].grad for c in range(len(devices))]) # \u5728\u6bcf\u4e2aGPU\u4e0a\u5206\u522b\u66f4\u65b0\u6a21\u578b\u53c2\u6570 for param in device_params: d2l.sgd(param, lr, X.shape[0]) # \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4f7f\u7528\u5168\u5c3a\u5bf8\u7684\u5c0f\u6279\u91cf \u8bc4\u4f30\u6a21\u578b\u7684\u65f6\u5019\u53ea\u5728\u4e00\u4e2aGPU\u4e0a def train(num_gpus, batch_size, lr): train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) devices = [d2l.try_gpu(i) for i in range(num_gpus)] # \u5c06\u6a21\u578b\u53c2\u6570\u590d\u5236\u5230num_gpus\u4e2aGPU device_params = [get_params(params, d) for d in devices] num_epochs = 10 animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs]) timer = d2l.Timer() for epoch in range(num_epochs): timer.start() for X, y in train_iter: # \u4e3a\u5355\u4e2a\u5c0f\u6279\u91cf\u6267\u884c\u591aGPU\u8bad\u7ec3 train_batch(X, y, device_params, devices, lr) torch.cuda.synchronize() timer.stop() # \u5728GPU0\u4e0a\u8bc4\u4f30\u6a21\u578b animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu( lambda x: lenet(x, device_params[0]), test_iter, devices[0]),)) print(f'\u6d4b\u8bd5\u7cbe\u5ea6\uff1a{animator.Y[0][-1]:.2f}\uff0c{timer.avg():.1f}\u79d2/\u8f6e\uff0c' f'\u5728{str(devices)}') \u7b80\u6d01\u5b9e\u73b0 net = nn.DataParallel(net, device_ids=devices) \uff0c\u548c\u4e4b\u524d\u51e0\u4e4e\u6ca1\u4ec0\u4e48\u533a\u522b def train(net, num_gpus, batch_size, lr): train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) devices = [d2l.try_gpu(i) for i in range(num_gpus)] def init_weights(m): if type(m) in [nn.Linear, nn.Conv2d]: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights) # \u5728\u591a\u4e2aGPU\u4e0a\u8bbe\u7f6e\u6a21\u578b net = nn.DataParallel(net, device_ids=devices) trainer = torch.optim.SGD(net.parameters(), lr) loss = nn.CrossEntropyLoss() timer, num_epochs = d2l.Timer(), 10 animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs]) for epoch in range(num_epochs): net.train() timer.start() for X, y in train_iter: trainer.zero_grad() X, y = X.to(devices[0]), y.to(devices[0]) l = loss(net(X), y) l.backward() trainer.step() timer.stop() animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),)) print(f'\u6d4b\u8bd5\u7cbe\u5ea6\uff1a{animator.Y[0][-1]:.2f}\uff0c{timer.avg():.1f}\u79d2/\u8f6e\uff0c' f'\u5728{str(devices)}')","title":"\u62c6\u5206\u6570\u636e"},{"location":"DeepLearning/#computer-vision","text":"","title":"Computer Vision"},{"location":"DeepLearning/#image-augmentation","text":"\u7ffb\u8f6c\uff08\u4e0a\u4e0b\u7ffb\u8f6c\u3001\u5de6\u53f3\u7ffb\u8f6c\uff09 torchvision.transforms.RandomHorizontalFlip() torchvision.transforms.RandomVerticalFlip() \u968f\u673a\u526a\u88c1 torchvision.transforms.RandomResizedCrop((200, 200), scale=(0.1, 1), ratio=(0.5, 2)) \u6539\u53d8\u989c\u8272\uff08\u4eae\u5ea6\u3001\u5bf9\u6bd4\u5ea6\u3001\u9971\u548c\u5ea6\u3001\u8272\u8c03\uff09 torchvision.transforms.ColorJitter(brightness=0.5, contrast=0, saturation=0, hue=0)","title":"Image Augmentation"},{"location":"DeepLearning/#_18","text":"","title":"\u76ee\u6807\u68c0\u6d4b"},{"location":"DeepLearning/#bounding-box","text":"\u4e24\u79cd\u8868\u793a - \u5de6\u4e0a\u89d2\u5750\u6807\u548c\u53f3\u4e0b\u89d2\u5750\u6807 - \u4e2d\u5fc3\u5750\u6807\u548cw\uff0ch","title":"Bounding Box"},{"location":"DeepLearning/#_19","text":"","title":"\u951a\u6846"},{"location":"DeepLearning/#_20","text":"\u8f93\u5165\u56fe\u50cf\u7684\u9ad8\u5ea6\u4e3a$h$\uff0c\u5bbd\u5ea6\u4e3a$w$\u3002\u4ee5\u56fe\u50cf\u7684\u6bcf\u4e2a\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u751f\u6210\u4e0d\u540c\u5f62\u72b6\u7684\u951a\u6846\uff1a \u7f29\u653e\u6bd4 \u4e3a$s\\in (0, 1]$\uff0c \u5bbd\u9ad8\u6bd4 \u4e3a$r > 0$\u3002 \u90a3\u4e48 \u951a\u6846\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u5206\u522b\u662f$hs\\sqrt{r}$\u548c$hs/\\sqrt{r}$\u3002 \u8bf7\u6ce8\u610f\uff0c\u5f53\u4e2d\u5fc3\u4f4d\u7f6e\u7ed9\u5b9a\u65f6\uff0c\u5df2\u77e5\u5bbd\u548c\u9ad8\u7684\u951a\u6846\u662f\u786e\u5b9a\u7684\u3002 \u7f29\u653e\u6bd4\uff08scale\uff09\u53d6\u503c$s_1,\\ldots, s_n$\u548c\u5bbd\u9ad8\u6bd4\uff08aspect ratio\uff09\u53d6\u503c$r_1,\\ldots, r_m$\u3002\u4f7f\u7528\u8fd9\u4e9b\u6bd4\u4f8b\u548c\u957f\u5bbd\u6bd4\u7684\u6240\u6709\u7ec4\u5408\u4ee5\u6bcf\u4e2a\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u65f6\uff0c\u8f93\u5165\u56fe\u50cf\u5c06\u603b\u5171\u6709$whnm$\u4e2a\u951a\u6846\u3002 \u5728\u5b9e\u8df5\u4e2d\uff0c\u8003\u8651\u5305\u542b$s_1$\u6216$r_1$\u7684\u7ec4\u5408\uff1a $$(s_1, r_1), (s_1, r_2), \\ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \\ldots, (s_n, r_1).$$ \u4e5f\u5c31\u662f\u8bf4\uff0c\u4ee5\u540c\u4e00\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u7684\u951a\u6846\u7684\u6570\u91cf\u662f$n+m-1$\u3002\u5bf9\u4e8e\u6574\u4e2a\u8f93\u5165\u56fe\u50cf\uff0c\u5c06\u5171\u751f\u6210$wh(n+m-1)$\u4e2a\u951a\u6846\u3002","title":"\u751f\u6210\u951a\u6846"},{"location":"DeepLearning/#iou","text":"$$J(\\mathcal{A},\\mathcal{B}) = \\frac{\\left|\\mathcal{A} \\cap \\mathcal{B}\\right|}{\\left| \\mathcal{A} \\cup \\mathcal{B}\\right|}.$$","title":"\u4ea4\u5e76\u6bd4(IoU)"},{"location":"DeepLearning/#_21","text":"\u6bcf\u6b21\u53d6\u6700\u5927IoU\u7684\u951a\u6846\u548c\u771f\u5b9e\u6846\uff0c\u53bb\u6389\u4e4b\u540e\u91cd\u590d\uff0c\u6700\u540e\u6839\u636e\u9608\u503c\u786e\u5b9a\u662f\u5426\u4e3a\u951a\u6846\u5206\u914d\u771f\u5b9e\u6846","title":"\u951a\u6846\u5206\u914d"},{"location":"DeepLearning/#_22","text":"\u7ed9\u5b9a\u6846$A$\u548c$B$\uff0c\u4e2d\u5fc3\u5750\u6807\u5206\u522b\u4e3a$(x_a, y_a)$\u548c$(x_b, y_b)$\uff0c\u5bbd\u5ea6\u5206\u522b\u4e3a$w_a$\u548c$w_b$\uff0c\u9ad8\u5ea6\u5206\u522b\u4e3a$h_a$\u548c$h_b$\uff0c\u53ef\u4ee5\u5c06$A$\u7684\u504f\u79fb\u91cf\u6807\u8bb0\u4e3a\uff1a $$\\left( \\frac{ \\frac{x_b - x_a}{w_a} - \\mu_x }{\\sigma_x}, \\frac{ \\frac{y_b - y_a}{h_a} - \\mu_y }{\\sigma_y}, \\frac{ \\log \\frac{w_b}{w_a} - \\mu_w }{\\sigma_w}, \\frac{ \\log \\frac{h_b}{h_a} - \\mu_h }{\\sigma_h}\\right),$$","title":"\u6807\u8bb0\u7c7b\u522b"},{"location":"DeepLearning/#_23","text":"\u5728\u540c\u4e00\u5f20\u56fe\u50cf\u4e2d\uff0c\u6240\u6709\u9884\u6d4b\u7684\u975e\u80cc\u666f\u8fb9\u754c\u6846\u90fd\u6309\u7f6e\u4fe1\u5ea6\u964d\u5e8f\u6392\u5e8f\uff0c\u4ee5\u751f\u6210\u5217\u8868$L$\u3002 \u4ece$L$\u4e2d\u9009\u53d6\u7f6e\u4fe1\u5ea6\u6700\u9ad8\u7684\u9884\u6d4b\u8fb9\u754c\u6846$B_1$\u4f5c\u4e3a\u57fa\u51c6\uff0c\u7136\u540e\u5c06\u6240\u6709\u4e0e$B_1$\u7684IoU\u8d85\u8fc7\u9884\u5b9a\u9608\u503c$\\epsilon$\u7684\u975e\u57fa\u51c6\u9884\u6d4b\u8fb9\u754c\u6846\u4ece$L$\u4e2d\u79fb\u9664\u3002 \u4ece$L$\u4e2d\u9009\u53d6\u7f6e\u4fe1\u5ea6\u7b2c\u4e8c\u9ad8\u7684\u9884\u6d4b\u8fb9\u754c\u6846$B_2$\u4f5c\u4e3a\u53c8\u4e00\u4e2a\u57fa\u51c6\uff0c\u7136\u540e\u5c06\u6240\u6709\u4e0e$B_2$\u7684IoU\u5927\u4e8e$\\epsilon$\u7684\u975e\u57fa\u51c6\u9884\u6d4b\u8fb9\u754c\u6846\u4ece$L$\u4e2d\u79fb\u9664\u3002 \u91cd\u590d\u4e0a\u8ff0\u8fc7\u7a0b\uff0c\u76f4\u5230$L$\u4e2d\u7684\u6240\u6709\u9884\u6d4b\u8fb9\u754c\u6846\u90fd\u66fe\u88ab\u7528\u4f5c\u57fa\u51c6\u3002\u6b64\u65f6\uff0c$L$\u4e2d\u4efb\u610f\u4e00\u5bf9\u9884\u6d4b\u8fb9\u754c\u6846\u7684IoU\u90fd\u5c0f\u4e8e\u9608\u503c$\\epsilon$\uff1b\u56e0\u6b64\uff0c\u6ca1\u6709\u4e00\u5bf9\u8fb9\u754c\u6846\u8fc7\u4e8e\u76f8\u4f3c\u3002 \u8f93\u51fa\u5217\u8868$L$\u4e2d\u7684\u6240\u6709\u9884\u6d4b\u8fb9\u754c\u6846\u3002","title":"\u975e\u6781\u5927\u503c\u6291\u5236"},{"location":"DeepLearning/#r-cnn","text":"","title":"R-CNN"},{"location":"DeepLearning/#region-based-cnn","text":"\u4f7f\u7528\u542f\u53d1\u5f0f\u641c\u7d22\u9009\u62e9\u951a\u6846 \u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u6bcf\u4e2a\u951a\u6846\u62bd\u53d6\u7279\u5f81 \u8bad\u7ec3\u4e00\u4e2aSVM\u6765\u5bf9\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b \u8bad\u7ec3\u4e00\u4e2a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u6765\u9884\u6d4b\u8fb9\u7f18\u6846\u504f\u79fb Rol Pooling \u7ed9\u5b9a\u4e00\u4e2a\u951a\u6846\uff0c\u5747\u5300\u5206\u6210$n \\times m$\u5757\uff0c\u8f93\u51fa\u6bcf\u5757\u91cc\u9762\u7684\u6700\u5927\u503c\uff0c\u4e0d\u7ba1\u951a\u6846\u5927\u5c0f\u4e3a\u591a\u5c11\uff0c\u90fd\u662fnm","title":"Region-based CNN"},{"location":"DeepLearning/#fast-rcnn","text":"\u4f7f\u7528CNN\u5bf9\u56fe\u7247\u62bd\u53d6\u7279\u5f81\uff08\u6574\u5f20\u56fe\u7247\uff09\uff0c\u4f7f\u7528Rol Pooling\u5c42\u5bf9\u6bcf\u4e2a\u951a\u6846\u751f\u6210\u56fa\u5b9a\u957f\u5ea6\u7684\u7279\u5f81","title":"Fast RCNN"},{"location":"DeepLearning/#faster-rcnn","text":"\u4f7f\u7528\u4e00\u4e2a\u7f51\u7edc\u6765\u66ff\u4ee3\u542f\u53d1\u5f0f\u641c\u7d22\u6765\u83b7\u5f97\u66f4\u597d\u7684\u951a\u6846","title":"Faster RCNN"},{"location":"DeepLearning/#mask-rcnn","text":"\u5982\u679c\u6709\u50cf\u7d20\u7ea7\u522b\u7684\u6807\u53f7\uff0c\u7528FCN\u6765\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f","title":"Mask RCNN"},{"location":"DeepLearning/#yolo","text":"\u53ea\u770b\u4e00\u6b21 - SSD\u951a\u6846\u5927\u91cf\u91cd\u53e0\u6d6a\u8d39\u8ba1\u7b97 - YOLO\u5c06\u56fe\u7247\u5747\u5300\u5206\u6210$S \\times S$ \u951a\u6846 - \u6bcf\u4e2a\u951a\u6846\u9884\u6d4b$B$\u4e2a\u8fb9\u7f18\u6846","title":"YOLO"},{"location":"DeepLearning/#ssd","text":"","title":"\u5355\u53d1\u591a\u6846\u68c0\u6d4b\uff08SSD\uff09"},{"location":"DeepLearning/#_24","text":"\u56fe\u50cf\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272","title":"\u8bed\u4e49\u5206\u5272"},{"location":"DeepLearning/#_25","text":"\u8f6c\u7f6e\u5377\u79ef\u53ef\u4ee5\u7528\u6765\u589e\u52a0\u9ad8\u5bbd $Y[i: i + h, j: j + w] += X[i, j] * K$ \u8f6c\u7f6e? $Y = X * W$ \u53ef\u4ee5\u5bf9W\u6784\u9020\u4e00\u4e2aV\uff0c\u4f7f\u5f97\u5377\u79ef\u7b49\u4ef7\u4e8e\u77e9\u9635\u4e58\u6cd5$Y^ \\prime = V X^ \\prime$, \u8fd9\u91cc$X^ \\prime Y^\\prime$\u662f $XY$\u5bf9\u5e94\u7684\u5411\u91cf\u7248\u672c \u8f6c\u7f6e\u5377\u79ef\u7b49\u4ef7\u4e8e $Y^\\prime = V^T X^\\prime $ \u57fa\u672c\u64cd\u4f5c def trans_conv(X, K): h, w = K.shape Y = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1)) for i in range(X.shape[0]): for j in range(X.shape[1]): Y[i: i + h, j: j + w] += X[i, j] * K return Y torch API X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2) tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False) tensor([ [ [ [ 0., 0., 1.], [ 0., 4., 6.], [ 4., 12., 9.] ] ] ]) tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False) padding\u5c06\u5220\u9664\u7b2c\u4e00\u548c\u6700\u540e\u4e00\u884c\u548c\u5217 tensor([ [ [ [4.] ] ] ]) tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False) \u6b65\u5e45\u4e3a2\u589e\u5927\u8f93\u51fa tensor([ [ [ [0., 0., 0., 1.], [0., 0., 2., 3.], [0., 2., 0., 3.], [4., 6., 6., 9.] ] ] ])","title":"\u8f6c\u7f6e\u5377\u79ef"},{"location":"DeepLearning/#fcn","text":"\u7528\u8f6c\u7f6e\u5377\u79ef\u5c42\u6765\u66ff\u6362CNN\u6700\u540e\u7684\u5168\u8fde\u63a5\u5c42\uff0c\u4ece\u800c\u5b9e\u73b0\u6bcf\u4e2a\u50cf\u7d20\u7684\u9884\u6d4b CNN --> 1x1 Conv --> \u8f6c\u7f6e\u5377\u79ef --> output \u5148\u7528Resnet18\u63d0\u53d6\u7279\u5f81 net = nn.Sequential(*list(pretrained_net.children())[:-2]) \u7136\u540e\u52a01x1\u7684\u5377\u79ef\u5c42\u548c\u8f6c\u7f6e\u5377\u79ef\u5c42\uff0c\u4f7f\u5f97\u8f93\u51fa\u5927\u5c0f\u548c\u539f\u56fe\u50cf\u5927\u5c0f\u76f8\u540c num_classes = 21 net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1)) net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, padding=16, stride=32))","title":"\u5168\u5377\u79ef\u7f51\u7edc(FCN)"},{"location":"DeepLearning_2/","text":"BERT \u8f93\u5165\u8868\u793a \u628a\u4e24\u4e2a\u53e5\u5b50\u53d8\u6210BERT\u7684\u8f93\u5165 def get_tokens_and_segments(tokens_a, tokens_b=None): \"\"\"\u83b7\u53d6\u8f93\u5165\u5e8f\u5217\u7684\u8bcd\u5143\u53ca\u5176\u7247\u6bb5\u7d22\u5f15\"\"\" tokens = ['<cls>'] + tokens_a + ['<sep>'] # 0\u548c1\u5206\u522b\u6807\u8bb0\u7247\u6bb5A\u548cB segments = [0] * (len(tokens_a) + 2) if tokens_b is not None: tokens += tokens_b + ['<sep>'] segments += [1] * (len(tokens_b) + 1) return tokens, segments Vision Transformer \u8bba\u6587 \u4ee3\u7801\u6765\u6e90 \u4e3b\u8981\u5c31\u662f\u56fe\u7247\u8f6c\u6362tokens\uff0cposition embedding\u8fd9\u91cc\u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570 Example \u8f93\u5165224x224x3 -> Embedding(16x16\u7684\u5377\u79ef\u6838\uff0c\u6b65\u8ddd\u4e3a16\u7684\u5377\u79ef\u5c42) 14x14x768 -> Flatten 196x768 -> Concat\u4e00\u4e2aClass token 197x768 -> \u52a0\u4e0aposition embedding -> Dropout -> \u91cd\u590dL\u6b21 Transformer Encoder 197x768-> LayerNorm -> Extract class token 1x768(\u4e4b\u524dconcat\u7684class token)-> MLP Head class PatchEmbed(nn.Module): \"\"\" 2D Image to Patch Embedding \"\"\" def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None): super().__init__() img_size = (img_size, img_size) patch_size = (patch_size, patch_size) self.img_size = img_size self.patch_size = patch_size # \u6bcf\u4e2apatch\u7684\u5927\u5c0f self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) # 224/16 -> 14*14 self.num_patches = self.grid_size[0] * self.grid_size[1] # patches\u7684\u6570\u76ee self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size) # \u5377\u79ef\u6838\u5927\u5c0f\u548cpatch_size\u90fd\u662f16*16 self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() # \u5982\u679c\u6ca1\u6709\u4f20\u5165norm\u5c42\uff0c\u5c31\u4f7f\u7528identity def forward(self, x): B, C, H, W = x.shape # \u6ce8\u610f\uff0c\u5728vit\u6a21\u578b\u4e2d\u8f93\u5165\u5927\u5c0f\u5fc5\u987b\u662f\u56fa\u5b9a\u7684\uff0c\u9ad8\u5bbd\u548c\u8bbe\u5b9a\u503c\u76f8\u540c assert H == self.img_size[0] and W == self.img_size[1], \\ f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\" # flatten: [B, C, H, W] -> [B, C, HW] # transpose: [B, C, HW] -> [B, HW, C] x = self.proj(x).flatten(2).transpose(1, 2) x = self.norm(x) return x class Attention(nn.Module): # Multi-head selfAttention \u6a21\u5757 def __init__(self, dim, # \u8f93\u5165token\u7684dim num_heads=8, # head\u7684\u4e2a\u6570 qkv_bias=False, # \u751f\u6210qkv\u65f6\u662f\u5426\u4f7f\u7528\u504f\u7f6e qk_scale=None, attn_drop_ratio=0., # \u4e24\u4e2adropout ratio proj_drop_ratio=0.): super(Attention, self).__init__() self.num_heads = num_heads head_dim = dim // num_heads # \u6bcf\u4e2ahead\u7684dim self.scale = qk_scale or head_dim ** -0.5 # \u4e0d\u53bb\u4f20\u5165qkscale\uff0c\u4e5f\u5c31\u662f1/\u221adim_k self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) # \u4f7f\u7528\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\uff0c\u4e00\u6b21\u5f97\u5230qkv self.attn_drop = nn.Dropout(attn_drop_ratio) self.proj = nn.Linear(dim, dim) # \u628a\u591a\u4e2ahead\u8fdb\u884cConcat\u64cd\u4f5c\uff0c\u7136\u540e\u901a\u8fc7Wo\u6620\u5c04\uff0c\u8fd9\u91cc\u7528\u5168\u8fde\u63a5\u5c42\u4ee3\u66ff self.proj_drop = nn.Dropout(proj_drop_ratio) def forward(self, x): # [batch_size, num_patches + 1, total_embed_dim] \u52a01\u4ee3\u8868\u7c7b\u522b\uff0c\u9488\u5bf9ViT-B/16\uff0cdim\u662f768 B, N, C = x.shape # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim] # reshape: -> [batch_size, num_patches + 1, 3\uff08\u4ee3\u8868qkv\uff09, num_heads\uff08\u4ee3\u8868head\u6570\uff09, embed_dim_per_head\uff08\u6bcf\u4e2ahead\u7684qkv\u7ef4\u5ea6\uff09] # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head] qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # [batch_size, num_heads, num_patches + 1, embed_dim_per_head] q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1] # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1] attn = (q @ k.transpose(-2, -1)) * self.scale # \u6bcf\u4e2aheader\u7684q\u548ck\u76f8\u4e58\uff0c\u9664\u4ee5\u221adim_k\uff08\u76f8\u5f53\u4e8enorm\u5904\u7406\uff09 attn = attn.softmax(dim=-1) # \u901a\u8fc7softmax\u5904\u7406\uff08\u76f8\u5f53\u4e8e\u5bf9\u6bcf\u4e00\u884c\u7684\u6570\u636esoftmax\uff09 attn = self.attn_drop(attn) # dropOut\u5c42 # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head] # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head] # reshape: -> [batch_size, num_patches + 1, total_embed_dim] x = (attn @ v).transpose(1, 2).reshape(B, N, C) # \u5f97\u5230\u7684\u7ed3\u679c\u548cV\u77e9\u9635\u76f8\u4e58\uff08\u52a0\u6743\u6c42\u548c\uff09\uff0creshape\u76f8\u5f53\u4e8e\u628ahead\u62fc\u63a5 x = self.proj(x) # \u901a\u8fc7\u5168\u8fde\u63a5\u8fdb\u884c\u6620\u5c04\uff08\u76f8\u5f53\u4e8e\u4e58\u8bba\u6587\u4e2d\u7684Wo\uff09 x = self.proj_drop(x) # dropOut return x class Mlp(nn.Module): # Encoder\u4e2d\u7684MLP Block \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks \"\"\" def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.): super().__init__() out_features = out_features or in_features # \u5982\u679c\u6ca1\u6709\u4f20\u5165out features\uff0c\u5c31\u9ed8\u8ba4\u662fin_features hidden_features = hidden_features or in_features self.fc1 = nn.Linear(in_features, hidden_features) self.act = act_layer() # \u9ed8\u8ba4\u662fGELU\u6fc0\u6d3b\u51fd\u6570 self.fc2 = nn.Linear(hidden_features, out_features) self.drop = nn.Dropout(drop) def forward(self, x): x = self.fc1(x) x = self.act(x) x = self.drop(x) x = self.fc2(x) x = self.drop(x) return x # Transformer Encoder\u5c42\u4ee3\u7801\u89e3\u8bfb class Block(nn.Module): # Encoder Block def __init__(self, dim, # \u6bcf\u4e2atoken\u7684\u7ef4\u5ea6 num_heads, # head\u4e2a\u6570 mlp_ratio=4., # \u7b2c\u4e00\u4e2a\u7ed3\u70b9\u4e2a\u6570\u662f\u8f93\u5165\u8282\u70b9\u7684\u56db\u500d qkv_bias=False, # \u662f\u5426\u4f7f\u7528bias qk_scale=None, drop_ratio=0., # Attention\u6a21\u5757\u4e2d\u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4f7f\u7528\u7684drop_ratio attn_drop_ratio=0., drop_path_ratio=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm): super(Block, self).__init__() self.norm1 = norm_layer(dim) # layer norm self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio) # Multihead Attention # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity() self.norm2 = norm_layer(dim) mlp_hidden_dim = int(dim * mlp_ratio) # MLP\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u7684\u4e2a\u6570 self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio) def forward(self, x): x = x + self.drop_path(self.attn(self.norm1(x))) x = x + self.drop_path(self.mlp(self.norm2(x))) return x class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, representation_size=None, distilled=False, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PatchEmbed, norm_layer=None, act_layer=None): super(VisionTransformer, self).__init__() self.num_classes = num_classes self.num_features = self.embed_dim = embed_dim # num_features for consistency with other models self.num_tokens = 2 if distilled else 1 # \u4e00\u822c\u7b49\u4e8e1 norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6) # \u4e3aNorm\u4f20\u5165\u9ed8\u8ba4\u53c2\u6570 act_layer = act_layer or nn.GELU self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # Patch Embedding\u5c42 num_patches = self.patch_embed.num_patches # patches\u7684\u603b\u4e2a\u6570 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # \u6784\u5efa\u53ef\u8bad\u7ec3\u53c2\u6570\u76840\u77e9\u9635\uff0c\u7528\u4e8e\u7c7b\u522b self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None # \u9ed8\u8ba4\u4e3aNone self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # \u4f4d\u7f6eembedding\uff0c\u548cconcat\u540e\u7684\u6570\u636e\u4e00\u6837 self.pos_drop = nn.Dropout(p=drop_ratio) # DropOut\u5c42\uff08\u6dfb\u52a0\u4e86pos_embed\u4e4b\u540e\uff09 dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)] # \u4ece0\u5230ratio\uff0c\u6709depth\u4e2a\u5143\u7d20\u7684\u7b49\u5dee\u5e8f\u5217 self.blocks = nn.Sequential(*[ Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i], norm_layer=norm_layer, act_layer=act_layer) for i in range(depth) # \u6709\u591a\u5c11\u5c42\u5faa\u73af\u591a\u5c11\u6b21 ]) self.norm = norm_layer(embed_dim) # Representation layer if representation_size and not distilled: # representation_size\u4e3aTrue\u5c31\u5728MLPHead\u6784\u5efaPreLogits,\u5426\u5219\u53ea\u6709Linear\u5c42 self.has_logits = True self.num_features = representation_size self.pre_logits = nn.Sequential(OrderedDict([ (\"fc\", nn.Linear(embed_dim, representation_size)), (\"act\", nn.Tanh()) ])) else: self.has_logits = False self.pre_logits = nn.Identity() # Classifier head(s) \u7ebf\u6027\u5206\u7c7b\u5c42 self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity() self.head_dist = None if distilled: self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity() # Weight init \u6743\u91cd\u521d\u59cb\u5316 nn.init.trunc_normal_(self.pos_embed, std=0.02) if self.dist_token is not None: nn.init.trunc_normal_(self.dist_token, std=0.02) nn.init.trunc_normal_(self.cls_token, std=0.02) self.apply(_init_vit_weights) def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] x = self.patch_embed(x) # [B, 196, 768] \u8f93\u5165Patch Embedding\u5c42 # [1, 1, 768] -> [B, 1, 768] \u5728batch\u7ef4\u5ea6\u590d\u5236batch_size\u4efd cls_token = self.cls_token.expand(x.shape[0], -1, -1) if self.dist_token is None: x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] \u5c06cls_token\u4e0ex\u5728\u7ef4\u5ea61\u4e0a\u62fc\u63a5\u3002\u6ce8\u610f\uff1acls_token\u5728\u524d\uff0cx\u5728\u540e else: x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1) x = self.pos_drop(x + self.pos_embed) # concat\u540e\u7684\u6570\u636e\u52a0\u4e0aposition\uff0c\u518d\u7ecf\u8fc7dropout\u5c42 x = self.blocks(x) # \u7ecf\u8fc7\u5806\u53e0\u7684Encoder blocks x = self.norm(x) # Layer Norm\u5c42 if self.dist_token is None: # \u4e00\u822c\u4e3aNone\uff0c\u672c\u8d28\u4e0a\u662fIdentity\u5c42 return self.pre_logits(x[:, 0]) # \u63d0\u53d6cls_token\u4fe1\u606f\uff0c\u56e0\u4e3acls_token\u7ef4\u5ea6\u5728\u524d\uff0c\u6240\u4ee5\u7d22\u5f15\u4e3a0\u5c31\u662fcls\u672c\u8eab else: return x[:, 0], x[:, 1] def forward(self, x): x = self.forward_features(x) if self.head_dist is not None: x, x_dist = self.head(x[0]), self.head_dist(x[1]) if self.training and not torch.jit.is_scripting(): # during inference, return the average of both classifier predictions return x, x_dist else: return (x + x_dist) / 2 else: x = self.head(x) return x Swin Transformer \u8bba\u6587 \u4f5c\u8005\u56e2\u961f\u4ee3\u7801 transformer \u7528\u4f5cCV\u9886\u57df\u7684\u9aa8\u5e72\u7f51\u7edc patch merging \u591a\u5c3a\u5ea6\u7684\u7279\u5f81\u56fe DETR \u7aef\u5230\u7aef\u76ee\u6807\u68c0\u6d4b\uff0c\u57fa\u4e8e\u96c6\u5408\u7684\u76ee\u6807\u51fd\u6570\u53bb\u505a\u76ee\u6807\u68c0\u6d4b\uff0c\u7b80\u5316\u4e86\u4ee5\u5f80\u76ee\u6807\u68c0\u6d4b\u975e\u6781\u5927\u503c\u6291\u5236\u7684\u90e8\u5206 \u8bad\u7ec3\uff1a CNN\u62bd\u7279\u5f81 -> Transformer Encoder \u5168\u5c40\u7279\u5f81 -> Decoder \u5f97\u5230\u56fa\u5b9a\u6570\u91cf\u9884\u6d4b\u6846 -> \u4e0eGround truth \u505a\u4e8c\u5206\u56fe\u5339\u914d\uff08100\u4e2a\u6846 2\u4e2a\u6846\u5339\u914d\u4e0a\u4e86 \u5269\u4e0b\u7684\u6807\u8bb0\u4e3a\u80cc\u666f\uff09\u7b97loss \u9884\u6d4b\uff1a \u6700\u540e\u4e00\u6b65\u5361\u7f6e\u4fe1\u5ea6 \u7ed3\u8bba\uff1a COCO\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u548cFaster RCNN \u5dee\u4e0d\u591a\u7684\u6548\u679c\uff0c\u5c0f\u7269\u4f53\u4e0a\u8868\u73b0\u4e0d\u600e\u4e48\u597d\uff0cDETR\u8bad\u7ec3\u592a\u6162\uff0c\u80fd\u591f\u4f5c\u4e3a\u7edf\u4e00\u7684\u6846\u67b6\u62d3\u5c55\u5230\u522b\u7684\u4efb\u52a1\u4e0a\uff08\u76ee\u6807\u8ffd\u8e2a\uff0c\u8bed\u4e49\u5206\u5272\uff0c\u89c6\u9891\u91cc\u7684\u59ff\u6001\u9884\u6d4b\u00b7\u00b7\u00b7\uff09","title":"Deep Learning_2"},{"location":"DeepLearning_2/#bert","text":"\u8f93\u5165\u8868\u793a \u628a\u4e24\u4e2a\u53e5\u5b50\u53d8\u6210BERT\u7684\u8f93\u5165 def get_tokens_and_segments(tokens_a, tokens_b=None): \"\"\"\u83b7\u53d6\u8f93\u5165\u5e8f\u5217\u7684\u8bcd\u5143\u53ca\u5176\u7247\u6bb5\u7d22\u5f15\"\"\" tokens = ['<cls>'] + tokens_a + ['<sep>'] # 0\u548c1\u5206\u522b\u6807\u8bb0\u7247\u6bb5A\u548cB segments = [0] * (len(tokens_a) + 2) if tokens_b is not None: tokens += tokens_b + ['<sep>'] segments += [1] * (len(tokens_b) + 1) return tokens, segments","title":"BERT"},{"location":"DeepLearning_2/#vision-transformer","text":"\u8bba\u6587 \u4ee3\u7801\u6765\u6e90 \u4e3b\u8981\u5c31\u662f\u56fe\u7247\u8f6c\u6362tokens\uff0cposition embedding\u8fd9\u91cc\u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570 Example \u8f93\u5165224x224x3 -> Embedding(16x16\u7684\u5377\u79ef\u6838\uff0c\u6b65\u8ddd\u4e3a16\u7684\u5377\u79ef\u5c42) 14x14x768 -> Flatten 196x768 -> Concat\u4e00\u4e2aClass token 197x768 -> \u52a0\u4e0aposition embedding -> Dropout -> \u91cd\u590dL\u6b21 Transformer Encoder 197x768-> LayerNorm -> Extract class token 1x768(\u4e4b\u524dconcat\u7684class token)-> MLP Head class PatchEmbed(nn.Module): \"\"\" 2D Image to Patch Embedding \"\"\" def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None): super().__init__() img_size = (img_size, img_size) patch_size = (patch_size, patch_size) self.img_size = img_size self.patch_size = patch_size # \u6bcf\u4e2apatch\u7684\u5927\u5c0f self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) # 224/16 -> 14*14 self.num_patches = self.grid_size[0] * self.grid_size[1] # patches\u7684\u6570\u76ee self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size) # \u5377\u79ef\u6838\u5927\u5c0f\u548cpatch_size\u90fd\u662f16*16 self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() # \u5982\u679c\u6ca1\u6709\u4f20\u5165norm\u5c42\uff0c\u5c31\u4f7f\u7528identity def forward(self, x): B, C, H, W = x.shape # \u6ce8\u610f\uff0c\u5728vit\u6a21\u578b\u4e2d\u8f93\u5165\u5927\u5c0f\u5fc5\u987b\u662f\u56fa\u5b9a\u7684\uff0c\u9ad8\u5bbd\u548c\u8bbe\u5b9a\u503c\u76f8\u540c assert H == self.img_size[0] and W == self.img_size[1], \\ f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\" # flatten: [B, C, H, W] -> [B, C, HW] # transpose: [B, C, HW] -> [B, HW, C] x = self.proj(x).flatten(2).transpose(1, 2) x = self.norm(x) return x class Attention(nn.Module): # Multi-head selfAttention \u6a21\u5757 def __init__(self, dim, # \u8f93\u5165token\u7684dim num_heads=8, # head\u7684\u4e2a\u6570 qkv_bias=False, # \u751f\u6210qkv\u65f6\u662f\u5426\u4f7f\u7528\u504f\u7f6e qk_scale=None, attn_drop_ratio=0., # \u4e24\u4e2adropout ratio proj_drop_ratio=0.): super(Attention, self).__init__() self.num_heads = num_heads head_dim = dim // num_heads # \u6bcf\u4e2ahead\u7684dim self.scale = qk_scale or head_dim ** -0.5 # \u4e0d\u53bb\u4f20\u5165qkscale\uff0c\u4e5f\u5c31\u662f1/\u221adim_k self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) # \u4f7f\u7528\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\uff0c\u4e00\u6b21\u5f97\u5230qkv self.attn_drop = nn.Dropout(attn_drop_ratio) self.proj = nn.Linear(dim, dim) # \u628a\u591a\u4e2ahead\u8fdb\u884cConcat\u64cd\u4f5c\uff0c\u7136\u540e\u901a\u8fc7Wo\u6620\u5c04\uff0c\u8fd9\u91cc\u7528\u5168\u8fde\u63a5\u5c42\u4ee3\u66ff self.proj_drop = nn.Dropout(proj_drop_ratio) def forward(self, x): # [batch_size, num_patches + 1, total_embed_dim] \u52a01\u4ee3\u8868\u7c7b\u522b\uff0c\u9488\u5bf9ViT-B/16\uff0cdim\u662f768 B, N, C = x.shape # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim] # reshape: -> [batch_size, num_patches + 1, 3\uff08\u4ee3\u8868qkv\uff09, num_heads\uff08\u4ee3\u8868head\u6570\uff09, embed_dim_per_head\uff08\u6bcf\u4e2ahead\u7684qkv\u7ef4\u5ea6\uff09] # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head] qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # [batch_size, num_heads, num_patches + 1, embed_dim_per_head] q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1] # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1] attn = (q @ k.transpose(-2, -1)) * self.scale # \u6bcf\u4e2aheader\u7684q\u548ck\u76f8\u4e58\uff0c\u9664\u4ee5\u221adim_k\uff08\u76f8\u5f53\u4e8enorm\u5904\u7406\uff09 attn = attn.softmax(dim=-1) # \u901a\u8fc7softmax\u5904\u7406\uff08\u76f8\u5f53\u4e8e\u5bf9\u6bcf\u4e00\u884c\u7684\u6570\u636esoftmax\uff09 attn = self.attn_drop(attn) # dropOut\u5c42 # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head] # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head] # reshape: -> [batch_size, num_patches + 1, total_embed_dim] x = (attn @ v).transpose(1, 2).reshape(B, N, C) # \u5f97\u5230\u7684\u7ed3\u679c\u548cV\u77e9\u9635\u76f8\u4e58\uff08\u52a0\u6743\u6c42\u548c\uff09\uff0creshape\u76f8\u5f53\u4e8e\u628ahead\u62fc\u63a5 x = self.proj(x) # \u901a\u8fc7\u5168\u8fde\u63a5\u8fdb\u884c\u6620\u5c04\uff08\u76f8\u5f53\u4e8e\u4e58\u8bba\u6587\u4e2d\u7684Wo\uff09 x = self.proj_drop(x) # dropOut return x class Mlp(nn.Module): # Encoder\u4e2d\u7684MLP Block \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks \"\"\" def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.): super().__init__() out_features = out_features or in_features # \u5982\u679c\u6ca1\u6709\u4f20\u5165out features\uff0c\u5c31\u9ed8\u8ba4\u662fin_features hidden_features = hidden_features or in_features self.fc1 = nn.Linear(in_features, hidden_features) self.act = act_layer() # \u9ed8\u8ba4\u662fGELU\u6fc0\u6d3b\u51fd\u6570 self.fc2 = nn.Linear(hidden_features, out_features) self.drop = nn.Dropout(drop) def forward(self, x): x = self.fc1(x) x = self.act(x) x = self.drop(x) x = self.fc2(x) x = self.drop(x) return x # Transformer Encoder\u5c42\u4ee3\u7801\u89e3\u8bfb class Block(nn.Module): # Encoder Block def __init__(self, dim, # \u6bcf\u4e2atoken\u7684\u7ef4\u5ea6 num_heads, # head\u4e2a\u6570 mlp_ratio=4., # \u7b2c\u4e00\u4e2a\u7ed3\u70b9\u4e2a\u6570\u662f\u8f93\u5165\u8282\u70b9\u7684\u56db\u500d qkv_bias=False, # \u662f\u5426\u4f7f\u7528bias qk_scale=None, drop_ratio=0., # Attention\u6a21\u5757\u4e2d\u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4f7f\u7528\u7684drop_ratio attn_drop_ratio=0., drop_path_ratio=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm): super(Block, self).__init__() self.norm1 = norm_layer(dim) # layer norm self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio) # Multihead Attention # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity() self.norm2 = norm_layer(dim) mlp_hidden_dim = int(dim * mlp_ratio) # MLP\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u7684\u4e2a\u6570 self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio) def forward(self, x): x = x + self.drop_path(self.attn(self.norm1(x))) x = x + self.drop_path(self.mlp(self.norm2(x))) return x class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, representation_size=None, distilled=False, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PatchEmbed, norm_layer=None, act_layer=None): super(VisionTransformer, self).__init__() self.num_classes = num_classes self.num_features = self.embed_dim = embed_dim # num_features for consistency with other models self.num_tokens = 2 if distilled else 1 # \u4e00\u822c\u7b49\u4e8e1 norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6) # \u4e3aNorm\u4f20\u5165\u9ed8\u8ba4\u53c2\u6570 act_layer = act_layer or nn.GELU self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # Patch Embedding\u5c42 num_patches = self.patch_embed.num_patches # patches\u7684\u603b\u4e2a\u6570 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # \u6784\u5efa\u53ef\u8bad\u7ec3\u53c2\u6570\u76840\u77e9\u9635\uff0c\u7528\u4e8e\u7c7b\u522b self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None # \u9ed8\u8ba4\u4e3aNone self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # \u4f4d\u7f6eembedding\uff0c\u548cconcat\u540e\u7684\u6570\u636e\u4e00\u6837 self.pos_drop = nn.Dropout(p=drop_ratio) # DropOut\u5c42\uff08\u6dfb\u52a0\u4e86pos_embed\u4e4b\u540e\uff09 dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)] # \u4ece0\u5230ratio\uff0c\u6709depth\u4e2a\u5143\u7d20\u7684\u7b49\u5dee\u5e8f\u5217 self.blocks = nn.Sequential(*[ Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i], norm_layer=norm_layer, act_layer=act_layer) for i in range(depth) # \u6709\u591a\u5c11\u5c42\u5faa\u73af\u591a\u5c11\u6b21 ]) self.norm = norm_layer(embed_dim) # Representation layer if representation_size and not distilled: # representation_size\u4e3aTrue\u5c31\u5728MLPHead\u6784\u5efaPreLogits,\u5426\u5219\u53ea\u6709Linear\u5c42 self.has_logits = True self.num_features = representation_size self.pre_logits = nn.Sequential(OrderedDict([ (\"fc\", nn.Linear(embed_dim, representation_size)), (\"act\", nn.Tanh()) ])) else: self.has_logits = False self.pre_logits = nn.Identity() # Classifier head(s) \u7ebf\u6027\u5206\u7c7b\u5c42 self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity() self.head_dist = None if distilled: self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity() # Weight init \u6743\u91cd\u521d\u59cb\u5316 nn.init.trunc_normal_(self.pos_embed, std=0.02) if self.dist_token is not None: nn.init.trunc_normal_(self.dist_token, std=0.02) nn.init.trunc_normal_(self.cls_token, std=0.02) self.apply(_init_vit_weights) def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] x = self.patch_embed(x) # [B, 196, 768] \u8f93\u5165Patch Embedding\u5c42 # [1, 1, 768] -> [B, 1, 768] \u5728batch\u7ef4\u5ea6\u590d\u5236batch_size\u4efd cls_token = self.cls_token.expand(x.shape[0], -1, -1) if self.dist_token is None: x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] \u5c06cls_token\u4e0ex\u5728\u7ef4\u5ea61\u4e0a\u62fc\u63a5\u3002\u6ce8\u610f\uff1acls_token\u5728\u524d\uff0cx\u5728\u540e else: x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1) x = self.pos_drop(x + self.pos_embed) # concat\u540e\u7684\u6570\u636e\u52a0\u4e0aposition\uff0c\u518d\u7ecf\u8fc7dropout\u5c42 x = self.blocks(x) # \u7ecf\u8fc7\u5806\u53e0\u7684Encoder blocks x = self.norm(x) # Layer Norm\u5c42 if self.dist_token is None: # \u4e00\u822c\u4e3aNone\uff0c\u672c\u8d28\u4e0a\u662fIdentity\u5c42 return self.pre_logits(x[:, 0]) # \u63d0\u53d6cls_token\u4fe1\u606f\uff0c\u56e0\u4e3acls_token\u7ef4\u5ea6\u5728\u524d\uff0c\u6240\u4ee5\u7d22\u5f15\u4e3a0\u5c31\u662fcls\u672c\u8eab else: return x[:, 0], x[:, 1] def forward(self, x): x = self.forward_features(x) if self.head_dist is not None: x, x_dist = self.head(x[0]), self.head_dist(x[1]) if self.training and not torch.jit.is_scripting(): # during inference, return the average of both classifier predictions return x, x_dist else: return (x + x_dist) / 2 else: x = self.head(x) return x","title":"Vision Transformer"},{"location":"DeepLearning_2/#swin-transformer","text":"\u8bba\u6587 \u4f5c\u8005\u56e2\u961f\u4ee3\u7801 transformer \u7528\u4f5cCV\u9886\u57df\u7684\u9aa8\u5e72\u7f51\u7edc patch merging \u591a\u5c3a\u5ea6\u7684\u7279\u5f81\u56fe","title":"Swin Transformer"},{"location":"DeepLearning_2/#detr","text":"\u7aef\u5230\u7aef\u76ee\u6807\u68c0\u6d4b\uff0c\u57fa\u4e8e\u96c6\u5408\u7684\u76ee\u6807\u51fd\u6570\u53bb\u505a\u76ee\u6807\u68c0\u6d4b\uff0c\u7b80\u5316\u4e86\u4ee5\u5f80\u76ee\u6807\u68c0\u6d4b\u975e\u6781\u5927\u503c\u6291\u5236\u7684\u90e8\u5206 \u8bad\u7ec3\uff1a CNN\u62bd\u7279\u5f81 -> Transformer Encoder \u5168\u5c40\u7279\u5f81 -> Decoder \u5f97\u5230\u56fa\u5b9a\u6570\u91cf\u9884\u6d4b\u6846 -> \u4e0eGround truth \u505a\u4e8c\u5206\u56fe\u5339\u914d\uff08100\u4e2a\u6846 2\u4e2a\u6846\u5339\u914d\u4e0a\u4e86 \u5269\u4e0b\u7684\u6807\u8bb0\u4e3a\u80cc\u666f\uff09\u7b97loss \u9884\u6d4b\uff1a \u6700\u540e\u4e00\u6b65\u5361\u7f6e\u4fe1\u5ea6 \u7ed3\u8bba\uff1a COCO\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u548cFaster RCNN \u5dee\u4e0d\u591a\u7684\u6548\u679c\uff0c\u5c0f\u7269\u4f53\u4e0a\u8868\u73b0\u4e0d\u600e\u4e48\u597d\uff0cDETR\u8bad\u7ec3\u592a\u6162\uff0c\u80fd\u591f\u4f5c\u4e3a\u7edf\u4e00\u7684\u6846\u67b6\u62d3\u5c55\u5230\u522b\u7684\u4efb\u52a1\u4e0a\uff08\u76ee\u6807\u8ffd\u8e2a\uff0c\u8bed\u4e49\u5206\u5272\uff0c\u89c6\u9891\u91cc\u7684\u59ff\u6001\u9884\u6d4b\u00b7\u00b7\u00b7\uff09","title":"DETR"}]}