{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ChenYuhan's notes","text":"<p>\u8bb0\u5f55\u4e86\u5b66\u4e60\u7684\u7b14\u8bb0</p>"},{"location":"Course/AI_chips/","title":"AI Chips","text":""},{"location":"Course/AI_chips/#out-of-order-execution","title":"Out-of-Order execution","text":"<p>\u987a\u5e8fPipeline\u7ecf\u5e38\u88abinstall\uff0cLoad\u4ece\u5185\u5b58\u91cc\u8bfb\u4e1c\u897f\u53ef\u80fd\u8d85\u8fc7200\u4e2a\u6307\u4ee4\u5468\u671f</p> <p>Move the dependent instructions out of the way of indeoendent ones</p> <pre><code>MUL  R3 &lt;- R1,R2 F|D|E|E|E|E|R|W\nADD  R3 &lt;- R3,R1   F|D|-|-|-|E|R|W\nADD  R1 &lt;- R6,R7     F|-|-|-|D|E|R|W\nMUL  R5 &lt;- R6,R8             F|D|E|E|E|E|R|W\nADD  R7 &lt;- R3,R5               F|D|-|-|-|E|R|W\n16 cycles\n</code></pre> <pre><code>MUL  R3 &lt;- R1,R2 F|D|E|E|E|E|R|W\nADD  R3 &lt;- R3,R1   F|D|wait |E|R|W\nADD  R1 &lt;- R6,R7     F|D|E|R|-|-|-|W\nMUL  R5 &lt;- R6,R8       F|D|E|E|E|E|R|W\nADD  R7 &lt;- R3,R5         F|D|wait |E|R|W\n(\u66f4\u6539\u987a\u5e8f\u540e)12 cycles\n</code></pre>"},{"location":"Course/AI_chips/#tomasulos-algorithms","title":"Tomasulo\u2019s Algorithms","text":"<p>Hump1:Reservation stations Hump2: Reorder buffer</p>"},{"location":"Course/AI_chips/#enabling-ooo-execution","title":"Enabling OoO Execution","text":"<ol> <li>Need to link the consumer of a value to the producer</li> <li>Need to buffer instructions until they are read to execute</li> <li>instruction need to keep track of radiness of source values</li> <li>When all source values of an instruction are ready,need to dispatch the instruction to its functional unit</li> </ol> <p>...</p> <p>\u4e0d\u5982\u770bq\u795e</p>"},{"location":"DeepLearning/Attention/","title":"Attention","text":"<p>\u6bcf\u4e2a\u503c\uff08Value\uff09\u548c\u4e00\u4e2a\u952e\uff08key\uff09\u4e00\u4e00\u5bf9\u5e94\uff0c\u901a\u8fc7\u67e5\u8be2\uff08query\uff09\u4e0e\u952e\u8fdb\u884c\u5339\u914d\uff0c\u5f97\u5230\u6700\u5339\u914d\u7684\u503c\u3002</p>"},{"location":"DeepLearning/Attention/#nadaraya-waston","title":"Nadaraya-Waston \u6838\u56de\u5f52","text":"\\[f(x) = \\sum_{i=1}^n \\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)} y_i\\] <p>\u66f4\u4e00\u822c\u7684\u8868\u793a</p> \\[f(x) = \\sum_{i=1}^n \\alpha(x, x_i) y_i,\\] <p>\u7528\u9ad8\u65af\u6838\\(K(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{u^2}{2}).\\)\u5e26\u5165\u7b2c\u4e00\u4e2a\u5f0f\u5b50</p> <p>\u5f97\u5230</p> \\[\\begin{aligned} f(x) &amp;= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}(x - x_i)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}(x - x_j)^2\\right)} y_i \\\\&amp;= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}(x - x_i)^2\\right) y_i. \\end{aligned}\\] <p>\u5e26\u5b66\u4e60\u53c2\u6570w</p> \\[\\begin{aligned}f(x) &amp;= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}((x - x_i)w)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}((x - x_j)w)^2\\right)} y_i \\\\&amp;= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}((x - x_i)w)^2\\right) y_i.\\end{aligned}\\]"},{"location":"DeepLearning/Attention/#_1","title":"\u6ce8\u610f\u529b\u8bc4\u5206\u51fd\u6570","text":"<p>\u7528\u6570\u5b66\u8bed\u8a00\u63cf\u8ff0\uff0c\u5047\u8bbe\u6709\u4e00\u4e2a\u67e5\u8be2\\(\\mathbf{q} \\in \\mathbb{R}^q\\)\u548c\\(m\\)\u4e2a\u201c\u952e\uff0d\u503c\u201d\u5bf9\\((\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)\\)\uff0c \u5176\u4e2d\\(\\mathbf{k}_i \\in \\mathbb{R}^k\\)\uff0c\\(\\mathbf{v}_i \\in \\mathbb{R}^v\\)\u3002 \u6ce8\u610f\u529b\u6c47\u805a\u51fd\u6570\\(f\\)\u5c31\u88ab\u8868\u793a\u6210\u503c\u7684\u52a0\u6743\u548c\uff1a</p> \\[f(\\mathbf{q}, (\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)) = \\sum_{i=1}^m \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i \\in \\mathbb{R}^v,\\] <p>\u5176\u4e2d\u67e5\u8be2\\(\\mathbf{q}\\)\u548c\u952e\\(\\mathbf{k}_i\\)\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff08\u6807\u91cf\uff09\u662f\u901a\u8fc7\u6ce8\u610f\u529b\u8bc4\u5206\u51fd\u6570\\(a\\)\u5c06\u4e24\u4e2a\u5411\u91cf\u6620\u5c04\u6210\u6807\u91cf\uff0c\u518d\u7ecf\u8fc7softmax\u8fd0\u7b97\u5f97\u5230\u7684\uff1a</p> \\[\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}_i))}{\\sum_{j=1}^m \\exp(a(\\mathbf{q}, \\mathbf{k}_j))} \\in \\mathbb{R}.\\]"},{"location":"DeepLearning/Attention/#additive-attention","title":"Additive Attention","text":"\\[a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R},\\] <p>\u5176\u4e2d\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u662f\\(\\mathbf W_q\\in\\mathbb R^{h\\times q}\\)\u3001\\(\\mathbf W_k\\in\\mathbb R^{h\\times k}\\)\u548c\\(\\mathbf w_v\\in\\mathbb R^{h}\\)\u3002</p> <pre><code>class AdditiveAttention(nn.Module):\n    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n        super(AdditiveAttention, self).__init__(**kwargs)\n        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, queries, keys, values, valid_lens):\n        queries, keys = self.W_q(queries), self.W_k(keys)\n        # \u5728\u7ef4\u5ea6\u6269\u5c55\u540e\uff0c\n        # queries\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570\uff0c1\uff0cnum_hidden)\n        # key\u7684\u5f62\u72b6\uff1a(batch_size\uff0c1\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0cnum_hiddens)\n        # \u4f7f\u7528\u5e7f\u64ad\u65b9\u5f0f\u8fdb\u884c\u6c42\u548c\n        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n        features = torch.tanh(features)\n        # self.w_v\u4ec5\u6709\u4e00\u4e2a\u8f93\u51fa\uff0c\u56e0\u6b64\u4ece\u5f62\u72b6\u4e2d\u79fb\u9664\u6700\u540e\u90a3\u4e2a\u7ef4\u5ea6\u3002\n        # scores\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570\uff0c\u201c\u952e-\u503c\u201d\u5bf9\u7684\u4e2a\u6570)\n        scores = self.w_v(features).squeeze(-1)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        # values\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0c\u503c\u7684\u7ef4\u5ea6)\n        # \u8f93\u51fa\u7684\u5f62\u72b6\uff1a(batch_size,\u67e5\u8be2\u7684\u4e2a\u6570,\u503c\u7684\u7ef4\u5ea6)\n        return torch.bmm(self.dropout(self.attention_weights), values)\n</code></pre>"},{"location":"DeepLearning/Attention/#scaled-dot-product-attention","title":"Scaled dot-product attention","text":"<p>\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\uff08scaled dot-product attention\uff09\u8981\u6c42query\u548cKey\u957f\u5ea6\u76f8\u540c\uff0c\u8bc4\u5206\u51fd\u6570\u4e3a\uff1a</p> \\[a(\\mathbf q, \\mathbf k) = \\mathbf{q}^\\top \\mathbf{k}  /\\sqrt{d}.\\] <p>\u57fa\u4e8e\\(n\\)\u4e2a\u67e5\u8be2\u548c\\(m\\)\u4e2a\u952e\uff0d\u503c\u5bf9\u8ba1\u7b97\u6ce8\u610f\u529b\uff0c\u5176\u4e2d\u67e5\u8be2\u548c\u952e\u7684\u957f\u5ea6\u5747\u4e3a\\(d\\)\uff0c\u503c\u7684\u957f\u5ea6\u4e3a\\(v\\)\u3002\u67e5\u8be2\\(\\mathbf Q\\in\\mathbb R^{n\\times d}\\)\u3001\u952e\\(\\mathbf K\\in\\mathbb R^{m\\times d}\\)\u548c\u503c\\(\\mathbf V\\in\\mathbb R^{m\\times v}\\)\u7684\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u662f\uff1a</p> \\[ \\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}.\\] <pre><code>class DotProductAttention(nn.Module):\n    \"\"\"\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\"\"\"\n    def __init__(self, dropout, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    # queries\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570\uff0cd)\n    # keys\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0cd)\n    # values\u7684\u5f62\u72b6\uff1a(batch_size\uff0c\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570\uff0c\u503c\u7684\u7ef4\u5ea6)\n    # valid_lens\u7684\u5f62\u72b6:(batch_size\uff0c)\u6216\u8005(batch_size\uff0c\u67e5\u8be2\u7684\u4e2a\u6570)\n    def forward(self, queries, keys, values, valid_lens=None):\n        d = queries.shape[-1]\n        # \u8bbe\u7f6etranspose_b=True\u4e3a\u4e86\u4ea4\u6362keys\u7684\u6700\u540e\u4e24\u4e2a\u7ef4\u5ea6\n        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        return torch.bmm(self.dropout(self.attention_weights), values)\n</code></pre>"},{"location":"DeepLearning/Attention/#attention-decoder","title":"Attention Decoder","text":"<p>\u4e0e\u4e4b\u524dSeq2Seq\u76f8\u6bd4\uff0c\u4e0a\u4e0b\u6587\u53d8\u91cf\\(\\mathbf{c}\\)\u5728\u4efb\u4f55\u89e3\u7801\u65f6\u95f4\u6b65\\(t'\\)\u90fd\u4f1a\u88ab\\(\\mathbf{c}_{t'}\\)\u66ff\u6362\u3002\u5047\u8bbe\u8f93\u5165\u5e8f\u5217\u4e2d\u6709\\(T\\)\u4e2a\u8bcd\u5143\uff0c\u89e3\u7801\u65f6\u95f4\u6b65\\(t'\\)\u7684\u4e0a\u4e0b\u6587\u53d8\u91cf\u662f\u6ce8\u610f\u529b\u96c6\u4e2d\u7684\u8f93\u51fa\uff1a</p> \\[\\mathbf{c}_{t'} = \\sum_{t=1}^T \\alpha(\\mathbf{s}_{t' - 1}, \\mathbf{h}_t) \\mathbf{h}_t\\] <p>\u5176\u4e2d\uff0c\u65f6\u95f4\u6b65\\(t' - 1\\)\u65f6\u7684\u89e3\u7801\u5668\u9690\u72b6\u6001\\(\\mathbf{s}_{t' - 1}\\)\u662f\u67e5\u8be2\uff0c\u7f16\u7801\u5668\u9690\u72b6\u6001\\(\\mathbf{h}_t\\)\u65e2\u662f\u952e\uff0c\u4e5f\u662f\u503c\u3002</p> <pre><code>class Seq2SeqAttentionDecoder(AttentionDecoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n        self.attention = d2l.AdditiveAttention(\n            num_hiddens, num_hiddens, num_hiddens, dropout)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.GRU(\n            embed_size + num_hiddens, num_hiddens, num_layers,\n            dropout=dropout)\n        self.dense = nn.Linear(num_hiddens, vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_lens, *args):\n        # outputs\u7684\u5f62\u72b6\u4e3a(batch_size\uff0cnum_steps\uff0cnum_hiddens).\n        # hidden_state\u7684\u5f62\u72b6\u4e3a(num_layers\uff0cbatch_size\uff0cnum_hiddens)\n        outputs, hidden_state = enc_outputs\n        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n\n    def forward(self, X, state):\n        # enc_outputs\u7684\u5f62\u72b6\u4e3a(batch_size,num_steps,num_hiddens).\n        # hidden_state\u7684\u5f62\u72b6\u4e3a(num_layers,batch_size,\n        # num_hiddens)\n        enc_outputs, hidden_state, enc_valid_lens = state\n        # \u8f93\u51faX\u7684\u5f62\u72b6\u4e3a(num_steps,batch_size,embed_size)\n        X = self.embedding(X).permute(1, 0, 2)\n        outputs, self._attention_weights = [], []\n        for x in X:\n            # query\u7684\u5f62\u72b6\u4e3a(batch_size,1,num_hiddens)\n            query = torch.unsqueeze(hidden_state[-1], dim=1)\n            # context\u7684\u5f62\u72b6\u4e3a(batch_size,1,num_hiddens)\n            context = self.attention(\n                query, enc_outputs, enc_outputs, enc_valid_lens)\n            # \u5728\u7279\u5f81\u7ef4\u5ea6\u4e0a\u8fde\u7ed3\n            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n            # \u5c06x\u53d8\u5f62\u4e3a(1,batch_size,embed_size+num_hiddens)\n            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n            outputs.append(out)\n            self._attention_weights.append(self.attention.attention_weights)\n        # \u5168\u8fde\u63a5\u5c42\u53d8\u6362\u540e\uff0coutputs\u7684\u5f62\u72b6\u4e3a\n        # (num_steps,batch_size,vocab_size)\n        outputs = self.dense(torch.cat(outputs, dim=0))\n        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,\n                                          enc_valid_lens]\n\n    @property\n    def attention_weights(self):\n        return self._attention_weights\n</code></pre>"},{"location":"DeepLearning/Attention/#mutihead-attention","title":"Mutihead-Attention","text":"<p>\u7ed9\u5b9a\u67e5\u8be2\\(\\mathbf{q} \\in \\mathbb{R}^{d_q}\\)\u3001\u952e\\(\\mathbf{k} \\in \\mathbb{R}^{d_k}\\)\u548c\u503c\\(\\mathbf{v} \\in \\mathbb{R}^{d_v}\\)\uff0c\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\\(\\mathbf{h}_i\\)\uff08\\(i = 1, \\ldots, h\\)\uff09\u7684\u8ba1\u7b97\u65b9\u6cd5\u4e3a\uff1a</p> \\[\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},\\] <p>\u5176\u4e2d\uff0c\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u5305\u62ec\\(\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}\\)\u3001\\(\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}\\)\u548c \\(\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}\\)\uff0c\u4ee5\u53ca\u4ee3\u8868\u6ce8\u610f\u529b\u6c47\u805a\u7684\u51fd\u6570\\(f\\)\u3002 \u591a\u5934\u6ce8\u610f\u529b\u7684\u8f93\u51fa\u9700\u8981\u7ecf\u8fc7\u53e6\u4e00\u4e2a\u7ebf\u6027\u8f6c\u6362\uff0c\u5b83\u5bf9\u5e94\u7740\\(h\\)\u4e2a\u5934\u8fde\u7ed3\u540e\u7684\u7ed3\u679c\uff0c\u53ef\u5b66\u4e60\u53c2\u6570\u662f\\(\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}\\)\uff1a</p> \\[\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.\\] <p>\u5728\u5b9e\u73b0\u8fc7\u7a0b\u4e2d\u901a\u5e38\u9009\u62e9\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u4f5c\u4e3a\u6bcf\u4e00\u4e2a\u6ce8\u610f\u529b\u5934\u3002 \u8bbe\u5b9a\\(p_q = p_k = p_v = p_o / h\\)\u3002\u5982\u679c\u5c06\u67e5\u8be2\u3001\u952e\u548c\u503c\u7684\u7ebf\u6027\u53d8\u6362\u7684\u8f93\u51fa\u6570\u91cf\u8bbe\u7f6e\u4e3a\\(p_q h = p_k h = p_v h = p_o\\)\uff0c\u5219\u53ef\u4ee5\u5e76\u884c\u8ba1\u7b97\\(h\\)\u4e2a\u5934\u3002</p>"},{"location":"DeepLearning/Attention/#self-attention","title":"Self-Attention","text":"<p>\u7ed9\u5b9a\u4e00\u4e2a\u7531\u8bcd\u5143\u7ec4\u6210\u7684\u8f93\u5165\u5e8f\u5217\\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\)\uff0c\u5176\u4e2d\u4efb\u610f\\(\\mathbf{x}_i \\in \\mathbb{R}^d\\)\uff08\\(1 \\leq i \\leq n\\)\uff09\u3002\u8be5\u5e8f\u5217\u7684\u81ea\u6ce8\u610f\u529b\u8f93\u51fa\u4e3a\u4e00\u4e2a\u957f\u5ea6\u76f8\u540c\u7684\u5e8f\u5217 \\(\\mathbf{y}_1, \\ldots, \\mathbf{y}_n\\)\uff0c\u5176\u4e2d\uff1a</p> \\[\\mathbf{y}_i = f(\\mathbf{x}_i, (\\mathbf{x}_1, \\mathbf{x}_1), \\ldots, (\\mathbf{x}_n, \\mathbf{x}_n)) \\in \\mathbb{R}^d\\]"},{"location":"DeepLearning/Attention/#positional-encoding","title":"Positional-Encoding","text":"<p>\u7531\u4e8eSelf Attention \u6ca1\u6709\u8bb0\u5f55\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5047\u8bbe\u957f\u5ea6\u4e3an\u7684\u5e8f\u5217\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\),\u7528\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635\\(\\mathbf{P} \\in \\mathbb{R}^{n \\times d}\\) \u6765\u8ba1\u7b97\\(\\mathbf{X} + \\mathbf{P}\\) \u4f5c\u4e3aSelf Attention \u7684\u8f93\u5165\u3002</p> <p>\u77e9\u9635\\(\\mathbf{P}\\)\u7b2c\\(i\\)\u884c\u3001\u7b2c\\(2j\\)\u5217\u548c\\(2j+1\\)\u5217\u4e0a\u7684\u5143\u7d20\u4e3a\uff1a</p> \\[\\begin{aligned} p_{i, 2j} &amp;= \\sin\\left(\\frac{i}{10000^{2j/d}}\\right),\\\\p_{i, 2j+1} &amp;= \\cos\\left(\\frac{i}{10000^{2j/d}}\\right).\\end{aligned}\\] <p>\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f \\((p_{i, 2j}, p_{i, 2j+1})\\)\u90fd\u53ef\u4ee5\u7ebf\u6027\u6295\u5f71\u5230\\((p_{i+\\delta, 2j}, p_{i+\\delta, 2j+1})\\)\uff1a</p> \\[\\begin{aligned} &amp;\\begin{bmatrix} \\cos(\\delta \\omega_j) &amp; \\sin(\\delta \\omega_j) \\\\  -\\sin(\\delta \\omega_j) &amp; \\cos(\\delta \\omega_j) \\\\ \\end{bmatrix} \\begin{bmatrix} p_{i, 2j} \\\\  p_{i, 2j+1} \\\\ \\end{bmatrix}\\\\ =&amp;\\begin{bmatrix} \\cos(\\delta \\omega_j) \\sin(i \\omega_j) + \\sin(\\delta \\omega_j) \\cos(i \\omega_j) \\\\  -\\sin(\\delta \\omega_j) \\sin(i \\omega_j) + \\cos(\\delta \\omega_j) \\cos(i \\omega_j) \\\\ \\end{bmatrix}\\\\ =&amp;\\begin{bmatrix} \\sin\\left((i+\\delta) \\omega_j\\right) \\\\  \\cos\\left((i+\\delta) \\omega_j\\right) \\\\ \\end{bmatrix}\\\\ =&amp; \\begin{bmatrix} p_{i+\\delta, 2j} \\\\  p_{i+\\delta, 2j+1} \\\\ \\end{bmatrix}, \\end{aligned}\\] <p>\u56e0\u6b64\u8fd9\u4e2a\u77e9\u9635\u4e0d\u4f9d\u8d56\u4e8e\u4efb\u4f55\u4f4d\u7f6e\u7684\u7d22\u5f15\u3002</p>"},{"location":"DeepLearning/Attention/#transformer","title":"Transformer","text":""},{"location":"DeepLearning/Attention/#addnorm","title":"Add&amp;Norm","text":"<p>\u4f7f\u7528LayerNorm \uff08\u4e0d\u6539\u53d8\u8bed\u4e49\u5411\u91cf\u7684\u65b9\u5411\uff0c\u6539\u53d8\u6a21\u957f\uff09</p> <pre><code>class AddNorm(nn.Module):\n    def __init__(self, normalized_shape, dropout, **kwargs):\n        super(AddNorm, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.ln = nn.LayerNorm(normalized_shape)\n\n    def forward(self, X, Y):\n        return self.ln(self.dropout(Y) + X)\n</code></pre>"},{"location":"DeepLearning/Attention/#encoder","title":"Encoder","text":"<pre><code>class EncoderBlock(nn.Module):\n    def __init__(self, key_size, query_size, value_size, num_hiddens,\n                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n                 dropout, use_bias=False, **kwargs):\n        super(EncoderBlock, self).__init__(**kwargs)\n        self.attention = d2l.MultiHeadAttention(\n            key_size, query_size, value_size, num_hiddens, num_heads, dropout,\n            use_bias)\n        self.addnorm1 = AddNorm(norm_shape, dropout)\n        self.ffn = PositionWiseFFN(\n            ffn_num_input, ffn_num_hiddens, num_hiddens)\n        self.addnorm2 = AddNorm(norm_shape, dropout)\n\n    def forward(self, X, valid_lens):\n        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n        return self.addnorm2(Y, self.ffn(Y))\n</code></pre> <p>Encoder\u5c42\u90fd\u4e0d\u4f1a\u6539\u53d8\u8f93\u5165\u7684\u5f62\u72b6\u3002</p> <pre><code>class TransformerEncoder(d2l.Encoder):\n    def __init__(self, vocab_size, key_size, query_size, value_size,\n                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n                 num_heads, num_layers, dropout, use_bias=False, **kwargs):\n        super(TransformerEncoder, self).__init__(**kwargs)\n        self.num_hiddens = num_hiddens\n        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n        self.blks = nn.Sequential()\n        for i in range(num_layers):\n            self.blks.add_module(\"block\"+str(i),\n                EncoderBlock(key_size, query_size, value_size, num_hiddens,\n                             norm_shape, ffn_num_input, ffn_num_hiddens,\n                             num_heads, dropout, use_bias))\n\n    def forward(self, X, valid_lens, *args):\n        # \u56e0\u4e3a\u4f4d\u7f6e\u7f16\u7801\u503c\u5728-1\u548c1\u4e4b\u95f4\uff0c\n        # \u56e0\u6b64\u5d4c\u5165\u503c\u4e58\u4ee5\u5d4c\u5165\u7ef4\u5ea6\u7684\u5e73\u65b9\u6839\u8fdb\u884c\u7f29\u653e\uff0c\n        # \u7136\u540e\u518d\u4e0e\u4f4d\u7f6e\u7f16\u7801\u76f8\u52a0\u3002\n        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n        self.attention_weights = [None] * len(self.blks)\n        for i, blk in enumerate(self.blks):\n            X = blk(X, valid_lens)\n            self.attention_weights[\n                i] = blk.attention.attention.attention_weights\n        return X\n</code></pre>"},{"location":"DeepLearning/Attention/#decoder","title":"Decoder","text":"<p>\u5728\u63a9\u853d\u591a\u5934\u89e3\u7801\u5668\u81ea\u6ce8\u610f\u529b\u5c42\uff08\u7b2c\u4e00\u4e2a\u5b50\u5c42\uff09\u4e2d\uff0c\u67e5\u8be2\u3001\u952e\u548c\u503c\u90fd\u6765\u81ea\u4e0a\u4e00\u4e2a\u89e3\u7801\u5668\u5c42\u7684\u8f93\u51fa\u3002\u4e3a\u4e86\u5728\u89e3\u7801\u5668\u4e2d\u4fdd\u7559\u81ea\u56de\u5f52\u7684\u5c5e\u6027\uff0c\u5176\u63a9\u853d\u81ea\u6ce8\u610f\u529b\u8bbe\u5b9a\u4e86\u53c2\u6570\uff0c\u4ee5\u4fbf\u4efb\u4f55\u67e5\u8be2\u90fd\u53ea\u4f1a\u4e0e\u89e3\u7801\u5668\u4e2d\u6240\u6709\u5df2\u7ecf\u751f\u6210\u8bcd\u5143\u7684\u4f4d\u7f6e\uff08\u5373\u76f4\u5230\u8be5\u67e5\u8be2\u4f4d\u7f6e\u4e3a\u6b62\uff09\u8fdb\u884c\u6ce8\u610f\u529b\u8ba1\u7b97\u3002</p> <pre><code>class DecoderBlock(nn.Module):\n    def __init__(self, key_size, query_size, value_size, num_hiddens,\n                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n                 dropout, i, **kwargs):\n        super(DecoderBlock, self).__init__(**kwargs)\n        self.i = i\n        self.attention1 = d2l.MultiHeadAttention(\n            key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n        self.addnorm1 = AddNorm(norm_shape, dropout)\n        self.attention2 = d2l.MultiHeadAttention(\n            key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n        self.addnorm2 = AddNorm(norm_shape, dropout)\n        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,\n                                   num_hiddens)\n        self.addnorm3 = AddNorm(norm_shape, dropout)\n\n    def forward(self, X, state):\n        enc_outputs, enc_valid_lens = state[0], state[1]\n        # \u8bad\u7ec3\u9636\u6bb5\uff0c\u8f93\u51fa\u5e8f\u5217\u7684\u6240\u6709\u8bcd\u5143\u90fd\u5728\u540c\u4e00\u65f6\u95f4\u5904\u7406\uff0c\n        # \u56e0\u6b64state[2][self.i]\u521d\u59cb\u5316\u4e3aNone\u3002\n        # \u9884\u6d4b\u9636\u6bb5\uff0c\u8f93\u51fa\u5e8f\u5217\u662f\u901a\u8fc7\u8bcd\u5143\u4e00\u4e2a\u63a5\u7740\u4e00\u4e2a\u89e3\u7801\u7684\uff0c\n        # \u56e0\u6b64state[2][self.i]\u5305\u542b\u7740\u76f4\u5230\u5f53\u524d\u65f6\u95f4\u6b65\u7b2ci\u4e2a\u5757\u89e3\u7801\u7684\u8f93\u51fa\u8868\u793a\n        if state[2][self.i] is None:\n            key_values = X\n        else:\n            key_values = torch.cat((state[2][self.i], X), axis=1)\n        state[2][self.i] = key_values\n        if self.training:\n            batch_size, num_steps, _ = X.shape\n            # dec_valid_lens\u7684\u5f00\u5934:(batch_size,num_steps),\n            # \u5176\u4e2d\u6bcf\u4e00\u884c\u662f[1,2,...,num_steps]\n            dec_valid_lens = torch.arange(\n                1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n        else:\n            dec_valid_lens = None\n\n        # \u81ea\u6ce8\u610f\u529b\n        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n        Y = self.addnorm1(X, X2)\n        # \u7f16\u7801\u5668\uff0d\u89e3\u7801\u5668\u6ce8\u610f\u529b\u3002\n        # enc_outputs\u7684\u5f00\u5934:(batch_size,num_steps,num_hiddens)\n        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n        Z = self.addnorm2(Y, Y2)\n        return self.addnorm3(Z, self.ffn(Z)), state\n</code></pre> <pre><code>class TransformerDecoder(d2l.AttentionDecoder):\n    def __init__(self, vocab_size, key_size, query_size, value_size,\n                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n                 num_heads, num_layers, dropout, **kwargs):\n        super(TransformerDecoder, self).__init__(**kwargs)\n        self.num_hiddens = num_hiddens\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n        self.blks = nn.Sequential()\n        for i in range(num_layers):\n            self.blks.add_module(\"block\"+str(i),\n                DecoderBlock(key_size, query_size, value_size, num_hiddens,\n                             norm_shape, ffn_num_input, ffn_num_hiddens,\n                             num_heads, dropout, i))\n        self.dense = nn.Linear(num_hiddens, vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_lens, *args):\n        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n\n    def forward(self, X, state):\n        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n        for i, blk in enumerate(self.blks):\n            X, state = blk(X, state)\n            # \u89e3\u7801\u5668\u81ea\u6ce8\u610f\u529b\u6743\u91cd\n            self._attention_weights[0][\n                i] = blk.attention1.attention.attention_weights\n            # \u201c\u7f16\u7801\u5668\uff0d\u89e3\u7801\u5668\u201d\u81ea\u6ce8\u610f\u529b\u6743\u91cd\n            self._attention_weights[1][\n                i] = blk.attention2.attention.attention_weights\n        return self.dense(X), state\n\n    @property\n    def attention_weights(self):\n        return self._attention_weights\n</code></pre>"},{"location":"DeepLearning/BERT/","title":"BERT","text":""},{"location":"DeepLearning/BERT/#_1","title":"\u8bba\u6587\u9605\u8bfb","text":"<p>Bidirectional Encoder Representations from Transformers</p> <p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p> <p>\u5b98\u65b9\u4ee3\u7801\u5e93</p> <p>\u8bba\u6587\u53d1\u5e03\u65f6\u95f4\uff082018\uff09\u5728GPT\u548cGPT2\u53d1\u5e03\u4e4b\u95f4</p> <p>\u505aNLP\u4e24\u79cd\u7b56\u7565</p> <p>feature-base:ELMo \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff0c\u6784\u9020\u4e00\u4e2a\u8ddf\u4efb\u52a1\u76f8\u5173\u7684\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\uff0c \u9884\u8bad\u7ec3\u597d\u7684\u8868\u793a\u4f5c\u4e3a\u989d\u5916\u7684\u7279\u5f81\u548c\u8f93\u5165\u4e00\u8d77\u8fdb\u5165\u6a21\u578b\uff0c\u4f7f\u5f97\u6a21\u578b\u8bad\u7ec3\u8d77\u6765\u4f1a\u8f83\u4e3a\u5bb9\u6613</p> <p>fine-tuning:GPT</p> <p>\u8fd9\u4e24\u79cd\u65b9\u6cd5\u4f7f\u7528\u76f8\u540c\u7684\u76ee\u6807\u51fd\u6570\uff0c\u4f7f\u7528\u5355\u5411\u7684\u8bed\u8a00\u6a21\u578b\u6765\u5b66\u4e60</p> <p>\u4f5c\u8005\u8ba4\u4e3a\u5355\u5411\u8bed\u8a00\u6a21\u578b\u6709\u6240\u9650\u5236\uff0c\u63d0\u51fa\u4ece\u4e24\u4e2a\u65b9\u5411\u6765\u7406\u89e3\u4e0b\u4e0a\u6587\uff0c\u4f7f\u7528masked language model\uff0c \u6bcf\u6b21\u968f\u673a\u9009\u62e9\u4e00\u4e9b\u8bcd\u5143\u63a9\u853d\u4f4f\uff0c\u9884\u6d4b\u63a9\u853d\u7684\u8bcd\u5143\uff08\u5b8c\u5f62\u586b\u7a7a\uff09\uff1b\u8fd8\u4f7f\u7528\u4e00\u4e2anext sentence prediction\u4efb\u52a1</p> <p>\u6bcf\u4e2a\u4e0b\u6e38\u4efb\u52a1\u90fd\u4f1a\u7528\u9884\u8bad\u7ec3\u597d\u7684BERT\u6743\u91cd\uff0c\u5bf9\u6240\u7528\u53c2\u6570\u8fdb\u884c\u5fae\u8c03\uff0c\u867d\u7136\u5728CV\u9886\u57df\u4e2d\u5341\u5206\u5e38\u89c1\uff0c\u4f46\u5728NLP\u9886\u57df\u4e2d\u4ee5\u5f80\u5e76\u672a\u53d6\u5f97\u5f88\u597d\u7684\u6548\u679c</p> <p>\u76f4\u63a5\u628aTransformer Encoder\u62ff\u6765\u7528\u4e86\uff0c\u8bad\u7ec3\u4e86\u4e24\u4e2a\u6a21\u578bBERT-BASE\uff08Transformer Encoder\u4e2a\u6570L=12\uff0cnum_hiddens H=768\uff0c\u591a\u5934\u6ce8\u610f\u529b\u5934\u7684\u4e2a\u6570A=12\uff0c\u603b\u53c2\u6570\u4e2a\u6570110M\uff09 \u548cBERT-LARGE\uff08L=24\uff0cH=1024\uff0cA=16\uff0c\u53c2\u6570340M\uff09</p> <p>BERT\u8f93\u5165\u53ef\u4ee5\u662f\u4e00\u4e2a\u53e5\u5b50\uff08\u8fde\u7eed\u6587\u672c\uff09\u548c\u4e24\u4e2a\u53e5\u5b50\uff0c\u5e76\u6210\u4e00\u4e2a\u5e8f\u5217</p> <p>\u4f7f\u7528WordPiece embeddings\uff08\u5982\u679c\u4e00\u4e2a\u8bcd\u51fa\u73b0\u6982\u7387\u4e0d\u5927\uff0c\u5207\u5f00\u770b\u8bcd\u7684\u5b50\u5e8f\u5217\uff09\uff0c30000 token vocabulary</p> <p>\u8f93\u5165Embedding\uff0c\u6bcf\u4e2a\u8f93\u5165\u5e8f\u5217\u6709\u4e2a\\&lt;CLS&gt;\uff0c\u53e5\u5b50\u5206\u9694\\&lt;SEP&gt;\uff0c\u628a\u4e09\u4e2aEmbedding\u76f8\u52a0\u4f5c\u4e3ainput </p> <p>Mask language model\u4e2d\u7531wordPiece embedding\u751f\u6210\u7684token\u670915%\u7684\u6982\u7387\u53d8\u4e3a[MASK] \u7531\u4e8e\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d[MASK] token\u4e0d\u4f1a\u51fa\u73b0\uff0c\u56e0\u6b64\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u8fd915%\u7684tokens\u6709 (1)80%\u53d8\u4e3a[MASK] (2)10%\u53d8\u6210\u4e00\u4e2a\u968f\u673atoken (3)10%\u4e0d\u53d8 \uff08\u8fd9\u91cc\u7684\u53c2\u6570\u90fd\u662f\u8bd5\u51fa\u6765\u6548\u679c\u8f83\u597d\uff09</p>"},{"location":"DeepLearning/BERT/#_2","title":"\u6a21\u578b\u4ee3\u7801","text":"<p>\u53c2\u8003\u674e\u6c90\u7684d2l\u5bf9\u5e94\u7ae0\u8282</p>"},{"location":"DeepLearning/BERT/#_3","title":"\u8f93\u5165\u8868\u793a","text":"<p>\u628a\u4e24\u4e2a\u53e5\u5b50\u53d8\u6210BERT\u7684\u8f93\u5165</p> <pre><code>def get_tokens_and_segments(tokens_a, tokens_b=None):\n    \"\"\"\u83b7\u53d6\u8f93\u5165\u5e8f\u5217\u7684\u8bcd\u5143\u53ca\u5176\u7247\u6bb5\u7d22\u5f15\"\"\"\n    tokens = ['&lt;cls&gt;'] + tokens_a + ['&lt;sep&gt;']\n    # 0\u548c1\u5206\u522b\u6807\u8bb0\u7247\u6bb5A\u548cB\n    segments = [0] * (len(tokens_a) + 2)\n    if tokens_b is not None:\n        tokens += tokens_b + ['&lt;sep&gt;']\n        segments += [1] * (len(tokens_b) + 1)\n    return tokens, segments\n</code></pre> <p>\u4e0eTransformer\u76f8\u6bd4\uff0cBERT Encoder\u4f7f\u7528\u4e86segment embedding \u4ee5\u53ca\u53ef\u5b66\u4e60\u7684position embedding(\u968f\u673a\u521d\u59cb\u5316)\u3002</p> <pre><code>class BERTEncoder(nn.Module):\n    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n                 ffn_num_hiddens, num_heads, num_layers, dropout,\n                 max_len=1000, key_size=768, query_size=768, value_size=768,\n                 **kwargs):\n        super(BERTEncoder, self).__init__(**kwargs)\n        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.segment_embedding = nn.Embedding(2, num_hiddens)\n        self.blks = nn.Sequential()\n        for i in range(num_layers):\n            self.blks.add_module(f\"{i}\", d2l.EncoderBlock(\n                key_size, query_size, value_size, num_hiddens, norm_shape,\n                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n        # \u5728BERT\u4e2d\uff0c\u4f4d\u7f6e\u5d4c\u5165\u662f\u53ef\u5b66\u4e60\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u8db3\u591f\u957f\u7684\u4f4d\u7f6e\u5d4c\u5165\u53c2\u6570\n        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n                                                      num_hiddens))\n\n    def forward(self, tokens, segments, valid_lens):\n        # \u5728\u4ee5\u4e0b\u4ee3\u7801\u6bb5\u4e2d\uff0cX\u7684\u5f62\u72b6\u4fdd\u6301\u4e0d\u53d8\uff1a\uff08\u6279\u91cf\u5927\u5c0f\uff0c\u6700\u5927\u5e8f\u5217\u957f\u5ea6\uff0cnum_hiddens\uff09\n        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n        for blk in self.blks:\n            X = blk(X, valid_lens)\n        return X\n</code></pre> <p>\u5b9e\u4f8b\u5316encoder</p> <pre><code>vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\nnorm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\nencoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,\n                      ffn_num_hiddens, num_heads, num_layers, dropout)\ntokens = torch.randint(0, vocab_size, (2, 8))\nsegments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\nencoded_X = encoder(tokens, segments, None)\nencoded_X.shape\n</code></pre> <pre><code>torch.Size([2, 8, 768])\n</code></pre> <p>\u8f93\u51fa\u5c31\u662f(batch_size,\u53e5\u5b50\u957f\u5ea6,num_hiddens)</p>"},{"location":"DeepLearning/BERT/#masked-language-modeling","title":"Masked Language Modeling","text":"<p>BERT\u968f\u673a\u63a9\u853d\u8bcd\u5143\u5e76\u4f7f\u7528\u4e0a\u4e0b\u6587\u7684\u8bcd\u5143\u7528\u81ea\u76d1\u7763\u7684\u65b9\u5f0f\u6765\u9884\u6d4b\u63a9\u853d\u7684\u8bcd\u5143</p> <pre><code>class MaskLM(nn.Module):\n    \"\"\"BERT\u7684\u63a9\u853d\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\"\"\"\n    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n        super(MaskLM, self).__init__(**kwargs)\n        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n                                 nn.ReLU(),\n                                 nn.LayerNorm(num_hiddens),\n                                 nn.Linear(num_hiddens, vocab_size))\n\n    def forward(self, X, pred_positions):\n        num_pred_positions = pred_positions.shape[1]\n        pred_positions = pred_positions.reshape(-1)\n        batch_size = X.shape[0]\n        batch_idx = torch.arange(0, batch_size)\n        # \u5047\u8bbebatch_size=2\uff0cnum_pred_positions=3\n        # \u90a3\u4e48batch_idx\u662fnp.array\uff08[0,0,0,1,1,1]\uff09\n        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n        masked_X = X[batch_idx, pred_positions]\n        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n        mlm_Y_hat = self.mlp(masked_X)\n        return mlm_Y_hat\n</code></pre> <p>\u5b9e\u4f8b\u5316</p> <pre><code>mlm = MaskLM(vocab_size, num_hiddens)\nmlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n#batch_size\u4e3a2\uff0c\u7b2c\u4e00\u4e2abatch\u5e0c\u671b\u9884\u6d4b\u7b2c1\uff0c5\uff0c2\u4e2a\u8bcd\u5143\nmlm_Y_hat = mlm(encoded_X, mlm_positions)\nmlm_Y_hat.shape\n</code></pre> <pre><code>torch.Size([2, 3, 10000])\n</code></pre> <p>\u8f93\u51fa\u7684\u5f62\u72b6\u7684\u5c31\u662f(batch_size,\u9884\u6d4b\u7684mask\u8bcd\u5143\u4e2a\u6570,vocab_size)</p> <p>\u7b97loss</p> <pre><code>mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\nloss = nn.CrossEntropyLoss(reduction='none')\nmlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n</code></pre> <p>mlm_l\u7684size\u4e3a[6]</p>"},{"location":"DeepLearning/BERT/#next-sentence-prediction","title":"Next Sentence Prediction","text":"<p>\u7528\u5355\u9690\u85cf\u5c42\u7684MLP\u6765\u9884\u6d4b\u7b2c\u4e8c\u4e2a\u53e5\u5b50\u662f\u5426\u662fBERT\u8f93\u5165\u5e8f\u5217\u4e2d\u7b2c\u4e00\u4e2a\u53e5\u5b50\u7684\u4e0b\u4e00\u4e2a\u53e5\u5b50\u3002 MLP\u7684\u8f93\u5165\u662f\u7f16\u7801\u540e\u7684 \\&lt;cls&gt; token\u3002</p> <pre><code>class NextSentencePred(nn.Module):\n    def __init__(self, num_inputs, **kwargs):\n        super(NextSentencePred, self).__init__(**kwargs)\n        self.output = nn.Linear(num_inputs, 2)\n\n    def forward(self, X):\n        # X\u7684\u5f62\u72b6\uff1a(batchsize,num_hiddens)\n        return self.output(X)\n</code></pre>"},{"location":"DeepLearning/BERT/#bert-model","title":"BERT Model","text":"<p>\u6574\u5408\u4ee5\u4e0a\u4ee3\u7801</p> <pre><code>class BERTModel(nn.Module):\n    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n                 ffn_num_hiddens, num_heads, num_layers, dropout,\n                 max_len=1000, key_size=768, query_size=768, value_size=768,\n                 hid_in_features=768, mlm_in_features=768,\n                 nsp_in_features=768):\n        super(BERTModel, self).__init__()\n        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,\n                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n                    dropout, max_len=max_len, key_size=key_size,\n                    query_size=query_size, value_size=value_size)\n        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n                                    nn.Tanh())\n        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n        self.nsp = NextSentencePred(nsp_in_features)\n\n    def forward(self, tokens, segments, valid_lens=None,\n                pred_positions=None):\n        encoded_X = self.encoder(tokens, segments, valid_lens)\n        if pred_positions is not None:\n            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n        else:\n            mlm_Y_hat = None\n        # \u7528\u4e8e\u4e0b\u4e00\u53e5\u9884\u6d4b\u7684\u591a\u5c42\u611f\u77e5\u673a\u5206\u7c7b\u5668\u7684\u9690\u85cf\u5c42\uff0c0\u662f\u201c&lt;cls&gt;\u201d\u6807\u8bb0\u7684\u7d22\u5f15\n        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n        return encoded_X, mlm_Y_hat, nsp_Y_hat\n</code></pre>"},{"location":"DeepLearning/CNN/","title":"CNN","text":""},{"location":"DeepLearning/CNN/#_1","title":"\u5377\u79ef\u5c42","text":"<p>\u5377\u79ef\u64cd\u4f5c\u5b9e\u73b0</p> <pre><code>def corr2d(X, K): \n    h, w = K.shape\n    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n    return Y\n</code></pre> <p>\u5377\u79ef\u5c42</p> <pre><code>class Conv2D(nn.Module):\n    def __init__(self, kernel_size):\n        super().__init__()\n        self.weight = nn.Parameter(torch.rand(kernel_size))\n        self.bias = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        return corr2d(x, self.weight) + self.bias\n</code></pre> <p>backprop\u8bad\u7ec3\u5377\u79ef\u6838</p> <p>nn.Conv2d()\u8f93\u5165\u548c\u8f93\u51fa:\u6279\u91cf\u5927\u5c0f\u3001\u901a\u9053\u3001\u9ad8\u5ea6\u3001\u5bbd\u5ea6</p>"},{"location":"DeepLearning/CNN/#_2","title":"\u591a\u8f93\u5165\u8f93\u51fa\u901a\u9053","text":"<p>\u591a\u8f93\u5165\u901a\u9053:</p> <pre><code>def corr2d_multi_in(X, K):\n    # \u5148\u904d\u5386\u201cX\u201d\u548c\u201cK\u201d\u7684\u7b2c0\u4e2a\u7ef4\u5ea6\uff08\u901a\u9053\u7ef4\u5ea6\uff09\uff0c\u518d\u628a\u5b83\u4eec\u52a0\u5728\u4e00\u8d77\n    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))\n</code></pre> <p>\u591a\u8f93\u51fa\u901a\u9053:</p> <pre><code>def corr2d_multi_in_out(X, K):\n    # \u8fed\u4ee3\u201cK\u201d\u7684\u7b2c0\u4e2a\u7ef4\u5ea6\uff0c\u6bcf\u6b21\u90fd\u5bf9\u8f93\u5165\u201cX\u201d\u6267\u884c\u4e92\u76f8\u5173\u8fd0\u7b97\u3002\n    # \u6700\u540e\u5c06\u6240\u6709\u7ed3\u679c\u90fd\u53e0\u52a0\u5728\u4e00\u8d77\n    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)\n</code></pre> <p>\u8f93\u5165\\(\\mathbf{X} : c_i \\times n_h \\times n_w\\) \u6838$\\mathbf{{W}} : c_0 \\times c_i \\times k_h \\times k_w $ \u8f93\u51fa\\(\\mathbf{Y} : c_o \\times m_h \\times m_w\\) </p>"},{"location":"DeepLearning/CNN/#pooling","title":"Pooling","text":"<pre><code>nn.MaxPool2d(3, padding=1, stride=2)\npool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n</code></pre> <p>\u591a\u901a\u9053\u5bf9\u6bcf\u4e2a\u901a\u9053\u8fdb\u884cpooling\uff0c\u8f93\u51fa\u901a\u9053\u6570\u4e0e\u8f93\u5165\u76f8\u540c</p> <p>\u8fc7pooling\u5c42\u53ea\u6709\u8ba1\u7b97\u7528\u5230\u7684\u6709\u68af\u5ea6\uff0cmaxpool\u53ea\u6709\u6700\u5927\u7684\u6709\uff0cavgpool\u90fd\u6709\u68af\u5ea6\u5747\u5206</p>"},{"location":"DeepLearning/CNN/#lenet","title":"LeNet","text":"<pre><code>net = nn.Sequential(\n    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Flatten(),\n    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n    nn.Linear(120, 84), nn.Sigmoid(),\n    nn.Linear(84, 10))\n</code></pre> <pre><code>Conv2d output shape: torch.Size([1, 6, 28, 28])\nSigmoid output shape: torch.Size([1, 6, 28, 28])\nAvgPool2d output shape: torch.Size([1, 6, 14, 14])\nConv2d output shape: torch.Size([1, 16, 10, 10])\nSigmoid output shape: torch.Size([1, 16, 10, 10])\nAvgPool2d output shape: torch.Size([1, 16, 5, 5])\nFlatten output shape: torch.Size([1, 400])\nLinear output shape: torch.Size([1, 120])\nSigmoid output shape: torch.Size([1, 120])\nLinear output shape: torch.Size([1, 84])\nSigmoid output shape: torch.Size([1, 84])\nLinear output shape: torch.Size([1, 10])\n</code></pre>"},{"location":"DeepLearning/CNN/#alexnet","title":"AlexNet","text":"<p>\u4f7f\u7528ReLU\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0cDropout\uff0c\u6570\u636e\u96c6\u9884\u5904\u7406\uff08\u88c1\u5207\u3001\u7ffb\u8f6c\u3001\u53d8\u8272\uff09</p> <pre><code>net = nn.Sequential(\n    # \u8fd9\u91cc\u4f7f\u7528\u4e00\u4e2a11*11\u7684\u66f4\u5927\u7a97\u53e3\u6765\u6355\u6349\u5bf9\u8c61\u3002\n    # \u540c\u65f6\uff0c\u6b65\u5e45\u4e3a4\uff0c\u4ee5\u51cf\u5c11\u8f93\u51fa\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002\n    # \u53e6\u5916\uff0c\u8f93\u51fa\u901a\u9053\u7684\u6570\u76ee\u8fdc\u5927\u4e8eLeNet\n    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    # \u51cf\u5c0f\u5377\u79ef\u7a97\u53e3\uff0c\u4f7f\u7528\u586b\u5145\u4e3a2\u6765\u4f7f\u5f97\u8f93\u5165\u4e0e\u8f93\u51fa\u7684\u9ad8\u548c\u5bbd\u4e00\u81f4\uff0c\u4e14\u589e\u5927\u8f93\u51fa\u901a\u9053\u6570\n    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    # \u4f7f\u7528\u4e09\u4e2a\u8fde\u7eed\u7684\u5377\u79ef\u5c42\u548c\u8f83\u5c0f\u7684\u5377\u79ef\u7a97\u53e3\u3002\n    # \u9664\u4e86\u6700\u540e\u7684\u5377\u79ef\u5c42\uff0c\u8f93\u51fa\u901a\u9053\u7684\u6570\u91cf\u8fdb\u4e00\u6b65\u589e\u52a0\u3002\n    # \u5728\u524d\u4e24\u4e2a\u5377\u79ef\u5c42\u4e4b\u540e\uff0c\u6c47\u805a\u5c42\u4e0d\u7528\u4e8e\u51cf\u5c11\u8f93\u5165\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\n    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    nn.Flatten(),\n    # \u8fd9\u91cc\uff0c\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\u6570\u91cf\u662fLeNet\u4e2d\u7684\u597d\u51e0\u500d\u3002\u4f7f\u7528dropout\u5c42\u6765\u51cf\u8f7b\u8fc7\u62df\u5408\n    nn.Linear(6400, 4096), nn.ReLU(),\n    nn.Dropout(p=0.5),\n    nn.Linear(4096, 4096), nn.ReLU(),\n    nn.Dropout(p=0.5),\n    # \u6700\u540e\u662f\u8f93\u51fa\u5c42\u3002\u7531\u4e8e\u8fd9\u91cc\u4f7f\u7528Fashion-MNIST\uff0c\u6240\u4ee5\u7528\u7c7b\u522b\u6570\u4e3a10\uff0c\u800c\u975e\u8bba\u6587\u4e2d\u76841000\n    nn.Linear(4096, 10))\n</code></pre> <pre><code>Conv2d output shape:torch.Size([1, 96, 54, 54])\nReLU output shape:torch.Size([1, 96, 54, 54])\nMaxPool2d output shape:torch.Size([1, 96, 26, 26])\nConv2d output shape:torch.Size([1, 256, 26, 26])\nReLU output shape:torch.Size([1, 256, 26, 26])\nMaxPool2d output shape:torch.Size([1, 256, 12, 12])\nConv2d output shape:torch.Size([1, 384, 12, 12])\nReLU output shape:torch.Size([1, 384, 12, 12])\nConv2d output shape:torch.Size([1, 384, 12, 12])\nReLU output shape:torch.Size([1, 384, 12, 12])\nConv2d output shape:torch.Size([1, 256, 12, 12])\nReLU output shape:torch.Size([1, 256, 12, 12])\nMaxPool2d output shape:torch.Size([1, 256, 5, 5])\nFlatten output shape:torch.Size([1, 6400])\nLinear output shape:torch.Size([1, 4096])\nReLU output shape:torch.Size([1, 4096])\nDropout output shape:torch.Size([1, 4096])\nLinear output shape:torch.Size([1, 4096])\nReLU output shape:torch.Size([1, 4096])\nDropout output shape:torch.Size([1, 4096])\nLinear output shape:torch.Size([1, 10])\n</code></pre>"},{"location":"DeepLearning/CNN/#vgg","title":"VGG","text":"<p>VGG\u5757 kernel\u5927\u5c0f\u90fd\u4e3a3 \\(\\times\\) 3</p> <pre><code>def vgg_block(num_convs, in_channels, out_channels):\n    layers = []\n    for _ in range(num_convs):\n        layers.append(nn.Conv2d(in_channels, out_channels,\n                                kernel_size=3, padding=1))\n        layers.append(nn.ReLU())\n        in_channels = out_channels\n    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n    return nn.Sequential(*layers)\n</code></pre> <p>VGG\u7f51\u7edc VGG-11</p> <pre><code>conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\ndef vgg(conv_arch):\n    conv_blks = []\n    in_channels = 1\n    # \u5377\u79ef\u5c42\u90e8\u5206\n    for (num_convs, out_channels) in conv_arch:\n        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n        in_channels = out_channels\n\n    return nn.Sequential(\n        *conv_blks, nn.Flatten(),\n        # \u5168\u8fde\u63a5\u5c42\u90e8\u5206\n        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n        nn.Linear(4096, 10))\n\nnet = vgg(conv_arch)\n</code></pre>"},{"location":"DeepLearning/CNN/#nin","title":"NIN","text":"<p>\u5728\u6bcf\u4e2a\u50cf\u7d20\u7684\u901a\u9053\u4e0a\u5206\u522b\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a</p> <pre><code>def nin_block(in_channels, out_channels, kernel_size, strides, padding):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\n        nn.ReLU(),\n        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())\n</code></pre> <pre><code>net = nn.Sequential(\n    nin_block(1, 96, kernel_size=11, strides=4, padding=0),\n    nn.MaxPool2d(3, stride=2),\n    nin_block(96, 256, kernel_size=5, strides=1, padding=2),\n    nn.MaxPool2d(3, stride=2),\n    nin_block(256, 384, kernel_size=3, strides=1, padding=1),\n    nn.MaxPool2d(3, stride=2),\n    nn.Dropout(0.5),\n    # \u6807\u7b7e\u7c7b\u522b\u6570\u662f10\n    nin_block(384, 10, kernel_size=3, strides=1, padding=1),\n    nn.AdaptiveAvgPool2d((1, 1)),\n    # \u5c06\u56db\u7ef4\u7684\u8f93\u51fa\u8f6c\u6210\u4e8c\u7ef4\u7684\u8f93\u51fa\uff0c\u5176\u5f62\u72b6\u4e3a(\u6279\u91cf\u5927\u5c0f,10)\n    nn.Flatten())\n</code></pre> <p><code>nn.AdaptiveAvgPool2d()</code>\u53c2\u6570\u4e3a\u6307\u5b9a\u8f93\u51fasize</p>"},{"location":"DeepLearning/CNN/#googlenet","title":"GoogLeNet","text":"<pre><code>class Inception(nn.Module):\n    # c1--c4\u662f\u6bcf\u6761\u8def\u5f84\u7684\u8f93\u51fa\u901a\u9053\u6570\n    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n        super(Inception, self).__init__(**kwargs)\n        # \u7ebf\u8def1\uff0c\u53551x1\u5377\u79ef\u5c42\n        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n        # \u7ebf\u8def2\uff0c1x1\u5377\u79ef\u5c42\u540e\u63a53x3\u5377\u79ef\u5c42\n        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n        # \u7ebf\u8def3\uff0c1x1\u5377\u79ef\u5c42\u540e\u63a55x5\u5377\u79ef\u5c42\n        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n        # \u7ebf\u8def4\uff0c3x3\u6700\u5927\u6c47\u805a\u5c42\u540e\u63a51x1\u5377\u79ef\u5c42\n        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n\n    def forward(self, x):\n        p1 = F.relu(self.p1_1(x))\n        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n        p4 = F.relu(self.p4_2(self.p4_1(x)))\n        # \u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u8fde\u7ed3\u8f93\u51fa\n        return torch.cat((p1, p2, p3, p4), dim=1)\n</code></pre> <p>\u8d85\u53c2\u6570\u4e3b\u8981\u4e3a\u901a\u9053\u6570,\u6bd4\u5982 <code>Inception(192, 64, (96, 128), (16, 32), 32)</code> \u8868\u793a\u4e00\u4e2aInception\u7684\u8f93\u5165\u901a\u9053\u6570\u4e3a192\uff0c\u8f93\u51fa\u901a\u9053\u6570\u4e3a 64 + 128 + 32 + 32 = 512</p> <p>\u7f51\u7edc\u6574\u4f53\u7ed3\u6784\u7565</p>"},{"location":"DeepLearning/CNN/#batch-norm","title":"Batch-Norm","text":"\\[\\mathrm{BN}(\\mathbf{x}) = \\mathbf{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\mathbf{\\mu}}_\\mathcal{B}}{\\hat{\\mathbf{\\sigma}}_\\mathcal{B}} + \\mathbf{\\beta}.\\] <p>\u5f52\u4e00\u5316\u540e\u7684\u65b9\u5dee\u4e3a\\(\\gamma\\)\uff0c\u5747\u503c\u4e3a\\(1 + \\beta\\)</p> <p>\\(\\gamma,\\beta\\) \u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570</p> <p>\u5377\u79ef\u5c42\u6709\u591a\u4e2a\u901a\u9053\u65f6\uff0c\u5bf9\u6bcf\u4e2a\u901a\u9053\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\uff0c\u5047\u8bbebatch_size\u4e3am\uff0c\u5377\u79ef\u5c42\u8f93\u51fa\u7684\u5927\u5c0f\u4e3ap\\(\\times\\)q,\u5728\u6bcf\u4e2a\u8f93\u51fa\u901a\u9053\u4e0a\u7684mqp\u4e2a\u5143\u7d20\u4e0a\u6c42\u5747\u503c\u548c\u65b9\u5dee</p> <pre><code>def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n    '''\n    moving_mean\u548cmoving_var\u8fd1\u4f3c\u8ba4\u4e3a\u5168\u5c40\u503c\n    eps\u9632\u6b62\u65b9\u5dee\u4e3a0\uff0c\u56fa\u5b9a\u503c\n    gmma,beta,moving_mean,moving_var\u7684\u5f62\u72b6\u548cX\u76f8\u540c\n    '''\n    # \u901a\u8fc7is_grad_enabled\u6765\u5224\u65ad\u5f53\u524d\u6a21\u5f0f\u662f\u8bad\u7ec3\u6a21\u5f0f\u8fd8\u662f\u9884\u6d4b\u6a21\u5f0f\n    if not torch.is_grad_enabled():\n        # \u5982\u679c\u662f\u5728\u9884\u6d4b\u6a21\u5f0f\u4e0b\uff0c\u76f4\u63a5\u4f7f\u7528\u4f20\u5165\u7684\u79fb\u52a8\u5e73\u5747\u6240\u5f97\u7684\u5747\u503c\u548c\u65b9\u5dee\n        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n    else:\n        assert len(X.shape) in (2, 4)\n        if len(X.shape) == 2:\n            # \u4f7f\u7528\u5168\u8fde\u63a5\u5c42\u7684\u60c5\u51b5\uff0c\u8ba1\u7b97\u7279\u5f81\u7ef4\u4e0a\u7684\u5747\u503c\u548c\u65b9\u5dee\n            mean = X.mean(dim=0)\n            var = ((X - mean) ** 2).mean(dim=0)\n        else:\n            # \u4f7f\u7528\u4e8c\u7ef4\u5377\u79ef\u5c42\u7684\u60c5\u51b5\uff0c\u8ba1\u7b97\u901a\u9053\u7ef4\u4e0a\uff08axis=1\uff09\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002\n            # \u8fd9\u91cc\u6211\u4eec\u9700\u8981\u4fdd\u6301X\u7684\u5f62\u72b6\u4ee5\u4fbf\u540e\u9762\u53ef\u4ee5\u505a\u5e7f\u64ad\u8fd0\u7b97\n            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n        # \u8bad\u7ec3\u6a21\u5f0f\u4e0b\uff0c\u7528\u5f53\u524d\u7684\u5747\u503c\u548c\u65b9\u5dee\u505a\u6807\u51c6\u5316\n        X_hat = (X - mean) / torch.sqrt(var + eps)\n        # \u66f4\u65b0\u79fb\u52a8\u5e73\u5747\u7684\u5747\u503c\u548c\u65b9\u5dee\n        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n        moving_var = momentum * moving_var + (1.0 - momentum) * var\n    Y = gamma * X_hat + beta  # \u7f29\u653e\u548c\u79fb\u4f4d\n    return Y, moving_mean.data, moving_var.data\n</code></pre> <pre><code>class BatchNorm(nn.Module):\n    # num_features\uff1a\u5b8c\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\u6570\u91cf\u6216\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\u3002\n    # num_dims\uff1a2\u8868\u793a\u5b8c\u5168\u8fde\u63a5\u5c42\uff0c4\u8868\u793a\u5377\u79ef\u5c42\n    def __init__(self, num_features, num_dims):\n        super().__init__()\n        if num_dims == 2:\n            shape = (1, num_features)\n        else:\n            shape = (1, num_features, 1, 1)\n        # \u53c2\u4e0e\u6c42\u68af\u5ea6\u548c\u8fed\u4ee3\u7684\u62c9\u4f38\u548c\u504f\u79fb\u53c2\u6570\uff0c\u5206\u522b\u521d\u59cb\u5316\u62101\u548c0\n        self.gamma = nn.Parameter(torch.ones(shape))\n        self.beta = nn.Parameter(torch.zeros(shape))\n        # \u975e\u6a21\u578b\u53c2\u6570\u7684\u53d8\u91cf\u521d\u59cb\u5316\u4e3a0\u548c1\n        self.moving_mean = torch.zeros(shape)\n        self.moving_var = torch.ones(shape)\n\n    def forward(self, X):\n        # \u5982\u679cX\u4e0d\u5728\u5185\u5b58\u4e0a\uff0c\u5c06moving_mean\u548cmoving_var\n        # \u590d\u5236\u5230X\u6240\u5728\u663e\u5b58\u4e0a\n        if self.moving_mean.device != X.device:\n            self.moving_mean = self.moving_mean.to(X.device)\n            self.moving_var = self.moving_var.to(X.device)\n        # \u4fdd\u5b58\u66f4\u65b0\u8fc7\u7684moving_mean\u548cmoving_var\n        Y, self.moving_mean, self.moving_var = batch_norm(\n            X, self.gamma, self.beta, self.moving_mean,\n            self.moving_var, eps=1e-5, momentum=0.9)\n        return Y\n</code></pre> <p>pytorch\u4f7f\u7528batch_norm:<code>nn.BatchNorm1d(),nn.BatchNorm2d()</code>\uff0c\u53c2\u6570\u4e3a\u901a\u9053\u6570\uff0c\u5206\u522b\u8868\u793a\u5168\u8fde\u63a5\u5c42\u548c\u5377\u79ef\u5c42</p>"},{"location":"DeepLearning/CNN/#resnet","title":"ResNet","text":"<p>\u8f93\u5165X\u548c\u8f93\u51faY\u7684\u5f62\u72b6\u8981\u76f8\u540c,3\\(\\times\\)3\u7684\u5377\u79ef\u6838\u6ca1\u6709\u6539\u53d8\u5f62\u72b6\uff0c\u4e3b\u8981\u662f\u901a\u9053\u6570\uff0c\u4e5f\u53ef\u4ee5\u75281\\(\\times\\)1\u7684\u5377\u79ef\u6838\u4fee\u6539X\u7684\u901a\u9053\u6570</p> <pre><code>class Residual(nn.Module): \n    def __init__(self, input_channels, num_channels,\n                 use_1x1conv=False, strides=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(input_channels, num_channels,\n                               kernel_size=3, padding=1, stride=strides)\n        self.conv2 = nn.Conv2d(num_channels, num_channels,\n                               kernel_size=3, padding=1)\n        if use_1x1conv:\n            self.conv3 = nn.Conv2d(input_channels, num_channels,\n                                   kernel_size=1, stride=strides)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n\n    def forward(self, X):\n        Y = F.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3:\n            X = self.conv3(X)\n        Y += X\n        return F.relu(Y)\n</code></pre>"},{"location":"DeepLearning/CNN/#densenet","title":"DenseNet","text":"<p>\u7f51\u7edc\u4e3b\u8981\u7531\u4e24\u90e8\u5206 \u7a20\u5bc6\u5c42\u548c\u8fc7\u6e21\u5c42</p> <p>\u628a\u524d\u9762\u7f51\u7edc\u5c42\u7684\u8f93\u51fa\u90fd\u4f5c\u4e3a\u8f93\u51fa\uff0c\u6bcf\u4e2a\u7f51\u7edc\u7684\u8f93\u5165\u4e3a\u524d\u9762\u6240\u6709\u7f51\u7edc\u5c42\u7684\u8f93\u51fa\u52a0\u4e0ainput</p> <pre><code>def conv_block(input_channels, num_channels):\n    return nn.Sequential(\n        nn.BatchNorm2d(input_channels), nn.ReLU(),\n        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_convs, input_channels, num_channels):\n        super(DenseBlock, self).__init__()\n        layer = []\n        for i in range(num_convs):\n            layer.append(conv_block(\n                num_channels * i + input_channels, num_channels))\n        self.net = nn.Sequential(*layer)\n\n    def forward(self, X):\n        for blk in self.net:\n            Y = blk(X)\n            # \u8fde\u63a5\u901a\u9053\u7ef4\u5ea6\u4e0a\u6bcf\u4e2a\u5757\u7684\u8f93\u5165\u548c\u8f93\u51fa\n            X = torch.cat((X, Y), dim=1)\n        return X\n</code></pre> <p>\u901a\u8fc7\u8fc7\u6e21\u5c42\u901a\u9053\u6570\u51cf\u5c11\uff0c\u9ad8\u548c\u5bbd\u51cf\u534a</p> <pre><code>def transition_block(input_channels, num_channels):\n    return nn.Sequential(\n        nn.BatchNorm2d(input_channels), nn.ReLU(),\n        nn.Conv2d(input_channels, num_channels, kernel_size=1),\n        nn.AvgPool2d(kernel_size=2, stride=2))\n</code></pre>"},{"location":"DeepLearning/CV/","title":"Computer Vision","text":""},{"location":"DeepLearning/CV/#image-augmentation","title":"Image Augmentation","text":"<ul> <li>\u7ffb\u8f6c\uff08\u4e0a\u4e0b\u7ffb\u8f6c\u3001\u5de6\u53f3\u7ffb\u8f6c\uff09 <code>torchvision.transforms.RandomHorizontalFlip()</code> <code>torchvision.transforms.RandomVerticalFlip()</code></li> <li>\u968f\u673a\u526a\u88c1 <code>torchvision.transforms.RandomResizedCrop((200, 200), scale=(0.1, 1), ratio=(0.5, 2))</code></li> <li>\u6539\u53d8\u989c\u8272\uff08\u4eae\u5ea6\u3001\u5bf9\u6bd4\u5ea6\u3001\u9971\u548c\u5ea6\u3001\u8272\u8c03\uff09 <code>torchvision.transforms.ColorJitter(brightness=0.5, contrast=0, saturation=0, hue=0)</code></li> </ul>"},{"location":"DeepLearning/CV/#_1","title":"\u76ee\u6807\u68c0\u6d4b","text":""},{"location":"DeepLearning/CV/#bounding-box","title":"Bounding Box","text":"<p>\u4e24\u79cd\u8868\u793a</p> <ul> <li>\u5de6\u4e0a\u89d2\u5750\u6807\u548c\u53f3\u4e0b\u89d2\u5750\u6807</li> <li>\u4e2d\u5fc3\u5750\u6807\u548cw\uff0ch</li> </ul>"},{"location":"DeepLearning/CV/#_2","title":"\u951a\u6846","text":""},{"location":"DeepLearning/CV/#_3","title":"\u751f\u6210\u951a\u6846","text":"<p>\u8f93\u5165\u56fe\u50cf\u7684\u9ad8\u5ea6\u4e3a\\(h\\)\uff0c\u5bbd\u5ea6\u4e3a\\(w\\)\u3002\u4ee5\u56fe\u50cf\u7684\u6bcf\u4e2a\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u751f\u6210\u4e0d\u540c\u5f62\u72b6\u7684\u951a\u6846\uff1a\u7f29\u653e\u6bd4\u4e3a\\(s\\in (0, 1]\\)\uff0c\u5bbd\u9ad8\u6bd4\u4e3a\\(r &gt; 0\\)\u3002 \u90a3\u4e48\u951a\u6846\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u5206\u522b\u662f\\(hs\\sqrt{r}\\)\u548c\\(hs/\\sqrt{r}\\)\u3002 \u8bf7\u6ce8\u610f\uff0c\u5f53\u4e2d\u5fc3\u4f4d\u7f6e\u7ed9\u5b9a\u65f6\uff0c\u5df2\u77e5\u5bbd\u548c\u9ad8\u7684\u951a\u6846\u662f\u786e\u5b9a\u7684\u3002</p> <p>\u7f29\u653e\u6bd4\uff08scale\uff09\u53d6\u503c\\(s_1,\\ldots, s_n\\)\u548c\u5bbd\u9ad8\u6bd4\uff08aspect ratio\uff09\u53d6\u503c\\(r_1,\\ldots, r_m\\)\u3002\u4f7f\u7528\u8fd9\u4e9b\u6bd4\u4f8b\u548c\u957f\u5bbd\u6bd4\u7684\u6240\u6709\u7ec4\u5408\u4ee5\u6bcf\u4e2a\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u65f6\uff0c\u8f93\u5165\u56fe\u50cf\u5c06\u603b\u5171\u6709\\(whnm\\)\u4e2a\u951a\u6846\u3002 \u5728\u5b9e\u8df5\u4e2d\uff0c\u8003\u8651\u5305\u542b\\(s_1\\)\u6216\\(r_1\\)\u7684\u7ec4\u5408\uff1a</p> \\[(s_1, r_1), (s_1, r_2), \\ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \\ldots, (s_n, r_1).\\] <p>\u4e5f\u5c31\u662f\u8bf4\uff0c\u4ee5\u540c\u4e00\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u7684\u951a\u6846\u7684\u6570\u91cf\u662f\\(n+m-1\\)\u3002\u5bf9\u4e8e\u6574\u4e2a\u8f93\u5165\u56fe\u50cf\uff0c\u5c06\u5171\u751f\u6210\\(wh(n+m-1)\\)\u4e2a\u951a\u6846\u3002</p>"},{"location":"DeepLearning/CV/#iou","title":"\u4ea4\u5e76\u6bd4(IoU)","text":"\\[J(\\mathcal{A},\\mathcal{B}) = \\frac{\\left|\\mathcal{A} \\cap \\mathcal{B}\\right|}{\\left| \\mathcal{A} \\cup \\mathcal{B}\\right|}.\\]"},{"location":"DeepLearning/CV/#_4","title":"\u951a\u6846\u5206\u914d","text":"<p>\u6bcf\u6b21\u53d6\u6700\u5927IoU\u7684\u951a\u6846\u548c\u771f\u5b9e\u6846\uff0c\u53bb\u6389\u4e4b\u540e\u91cd\u590d\uff0c\u6700\u540e\u6839\u636e\u9608\u503c\u786e\u5b9a\u662f\u5426\u4e3a\u951a\u6846\u5206\u914d\u771f\u5b9e\u6846</p>"},{"location":"DeepLearning/CV/#_5","title":"\u6807\u8bb0\u7c7b\u522b","text":"<p>\u7ed9\u5b9a\u6846\\(A\\)\u548c\\(B\\)\uff0c\u4e2d\u5fc3\u5750\u6807\u5206\u522b\u4e3a\\((x_a, y_a)\\)\u548c\\((x_b, y_b)\\)\uff0c\u5bbd\u5ea6\u5206\u522b\u4e3a\\(w_a\\)\u548c\\(w_b\\)\uff0c\u9ad8\u5ea6\u5206\u522b\u4e3a\\(h_a\\)\u548c\\(h_b\\)\uff0c\u53ef\u4ee5\u5c06\\(A\\)\u7684\u504f\u79fb\u91cf\u6807\u8bb0\u4e3a\uff1a</p> \\[\\left( \\frac{ \\frac{x_b - x_a}{w_a} - \\mu_x }{\\sigma_x}, \\frac{ \\frac{y_b - y_a}{h_a} - \\mu_y }{\\sigma_y}, \\frac{ \\log \\frac{w_b}{w_a} - \\mu_w }{\\sigma_w}, \\frac{ \\log \\frac{h_b}{h_a} - \\mu_h }{\\sigma_h}\\right),\\]"},{"location":"DeepLearning/CV/#_6","title":"\u975e\u6781\u5927\u503c\u6291\u5236","text":"<p>\u5728\u540c\u4e00\u5f20\u56fe\u50cf\u4e2d\uff0c\u6240\u6709\u9884\u6d4b\u7684\u975e\u80cc\u666f\u8fb9\u754c\u6846\u90fd\u6309\u7f6e\u4fe1\u5ea6\u964d\u5e8f\u6392\u5e8f\uff0c\u4ee5\u751f\u6210\u5217\u8868\\(L\\)\u3002</p> <ol> <li>\u4ece\\(L\\)\u4e2d\u9009\u53d6\u7f6e\u4fe1\u5ea6\u6700\u9ad8\u7684\u9884\u6d4b\u8fb9\u754c\u6846\\(B_1\\)\u4f5c\u4e3a\u57fa\u51c6\uff0c\u7136\u540e\u5c06\u6240\u6709\u4e0e\\(B_1\\)\u7684IoU\u8d85\u8fc7\u9884\u5b9a\u9608\u503c\\(\\epsilon\\)\u7684\u975e\u57fa\u51c6\u9884\u6d4b\u8fb9\u754c\u6846\u4ece\\(L\\)\u4e2d\u79fb\u9664\u3002</li> <li>\u4ece\\(L\\)\u4e2d\u9009\u53d6\u7f6e\u4fe1\u5ea6\u7b2c\u4e8c\u9ad8\u7684\u9884\u6d4b\u8fb9\u754c\u6846\\(B_2\\)\u4f5c\u4e3a\u53c8\u4e00\u4e2a\u57fa\u51c6\uff0c\u7136\u540e\u5c06\u6240\u6709\u4e0e\\(B_2\\)\u7684IoU\u5927\u4e8e\\(\\epsilon\\)\u7684\u975e\u57fa\u51c6\u9884\u6d4b\u8fb9\u754c\u6846\u4ece\\(L\\)\u4e2d\u79fb\u9664\u3002</li> <li>\u91cd\u590d\u4e0a\u8ff0\u8fc7\u7a0b\uff0c\u76f4\u5230\\(L\\)\u4e2d\u7684\u6240\u6709\u9884\u6d4b\u8fb9\u754c\u6846\u90fd\u66fe\u88ab\u7528\u4f5c\u57fa\u51c6\u3002\u6b64\u65f6\uff0c\\(L\\)\u4e2d\u4efb\u610f\u4e00\u5bf9\u9884\u6d4b\u8fb9\u754c\u6846\u7684IoU\u90fd\u5c0f\u4e8e\u9608\u503c\\(\\epsilon\\)\uff1b\u56e0\u6b64\uff0c\u6ca1\u6709\u4e00\u5bf9\u8fb9\u754c\u6846\u8fc7\u4e8e\u76f8\u4f3c\u3002</li> <li>\u8f93\u51fa\u5217\u8868\\(L\\)\u4e2d\u7684\u6240\u6709\u9884\u6d4b\u8fb9\u754c\u6846\u3002</li> </ol>"},{"location":"DeepLearning/CV/#r-cnn","title":"R-CNN","text":""},{"location":"DeepLearning/CV/#region-based-cnn","title":"Region-based CNN","text":"<ol> <li>\u4f7f\u7528\u542f\u53d1\u5f0f\u641c\u7d22\u9009\u62e9\u951a\u6846</li> <li>\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u6bcf\u4e2a\u951a\u6846\u62bd\u53d6\u7279\u5f81</li> <li>\u8bad\u7ec3\u4e00\u4e2aSVM\u6765\u5bf9\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b</li> <li>\u8bad\u7ec3\u4e00\u4e2a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u6765\u9884\u6d4b\u8fb9\u7f18\u6846\u504f\u79fb</li> </ol> <p>Rol Pooling \u7ed9\u5b9a\u4e00\u4e2a\u951a\u6846\uff0c\u5747\u5300\u5206\u6210\\(n \\times m\\)\u5757\uff0c\u8f93\u51fa\u6bcf\u5757\u91cc\u9762\u7684\u6700\u5927\u503c\uff0c\u4e0d\u7ba1\u951a\u6846\u5927\u5c0f\u4e3a\u591a\u5c11\uff0c\u90fd\u662fnm</p>"},{"location":"DeepLearning/CV/#fast-rcnn","title":"Fast RCNN","text":"<p>\u4f7f\u7528CNN\u5bf9\u56fe\u7247\u62bd\u53d6\u7279\u5f81\uff08\u6574\u5f20\u56fe\u7247\uff09\uff0c\u4f7f\u7528Rol Pooling\u5c42\u5bf9\u6bcf\u4e2a\u951a\u6846\u751f\u6210\u56fa\u5b9a\u957f\u5ea6\u7684\u7279\u5f81</p>"},{"location":"DeepLearning/CV/#faster-rcnn","title":"Faster RCNN","text":"<p>\u4f7f\u7528\u4e00\u4e2a\u7f51\u7edc\u6765\u66ff\u4ee3\u542f\u53d1\u5f0f\u641c\u7d22\u6765\u83b7\u5f97\u66f4\u597d\u7684\u951a\u6846</p>"},{"location":"DeepLearning/CV/#mask-rcnn","title":"Mask RCNN","text":"<p>\u5982\u679c\u6709\u50cf\u7d20\u7ea7\u522b\u7684\u6807\u53f7\uff0c\u7528FCN\u6765\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f</p>"},{"location":"DeepLearning/CV/#yolo","title":"YOLO","text":"<p>\u53ea\u770b\u4e00\u6b21 - SSD\u951a\u6846\u5927\u91cf\u91cd\u53e0\u6d6a\u8d39\u8ba1\u7b97 - YOLO\u5c06\u56fe\u7247\u5747\u5300\u5206\u6210\\(S \\times S\\) \u951a\u6846 - \u6bcf\u4e2a\u951a\u6846\u9884\u6d4b\\(B\\)\u4e2a\u8fb9\u7f18\u6846</p>"},{"location":"DeepLearning/CV/#ssd","title":"\u5355\u53d1\u591a\u6846\u68c0\u6d4b\uff08SSD\uff09","text":""},{"location":"DeepLearning/CV/#_7","title":"\u8bed\u4e49\u5206\u5272","text":"<p>\u56fe\u50cf\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272</p>"},{"location":"DeepLearning/CV/#_8","title":"\u8f6c\u7f6e\u5377\u79ef","text":"<p>\u8f6c\u7f6e\u5377\u79ef\u53ef\u4ee5\u7528\u6765\u589e\u52a0\u9ad8\u5bbd</p> <p>\\(Y[i: i + h, j: j + w] += X[i, j] * K\\)</p> <p>\u8f6c\u7f6e? \\(Y = X * W\\) \u53ef\u4ee5\u5bf9W\u6784\u9020\u4e00\u4e2aV\uff0c\u4f7f\u5f97\u5377\u79ef\u7b49\u4ef7\u4e8e\u77e9\u9635\u4e58\u6cd5\\(Y^ \\prime = V X^ \\prime\\), \u8fd9\u91cc\\(X^ \\prime Y^\\prime\\)\u662f \\(XY\\)\u5bf9\u5e94\u7684\u5411\u91cf\u7248\u672c \u8f6c\u7f6e\u5377\u79ef\u7b49\u4ef7\u4e8e $Y^\\prime = V^T X^\\prime $</p> <p>\u57fa\u672c\u64cd\u4f5c</p> <pre><code>def trans_conv(X, K):\n    h, w = K.shape\n    Y = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            Y[i: i + h, j: j + w] += X[i, j] * K\n    return Y\n</code></pre> <p>torch API</p> <pre><code>X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\ntconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)\n</code></pre> <pre><code>tensor([[[[ 0.,  0.,  1.],\n          [ 0.,  4.,  6.],\n          [ 4., 12.,  9.]]]])\n</code></pre> <p><code>tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)</code>padding\u5c06\u5220\u9664\u7b2c\u4e00\u548c\u6700\u540e\u4e00\u884c\u548c\u5217</p> <pre><code>tensor([[[[4.]]]])\n</code></pre> <p><code>tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)</code> \u6b65\u5e45\u4e3a2\u589e\u5927\u8f93\u51fa</p> <pre><code>tensor([[[[0., 0., 0., 1.],\n          [0., 0., 2., 3.],\n          [0., 2., 0., 3.],\n          [4., 6., 6., 9.]]]])\n</code></pre>"},{"location":"DeepLearning/CV/#fcn","title":"\u5168\u5377\u79ef\u7f51\u7edc(FCN)","text":"<p>\u7528\u8f6c\u7f6e\u5377\u79ef\u5c42\u6765\u66ff\u6362CNN\u6700\u540e\u7684\u5168\u8fde\u63a5\u5c42\uff0c\u4ece\u800c\u5b9e\u73b0\u6bcf\u4e2a\u50cf\u7d20\u7684\u9884\u6d4b</p> <pre><code>CNN --&gt; 1x1 Conv --&gt; \u8f6c\u7f6e\u5377\u79ef --&gt; output\n</code></pre> <p>\u5148\u7528Resnet18\u63d0\u53d6\u7279\u5f81<code>net = nn.Sequential(*list(pretrained_net.children())[:-2])</code></p> <p>\u7136\u540e\u52a01x1\u7684\u5377\u79ef\u5c42\u548c\u8f6c\u7f6e\u5377\u79ef\u5c42\uff0c\u4f7f\u5f97\u8f93\u51fa\u5927\u5c0f\u548c\u539f\u56fe\u50cf\u5927\u5c0f\u76f8\u540c</p> <pre><code>num_classes = 21\nnet.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\nnet.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,\n                                    kernel_size=64, padding=16, stride=32))\n</code></pre>"},{"location":"DeepLearning/DETR/","title":"DETR","text":""},{"location":"DeepLearning/DETR/#_1","title":"\u8bba\u6587\u9605\u8bfb","text":"<p>End-to-End Object Detection with Transformers \uff082020.5\uff09</p> <p>\u5b98\u65b9\u4ee3\u7801</p> <p>\u628a\u76ee\u6807\u68c0\u6d4b\u770b\u6210\u96c6\u5408\uff08\u6846\uff09\u9884\u6d4b\u7684\u4efb\u52a1\uff0c\u628a\u76ee\u6807\u68c0\u6d4b\u505a\u6210\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u51cf\u5c11\u4e86\u751f\u6210anchor\u7684\u6728\u7c89\uff0c\u4e5f\u7528\u4e0d\u5230nms</p> <p>\u63d0\u51fa\u7684\u65b0\u7684\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7\u4e8c\u5206\u56fe\u5339\u914d\u7684\u65b9\u5f0f\uff0c\u5f3a\u5236\u6a21\u578b\u751f\u6210\u72ec\u4e00\u65e0\u4e8c\u7684\u9884\u6d4b\u3002\u7ed9\u5b9a\u4e00\u7ec4\u67e5\u8be2\uff0cDETR \u63a8\u7406\u5bf9\u8c61\u548c\u5168\u5c40\u56fe\u50cf\u4e0a\u4e0b\u6587\u7684\u5173\u7cfb\uff0c\u4ee5\u5e76\u884c\u76f4\u63a5\u8f93\u51fa\u6700\u7ec8\u7684\u9884\u6d4b\u96c6\u3002</p> <p></p> <p>\u9996\u5148\u7528CNN\u62bd\u7279\u5f81\uff0c\u7136\u540e\u7528Transformer Encoder\u5b66\u5168\u5c40\u7279\u5f81\uff0c\u7528Decoder\u53bb\u751f\u6210\u9884\u6d4b\u6846 \uff08decoder\u4e2d\u8fd8\u6709\u8f93\u5165object query\uff09\u5f97\u5230\u6700\u540e\u8f93\u51fa\u7684\u6846\uff08100\u4e2a\uff09\uff0c\u7528\u4e8c\u5206\u56fe\u5339\u914d\u7684\u65b9\u5f0f\u7b97loss</p> <p>\u63a8\u7406\u7684\u65f6\u5019\u6700\u540e\u4e00\u6b65\u7528\u4e00\u4e2a\u9608\u503c\u5361\u4e2a\u7f6e\u4fe1\u5ea6\uff0c\u5927\u4e8e\u67d0\u4e2a\u503c\u7b97\u524d\u666f\u7269\u4f53</p>"},{"location":"DeepLearning/DETR/#_2","title":"\u57fa\u4e8e\u96c6\u5408\u7684\u76ee\u6807\u51fd\u6570","text":"<p>\\(y\\)\u662fground truth\uff0c\\(\\hat{y} = \\{ \\hat{y}_i \\}_i ^N\\) \u662fN\u4e2a\u9884\u6d4b\u503c\uff0cground truth\u4e2d\u6ca1\u6709\u51fa\u73b0\u7684\u6807\u8bb0\u4e3a\\(\\varnothing\\) \uff08back ground\uff09\uff0c\u6700\u4f18\u5339\u914d \\(\\hat{\\sigma}\\) \u6ee1\u8db3\u6700\u5c0f\u5316\u5339\u914d\u8bef\u5dee\\(\\sum_i^N{\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)}})\\) \uff08\u6700\u4f18\u5339\u914d\u95ee\u9898\u53ef\u4ee5\u7528\u5308\u7259\u5229\u7b97\u6cd5\u89e3\u51b3\uff09</p> \\[ \\hat{\\sigma} = \\text{argmin} \\sum_i^N{\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)}}) \\] <p>\u6bcf\u4e2a\\(y_i = (c_i,b_i)\\)\uff0c\\(c_i\\)\u8868\u793a\u7c7b\u522b\uff0c\\(b_i \\in {[0,1]}^4\\)\u8868\u793a\u76f8\u5bf9\u4e8e\u56fe\u50cf\u5927\u5c0f\u7684\u6846\u7684\u4e2d\u5fc3\u5750\u6807\u548c\u5bbd\u9ad8\uff0c\u5b9a\u4e49\\(y_i\u548c\\hat{y}_{\\sigma(i)}\\)\u4e4b\u95f4\u7684\u635f\u5931\u4e3a</p> \\[ \\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)}) = - \\mathbb{1}_{c_i \\neq \\varnothing}\\hat{p}_{\\sigma(i)}(c_i) + \\mathbb{1}_{c_i \\neq \\varnothing} \\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)}) \\] <p>bounding box \u635f\u5931\u5b9a\u4e49\u4e3a</p> \\[ \\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)}) = \\lambda_{IoU}\\mathcal{L}_{IoU}(b_i,\\hat{b}_{\\sigma(i)}) + \\lambda_{L_1}  \\lVert b_i - \\hat{b}_{\\sigma(i)} \\rVert_1 \\]"},{"location":"DeepLearning/DETR/#_3","title":"\u6a21\u578b\u7ed3\u6784","text":"<pre><code>\u8f93\u5165\u56fe\u7247\u5927\u5c0f\u4e3a3x800x1066\n--&gt; \u8fc7CNN\u62bd\u7279\u5f81 2048x25x34(800\u548c1066\u76841/32) --&gt;256x25x34\n--&gt; \u52a0\u4e0a\u4f4d\u7f6e\u7f16\u7801\n--&gt; Transformer Encoder 850x256\n--&gt; \u548cobject query 100x256 (100\u8868\u793a\u8f93\u51fa100\u4e2a\u6846)\u505a\u81ea\u6ce8\u610f\u529b\u64cd\u4f5c\n--&gt; decoder\u8f93\u51fa 100x256\u8fc7FFN(\u5171\u4eab\u53c2\u6570)\u5f97\u5230100\u4e2a\u6846\u548c\u7c7b\u522b\u7684\u9884\u6d4b\n</code></pre>"},{"location":"DeepLearning/DETR/#_4","title":"\u6a21\u578b\u4ee3\u7801","text":"<p>\u6587\u7ae0\u7ed9\u7684\u7b80\u5316\u7248\u4ee3\u7801</p> <pre><code>import torch\nfrom torch import nn\nfrom torchvision.models import resnet50\n\nclass DETR(nn.Module):\n    def __init__(self, num_classes, hidden_dim, nheads, num_encoder_layers, num_decoder_layers):\n        super().__init__()\n        # We take only convolutional layers from ResNet-50 model\n        self.backbone = nn.Sequential(*list(resnet50(pretrained=True).children())[:-2])\n        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n        self.transformer = nn.Transformer(hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n        self.linear_bbox = nn.Linear(hidden_dim, 4)\n        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n\n    def forward(self, inputs):\n        x = self.backbone(inputs)\n        h = self.conv(x)\n        H, W = h.shape[-2:]\n        pos = torch.cat([\n            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n        ], dim=-1).flatten(0, 1).unsqueeze(1)\n        h = self.transformer(pos + h.flatten(2).permute(2, 0, 1), self.query_pos.unsqueeze(1))\n        return self.linear_class(h), self.linear_bbox(h).sigmoid()\n\ndetr = DETR(num_classes=91, hidden_dim=256, nheads=8, num_encoder_layers=6, num_decoder_layers=6)\ndetr.eval()\ninputs = torch.randn(1, 3, 800, 1200)\nlogits, bboxes = detr(inputs)\n</code></pre>"},{"location":"DeepLearning/Deformable-DETR/","title":"Deformable DETR","text":"<p>Deformable DETR: Deformable Transformers For End-to-end Object Detection \uff082020.8\uff09</p> <p>\u5b98\u65b9\u4ee3\u7801</p> <p>\u53c2\u8003\u535a\u5ba2</p> <p>DETR\u7684\u95ee\u9898\uff1a 1.\u6536\u655b\u901f\u5ea6\u6162 2.\u5c0f\u76ee\u6807\u68c0\u6d4b\u6548\u679c\u5dee</p> <p>Deformable DETR\u7ed3\u5408\u548c\u53d8\u5f62\u5377\u79ef\u548cDETR\u89e3\u51b3DETR\u7684\u95ee\u9898</p> <p>\u6bcf\u4e2a\u7279\u5f81\u50cf\u7d20\u4e0d\u5fc5\u4e0e\u6240\u6709\u7684\u7279\u5f81\u50cf\u7d20\u4ea4\u4e92\u8ba1\u7b97\uff0c\u53ea\u9700\u4e0e\u90e8\u5206\u57fa\u4e8e\u91c7\u6837\u7684\u5176\u4ed6\u50cf\u7d20\u4ea4\u4e92\u5373\u53ef\uff0c\u52a0\u5feb\u4e86\u6a21\u578b\u7684\u6536\u655b</p>"},{"location":"DeepLearning/Deformable-DETR/#dcn","title":"DCN","text":"<p>\u53c2\u8003\u535a\u5ba2</p> <p>\u53ef\u5f62\u53d8\u5377\u79ef\u516c\u5f0f</p> \\[     \\mathbf{y}(\\mathbf{p_0}) = \\sum_{p_n \\in \\mathcal{R}} \\mathbf{w}(\\mathbf{p_n}) \\mathbf{x}(\\mathbf{p_0} + \\mathbf{p_n} + \\Delta\\mathbf{p_n}) \\] <pre><code>#...\nself.offsets = nn.Conv2d(128, 18, kernel_size=3, padding=1)\nself.conv4 = DeformConv2D(128, 128, kernel_size=3, padding=1)\n#...\noffsets = self.offsets(x)\nx = F.relu(self.conv4(x, offsets))\n#...\n</code></pre> <p>\u5bf9\u6bcf\u4e2a\u8f93\u5165\u7684\u7279\u5f81\u56fe\uff0c\u6bd4\u5982\u4f7f\u75283x3\u7684\u5377\u79ef\u6838\uff0coffset\u5c31\u662f2x3x3=18\uff08x\uff0cy\uff09\uff0c\u5148\u8fc7\u5377\u79ef\u5c42\u5f97\u5230feature map\u4e0a\u6bcf\u4e2a\u70b9\u5bf9\u5e94\u76849\u4e2a\u70b9\u7684offset\u7684xy\uff0c\u7136\u540e\u901a\u8fc7\u7b97\u51fa\u6bcf\u4e2a\u70b9\u5bf9\u5e94\u7684\u4e5d\u4e2a\u70b9\u7684\u503c\uff08\u5bf99\u4e2a\u70b9\u6bcf\u4e2a\u70b9\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u7b97\u51fa\u503c\uff09\uff0c\u7136\u540e\u628a\u6bcf\u4e2a\u70b9\u5bf9\u5e94\u76843x3\u7684\u641e\u5728\u4e00\u8d77\uff0c\u6bd4\u598210x10\u7684\u7279\u5f81\u56fe\u53d8\u621030x30\uff0c\u7136\u540e\u8fc7\u4e00\u4e2a3x3\u7684kernel\uff0cstrike\u4e3a3\u7684\u5377\u79ef\u5c42\u3002DCN\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\u5c31\u662f\u5f97\u5230offset\u7684\u5377\u79ef\u6838\u7684\u53c2\u6570\u4ee5\u53ca\u6700\u540e\u90a3\u4e2a\u5377\u79ef\u6838\u7684\u53c2\u6570\u3002\u8fd9\u91cc\u5bf9\u4e8e\u6240\u6709\u8fdbdefromConv2D\u7684\u7279\u5f81\u56fe\u90fd\u7528\u7684\u540c\u4e00\u4e2a\u504f\u79fb\u91cf\u3002</p>"},{"location":"DeepLearning/Deformable-DETR/#deformable-transformers","title":"Deformable Transformers","text":""},{"location":"DeepLearning/Deformable-DETR/#multi-head-attention","title":"Multi-Head Attention","text":"<p>\u8fd9\u91cc\u91cd\u5199\u4e86\u4e00\u4e0b\u591a\u5934\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u516c\u5f0f</p> \\[ \\text{MultiHeadAttn}(z_q,\\mathbf{x}) = \\sum_{m=1}^{M} \\mathbf{W}_m[\\sum_{k \\in \\Omega_k} A_{mqk}\\mathbf{W}_m^\\prime x_k] \\] <p>\u548c \\(\\text{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\)\u90a3\u4e2a\u7684\u591a\u5934\u7248\u7684\u516c\u5f0f\u662f\u7b49\u4ef7\u7684\u3002d2l-multihead</p> <p>\u5176\u4e2d \\(q \\in \\Omega_q\\) \u8868\u793aquery\u7684index\uff0c\\(k \\in \\Omega_k\\) \u8868\u793akey\u548cvalue\u7684index\uff08\u5305\u62ec\u6240\u6709\u7684HW\u4e2a\u70b9\uff09\uff0c\\(z_q,x_k\\in \\mathbb{R}^C\\)\uff0cM\u8868\u793a\u5934\u7684\u4e2a\u6570\uff0c\\(\\mathbf{W}_m^\\prime \\in \\mathbb{R}^{C_v \\times C}\\)\uff0c\\(\\mathbf{W}_m \\in \\mathbb{R}^{C \\times C_v}\\) \uff08\\(C_v =C / M\\)\uff09\uff0cattention\u7684\u6743\u91cd\\(A_{mqk} \\propto \\exp{(\\frac{z_q^TU_m^TV_mx_k}{\\sqrt{C_v}})}\\)\uff0c\u5e76\u4e14\u6ee1\u8db3\\(\\sum_{k \\in \\Omega_k}A_{mqk} = 1\\) \uff08softmax\uff09\uff0c\\(U_m,V_m \\in \\mathbb{R}^{C_v \\times C}\\)\u3002</p> <p>\u8fd9\u79cd\u8ba1\u7b97\u65b9\u5f0f\u6709\u4e24\u4e2a\u95ee\u9898\uff1a\u4e00\u662f\u6536\u655b\u5f88\u6162\uff0c\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\uff0c\u56e0\u4e3a\u5f53 \\(N_k\\) \u5f88\u5927\u7684\u65f6\u5019\\(A_{mqv}\\)\u63a5\u8fd1\u4e8e\\(1/N_k\\)\u5bfc\u81f4\u8f93\u5165\u7279\u5f81\u7684\u68af\u5ea6\u6a21\u7cca\uff1b\u4e8c\u662f\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u590d\u6742\u5ea6\u5f88\u9ad8\uff0c\u4e0a\u5f0f\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a \\(O(N_qC^2 + N_k C^2 + N_qN_kC)\\)\uff08\u6ca1\u641e\u61c2\u600e\u4e48\u7b97\u7684\u6587\u7ae0\u662f\u8fd9\u6837\uff09\uff0c\u5728\u56fe\u50cf\u9886\u57df\u4e00\u822c\u6709\\(N_q = N_k \\gg C\\)\uff0c\u6709\u590d\u6742\u5ea6\u4e3a\\(O(N_qN_kC)\\)\u968f\u7740\u7279\u5f81\u56fe\u5927\u5c0f\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u589e\u957f\u3002</p>"},{"location":"DeepLearning/Deformable-DETR/#deformable-attention","title":"Deformable Attention","text":"<p>\u7ed9\u5b9a\u4e00\u4e2a feature map \\(\\mathbf{x} \\in \\mathbb{R}^{C \\times H \\times W}\\),2-d\u7684point \\(p_q\\)\uff0c\u8fd9\u91cc\\(K\\)\u8868\u793a\u91c7\u6837\u7684\u70b9\u7684\u4e2a\u6570\uff0c\u6709 \\(HW \\gg K\\)</p> \\[ \\text{DeformAttn}(z_q,\\mathbf{x}) = \\sum_{m=1}^{M} \\mathbf{W}_m[\\sum_{k=1}^K A_{mqk}\\mathbf{W}_m^\\prime \\mathbf{x}(p_q + \\Delta p_{mqk})] \\] <p>\u8fd9\u91cc \\(\\Delta p_{mqk} \\in \\mathbb{R}^2\\) \u6ca1\u6709\u7ea6\u675f\uff0c\u7531\u4e8e\\(\\mathbf{x}(p_q + \\Delta p_{mqk})\\) \u5e76\u4e0d\u4e00\u5b9a\u662f\u6574\u6570\uff0c\u4f7f\u7528\u4e86\u53cc\u7ebf\u6027\u63d2\u503c\u8ba1\u7b97\u3002</p>"},{"location":"DeepLearning/Deformable-DETR/#multi-scale-deformable-attention","title":"Multi-scale Deformable Attention","text":"\\[ \\text{MSDeformAttn}(z_q,\\hat{p}_q,\\{\\mathbf{x}^l\\}^L_{l=1}) = \\sum_{m=1}^{M} \\mathbf{W}_m[\\sum_{l=1}^L\\sum_{k=1}^K A_{mlqk}\\mathbf{W}_m^\\prime \\mathbf{x}^l(\\phi_l(\\hat{p_q}) + \\Delta p_{mlqk})] \\] <p>L\u8868\u793a\u8f93\u5165\u7279\u5f81\u7684\u5c42\u7ea7\uff0c\\(\\phi_l(\\hat{p}_q)\\) \u5c06\u5f52\u4e00\u5316\u5750\u6807 \\(\\hat{p}_q\\) \u91cd\u65b0\u7f29\u653e\u5230\u7b2c \\(l\\) \u5c42\u7ea7\u7684\u7279\u5f81\u56fe\uff0c\\(A_{mlqk}\\) \u6ee1\u8db3 \\(\\sum_{l=1}^L\\sum_{k=1}^KA_{mlqk} = 1\\)</p> <p>\u5f53 \\(L=K=1\\)\uff0c\\(W_m^\\prime\\) \u4e3a\u5355\u4f4d\u77e9\u9635\u5462\u65f6\u76f8\u5f53\u4e8e\u53ef\u53d8\u5f62\u5377\u79ef</p>"},{"location":"DeepLearning/Deformable-DETR/#encoder-decoder","title":"Encoder Decoder","text":"<p>\u7528\u591a\u5c3a\u5ea6\u53ef\u5f62\u53d8\u6ce8\u610f\u529b\u6a21\u5757\u66ff\u6362DETR\u4e2d\u5904\u7406\u7279\u5f81\u7684Transformer Encoder</p> <p>Decoder\u4e2d\u53ea\u628across-attention\u7684\u6a21\u5757\u66ff\u6362\u4e3a\u591a\u5c3a\u5ea6\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\uff0cself-attention\u4fdd\u6301\u4e0d\u53d8</p>"},{"location":"DeepLearning/Deformable-DETR/#additional-inprovements","title":"Additional Inprovements","text":"<p>Iterative Bounding Box Refinement\u548cTwo-Stage Deformable DETR</p>"},{"location":"DeepLearning/Deformable-DETR/#iterative-bounding-box-refinement","title":"Iterative Bounding Box Refinement","text":"<p>Bounding Box \u504f\u79fb\u91cf\u4fee\u6b63 deformable DERR \u7684bbox\u9884\u6d4b\u5934\u7684\u9884\u6d4b\u7ed3\u679c\u662f\u76f8\u5bf9\u4e8e\u53c2\u8003\u70b9\u7684\u5750\u6807\u504f\u79fb\u91cf\uff0c\u8fd9\u6837\u8bbe\u8ba1\u53ef\u4ee5\u964d\u4f4e\u7f51\u7edc\u7684\u4f18\u5316\u96be\u5ea6</p> <p>\u9996\u5148\u7ecf\u8fc7simoid\u5f97\u5230\u7684\u53c2\u8003\u70b9\u5750\u6807\u8fc7\u4e00\u4e2a\u9006\u51fd\u6570\uff0c\u540e\u9762\u52a0\u4e0a\u504f\u79fb\u91cf\u4fee\u6b63\u5750\u6807</p> \\[ \\hat{\\mathbf{b}}_q = \\{\\sigma(\\mathbf{b}_{qx} + \\sigma ^{-1}(\\hat{p}_{qx})),\\sigma(\\mathbf{b}_{qy} + \\sigma ^{-1}(\\hat{p}_{qy})),\\sigma(\\mathbf{b}_{qw}),\\sigma(\\mathbf{b}_{qh}) \\} \\] <p>\u5c31\u662f\u6bcf\u6b21\u8fc7\u4e00\u5c42decoder\uff0creference point\u90fd\u7528\u8f93\u51fa\u7684\u53c2\u8003\u70b9\u7684\u503c\uff08\u4e0d\u542f\u7528\u7684\u8bdd\u5c31\u662f\u7528\u521d\u59cb\u7684\u503c\u52a0\u4e0a\u504f\u79fb\u91cf\uff09</p> <p>\u7b2cd\u5c42decoder layer</p> \\[ \\hat{\\mathbf{b}}_q^d = \\{\\sigma(\\Delta b^{d}_{qx} + \\sigma^{-1}(\\hat{b}^{d-1}_{qx})),\\sigma(\\Delta b^{d}_{qy} + \\sigma^{-1}(\\hat{b}^{d-1}_{qy})),\\sigma(\\Delta b^{d}_{qw} + \\sigma^{-1}(\\hat{b}^{d-1}_{qw})),\\sigma(\\Delta b^{d}_{qh} + \\sigma^{-1}(\\hat{b}^{d-1}_{qh}))\\} \\] <p>\u521d\u59cb\u5316\\(b^{0}_{qx} = \\hat{p}_{qx}\\)\uff0c\\(b^{0}_{qy} = \\hat{p}_{qy}\\)\uff0c\\(b^{0}_{qw} = 0.1\\)\uff0c\\(b^{0}_{qh} = 0.1\\)</p> <p>\u5e76\u4e14\u6bcf\u4e00\u5c42decoder\u5f97\u5230box\u7684FFN\u4e0d\u5171\u4eab\u53c2\u6570\uff08clone\u4e866\u4e2a\uff09\u3002</p>"},{"location":"DeepLearning/Deformable-DETR/#two-stage-deformable-detr","title":"Two-Stage Deformable DETR","text":"<p>\u6ca1\u6709two stage\u7684\u8bdddecoder\u7684query embedding\u662f\u53ef\u4ee5\u5b66\u4e60\u7684\u53c2\u6570\uff0ctwo stage decoder \u7684query\u662fencoder\u7684\u8f93\u51fa\u5f97\u5230proposal\uff0c\u76f4\u63a5\u7528decoder\u6700\u540e\u7684class\u5206\u7c7b\u5934\u5f97\u5230top score\uff08300\u4e2a\uff09\u7684\u5f97\u5230proposal\u751f\u6210\u7684\u3002</p>"},{"location":"DeepLearning/Deformable-DETR/#code","title":"Code","text":"<p>\u6ce8\u91ca\u7248\u6e90\u4ee3\u7801</p> <p>\u627e\u5230\u4e86\u4e00\u7bc7\u6e90\u7801\u89e3\u6790\u7684\u535a\u5ba2</p>"},{"location":"DeepLearning/Deformable-DETR/#msdefromatten","title":"MSDefromAtten","text":"<pre><code>#class MSDeformAttn\n    def forward(self, query, reference_points,\n                input_flatten, input_spatial_shapes,\n                input_level_start_index,\n                input_padding_mask=None):\n        \"\"\"\n        query\u662f\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u52a0\u4e0a\u4e86\u4f4d\u7f6e\u7f16\u7801\n        :param query                       (N, Length_{query}, C)\n        \u53c2\u8003\u70b9\u4f4d\u7684\u5750\u6807\n        :param reference_points            (N, Length_{query}, n_levels, 2), range in [0, 1], top-left (0,0), bottom-right (1, 1), including padding area\n                                        or (N, Length_{query}, n_levels, 4), add additional (w, h) to form reference boxes\n        encoder\u662f\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\uff0cdecoder\u4f7f\u7528\u7684\u662fencoder\u7684\u8f93\u51fa [bs, all hw, 256]\n        :param input_flatten               (N, \\sum_{l=0}^{L-1} H_l \\cdot W_l, C)\n        4\u4e2a\u7279\u5f81\u5c42\u7684\u9ad8\u5bbd\n        :param input_spatial_shapes        (n_levels, 2), [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\n        \u5404\u4e2a\u7279\u5f81\u5c42\u7684\u8d77\u59cbindex\u7684\u4e0b\u6807 \u5982: [    0,  8056, 10070, 10583]\n        :param input_level_start_index     (n_levels, ), [0, H_0*W_0, H_0*W_0+H_1*W_1, H_0*W_0+H_1*W_1+H_2*W_2, ..., H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}]\n        [bs,all hw]\n        :param input_padding_mask          (N, \\sum_{l=0}^{L-1} H_l \\cdot W_l), True for padding elements, False for non-padding elements\n\n        :return output                     (N, Length_{query}, C)\n        \"\"\"\n\n        # query \u662fsrc+pos\uff0cquery\u4e0b\u9762\u53d8\u6210\u4e86attention_weights\n        # input_flatten \u662fsrc\uff0cinput_flatten \u5bf9\u5e94\u4e86V\n        # bs, all hw\uff08decoder \u662f300\uff09, 256\n        N, Len_q, _ = query.shape\n        # [bs,all hw,256]\n        N, Len_in, _ = input_flatten.shape\n\n        assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n        # \u5bf9encoder\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\uff0c\u6216\u8005decoder\u4f7f\u7528\u7684encoder\u7684\u8f93\u51fa \u8fdb\u884c\u4e00\u5c42\u5168\u8fde\u63a5\u53d8\u6362\uff0cchannel\u4e0d\u53d8\n        value = self.value_proj(input_flatten)\n\n        if input_padding_mask is not None:\n            # \u586b\u51450 [bs, all hw,256]\n            value = value.masked_fill(input_padding_mask[..., None], float(0))\n\n        # \u5206\u6210\u591a\u5934\uff0c\u62c6\u5206\u7684\u662f\u6700\u540e\u7684256 [bs,all hw,256] -&gt; [bs,all hw, 8, 32]\n        value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)\n        # sampling_offsets \u662f\u4e00\u4e2a\u5168\u8fde\u63a5\n        # like (bs, all hw,8,4,4,2) 8\u4e2a\u5934\uff0c4\u4e2a\u7279\u5f81\u5c42\uff0c4\u4e2a\u91c7\u6837\u70b9 2\u4e2a\u504f\u79fb\u91cf\u5750\u6807\n        sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, 2)\n        # attention_weights \u662f\u4e00\u4e2a\u5168\u8fde\u63a5\n        # like (bs, all hw,8,16)\n        attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)\n\n        # like (bs,all hw,8,4,4)\n        # \u7ecf\u8fc7softmax \u4fdd\u8bc1\u6743\u91cd\u548c\u4e3a1\n        attention_weights = F.softmax(attention_weights, -1).view(N, Len_q, self.n_heads, self.n_levels, self.n_points)\n\n        # N, Len_q, n_heads, n_levels, n_points, 2\n        if reference_points.shape[-1] == 2:\n\n            # input_spatial_shapes \u6362\u4f4d\u7f6e\uff0c\u9ad8\u5bbd \u53d8\u6210 \u5bbd\u9ad8\n            offset_normalizer = torch.stack([input_spatial_shapes[..., 1], input_spatial_shapes[..., 0]], -1)\n\n            # reference_points  [bs,all hw,4,2] -&gt; [bs,all hw,1,4,1,2]\n            # sampling_offsets  [bs,all hw,8,4,4,2]\n            # offset_normalizer [4,2] -&gt; [1,1,1,4,1,2]\n            # like (bs, hw,8,4,4,2)\n            # \u91c7\u6837\u70b9\u52a0\u4e0a\u504f\u79fb\u91cf\n            sampling_locations = reference_points[:, :, None, :, None, :] \\\n                                 + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n\n        elif reference_points.shape[-1] == 4:\n\n            sampling_locations = reference_points[:, :, None, :, None, :2] \\\n                                 + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n        else:\n            raise ValueError(\n                'Last dim of reference_points must be 2 or 4, but get {} instead.'.format(reference_points.shape[-1]))\n\n        # \u8fd9\u91cc\u8c03\u7528\u4e86cuda\u5b9e\u73b0\n        output = MSDeformAttnFunction.apply(\n            value, input_spatial_shapes, input_level_start_index,\n            sampling_locations, attention_weights,\n            self.im2col_step)\n\n        output = self.output_proj(output)\n        return output\n</code></pre> <p>\u8fd9\u91cc\u7b97\u5230\u7684\u662fattention weight\u7b97\u51fa\u6765\u4e86\uff0c \\(\\mathbf{x}^l(\\phi_l(\\hat{p_q}) + \\Delta p_{mlqk})\\) \u8fd9\u4e2a\u662f\u901a\u8fc7<code>F.grid_sample()</code>\u53cc\u7ebf\u6027\u63d2\u503c\u7b97\u51fa\u6765\u7684</p> <pre><code>def ms_deform_attn_core_pytorch(value, value_spatial_shapes, sampling_locations, attention_weights):\n    # for debug and test only,\n    # need to use cuda version instead\n    N_, S_, M_, D_ = value.shape\n    _, Lq_, M_, L_, P_, _ = sampling_locations.shape\n    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes], dim=1)\n    sampling_grids = 2 * sampling_locations - 1\n    sampling_value_list = []\n    for lid_, (H_, W_) in enumerate(value_spatial_shapes):\n        # N_, H_*W_, M_, D_ -&gt; N_, H_*W_, M_*D_ -&gt; N_, M_*D_, H_*W_ -&gt; N_*M_, D_, H_, W_\n        value_l_ = value_list[lid_].flatten(2).transpose(1, 2).reshape(N_ * M_, D_, H_, W_)\n        # N_, Lq_, M_, P_, 2 -&gt; N_, M_, Lq_, P_, 2 -&gt; N_*M_, Lq_, P_, 2\n        sampling_grid_l_ = sampling_grids[:, :, :, lid_].transpose(1, 2).flatten(0, 1)\n        # N_*M_, D_, Lq_, P_\n        sampling_value_l_ = F.grid_sample(value_l_, sampling_grid_l_,\n                                          mode='bilinear', padding_mode='zeros', align_corners=False)\n        sampling_value_list.append(sampling_value_l_)\n    # (N_, Lq_, M_, L_, P_) -&gt; (N_, M_, Lq_, L_, P_) -&gt; (N_, M_, 1, Lq_, L_*P_)\n    attention_weights = attention_weights.transpose(1, 2).reshape(N_ * M_, 1, Lq_, L_ * P_)\n    output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights).sum(-1).view(N_, M_ * D_, Lq_)\n    return output.transpose(1, 2).contiguous()\n</code></pre>"},{"location":"DeepLearning/Deformable-DETR/#deformable-transformer","title":"Deformable transformer","text":"<p>Encoder\u4e3b\u8981\u5c31\u662f\u628aMutihead-attention\u6362\u6210MSDeformAtten\uff0c\u8f93\u5165\u589e\u52a0\u4e86\u4e00\u4e9b\u70b9\u548c\u5c42\u7ea7\u7684\u4fe1\u606f\uff0c\u8f93\u5165\u7684src\u7684\u5f62\u72b6\u4e3a(batch_size,sum of hw, d_model)</p> <pre><code>class DeformableTransformerEncoderLayer(nn.Module):\n    def __init__(self,\n                 d_model=256, d_ffn=1024,\n                 dropout=0.1, activation=\"relu\",\n                 n_levels=4, n_heads=8,\n                 # Deformable\u7684\u5185\u5bb9\n                 n_points=4):\n        super().__init__()\n\n        # self attention\n        self.self_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n\n        # ffn \u7684\u5185\u5bb9\n        self.linear1 = nn.Linear(d_model, d_ffn)\n        self.activation = _get_activation_fn(activation)\n        self.dropout2 = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ffn, d_model)\n        self.dropout3 = nn.Dropout(dropout)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    @staticmethod\n    def with_pos_embed(tensor, pos):\n        return tensor if pos is None else tensor + pos\n\n    def forward_ffn(self, src):\n        # \u4e00\u4e2a\u5168\u8fde\u63a5\uff0c\u4e00\u4e2a\u6fc0\u6d3b\uff0c\u4e00\u4e2adropout\uff0c\u4e00\u4e2a\u5168\u8fde\u63a5\n        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))\n        # add \u548c dropout\n        src = src + self.dropout3(src2)\n        # norm\n        src = self.norm2(src)\n        return src\n\n    def forward(self, src, pos, reference_points, spatial_shapes, level_start_index, padding_mask=None):\n        \"\"\"\n        src: [bs,all hw,256]\n        pos: [bs,all hw,256]\n        reference_points: [bs,all hw,4,2]\n        spatial_shapes: [4,2] 4\u4e2a\u7279\u5f81\u5c42\u7684\u9ad8\u5bbd\n        level_start_index: \u5404\u4e2a\u7279\u5f81\u5c42\u7684\u8d77\u59cbindex\u7684\u4e0b\u6807 \u5982: [    0,  8056, 10070, 10583]\n        padding_mask: [bs,all hw]\n        \"\"\"\n        # with_pos_embed \u5c31\u662f\u5c06src\u548cpos\u76f8\u52a0, src\u662f\u4e0a\u4e00\u5c42encoder\u7684\u8f93\u51fa\n        # 1. self attention\n        src2 = self.self_attn(self.with_pos_embed(src, pos), reference_points, src,\n                              spatial_shapes, level_start_index, padding_mask)\n        # 2. add\n        src = src + self.dropout1(src2)\n        # 3. norm\n        src = self.norm1(src)\n\n        # 4. ffn + add &amp; norm\n        src = self.forward_ffn(src)\n\n        return src\n</code></pre> <p>\u83b7\u53d6\u53c2\u8003\u70b9</p> <p>valid ratio\u5728transformer\u5757\u4e2d\u5b9a\u4e49</p> <pre><code>#class DeformableTransformer\n    def get_valid_ratio(self, mask):\n        \"\"\"\n        \u56fe\u50cf\u7684\u5927\u5c0f\u5728\u4e00\u4e2abatch\u5185\u7edf\u4e00\u6210\u4e86\u6700\u5927\u7684\u9ad8\u5bbd\uff0c\u4f46\u662f\u5177\u4f53\u7684\u4e00\u4e2a\u5f20\u56fe\u4f1a\u5360\u636e\u5176\u4e2d\u5de6\u4e0a\u7684\u67d0\u4e2a\u533a\u57df\uff0c\u5176\u4ed6\u7684\u533a\u57df\u5728Mask\u4e2d\u662fTrue\n        \u8fd9\u91cc\u5c31\u8981\u6c42\u51fa\u771f\u5b9e\u7684\u56fe\u50cf\u5927\u5c0f\u5360\u636e\u7684\u6bd4\u7387\n        mask [bs,h,w]\n        \u6709\u6548\u9ad8\u5bbd\u5360\u603b\u7684\u9ad8\u5bbd\u7684\u6bd4\u7387\n        \"\"\"\n        # \u7279\u5f81\u56fe\u7684\u9ad8 \u5bbd\n        _, H, W = mask.shape\n\n        # tensor\u7684size\u5c31\u662fbs\u7684\u5927\u5c0f [:,:,0] \u5c31\u662f\u53d6\u7684\u7b2c\u4e00\u5217\uff0c\u90a3\u4e48\u5c31\u662f\u9ad8\u5ea6\u7684\u610f\u601d\uff0c~mask\uff0c\u6709mask\u7684\u4f4d\u7f6e\u662fTrue\uff0c~mask\u53d6\u53cd\n        # \u6709\u6548\u7684\u9ad8\u5ea6\n        valid_H = torch.sum(~mask[:, :, 0], 1)\n        # \u6709\u6548\u7684\u5bbd\u5ea6\n        valid_W = torch.sum(~mask[:, 0, :], 1)\n        # \u5360\u603b\u957f\u5ea6\u7684\u6bd4\u4f8b\n        valid_ratio_h = valid_H.float() / H\n        # \u5360\u603b\u5bbd\u5ea6\u7684\u6bd4\u4f8b\n        valid_ratio_w = valid_W.float() / W\n\n        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)\n        #(bs,2)\n        return valid_ratio\n</code></pre> <pre><code>class DeformableTransformerEncoder(nn.Module):\n    def __init__(self, encoder_layer, num_layers):\n        super().__init__()\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n\n    @staticmethod\n    def get_reference_points(spatial_shapes, valid_ratios, device):\n        # spatial_shapes [\u7279\u5f81\u5c42\u6570,2] valid_ratios [bs,\u7279\u5f81\u5c42\u6570,2]\n        reference_points_list = []\n\n        for lvl, (H_, W_) in enumerate(spatial_shapes):\n            # \u751f\u6210\u7f51\u683c\u70b9,\u4ece0.5\u5f00\u59cb \u5230 \u51cf\u6389\u4e00\u4e2a0.5\n            ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),\n                                          torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device))\n\n            # \u5750\u6807\u8fdb\u884c\u7f29\u653e valid_ratios[:, None, lvl, 1] * H_\u662f\u5728H_\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u7f29\u51cf\u8303\u56f4\n\n            # reshape(-1) \u62c9\u5e73\u4f1a\u53d8\u6210\u4e00\u7ef4\u7684 shape=hw\uff0c[None]\uff0c\u4f1a\u5728\u6700\u524d\u9762\u52a0\u4e0a\u4e00\u4e2a1\u7ef4\u5ea6(1,hw)\n            # valid_ratios(bs,num_lvl,2) \u5207\u7247\u64cd\u4f5c -&gt; (bs,1)\n            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * H_)\n            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * W_)\n            # ref \u7684\u5f62\u72b6(bs,hw,2)\n            ref = torch.stack((ref_x, ref_y), -1)\n\n            reference_points_list.append(ref)\n        # \u6240\u6709\u7279\u5f81\u5c42\u7684\u53c2\u8003\u70b9\u62fc\u5728\u4e00\u8d77 [bs,all hw,2]\n        reference_points = torch.cat(reference_points_list, 1)\n\n        # reference_points[:,:,None] -&gt; [bs,all hw,1,2]\n        # valid_ratios[:,None] -&gt; [bs,1,\u7279\u5f81\u5c42\u6570\u91cf,2]\n        reference_points = reference_points[:, :, None] * valid_ratios[:, None]\n        # [bs,all hw,\u7279\u5f81\u5c42\u6570,2]\n        return reference_points\n\n    def forward(self, src, spatial_shapes, level_start_index, valid_ratios, pos=None, padding_mask=None):\n        \"\"\"\n        src: [bs,all hw,256] backbone\u7684\u7279\u5f81\n        spatial_shapes: [\u7279\u5f81\u5c42\u7684\u6570\u91cf,2] \u5404\u4e2a\u7279\u5f81\u5c42\u7684\u9ad8\u5bbd\n        level_start_index: \u5404\u4e2a\u5c42\u7684 all hw\u4e2d\u7684\u8d77\u59cb\u5750\u6807\u4f4d\u7f6e\n        valid_ratios: [bs,4,2] todo \n        pos: [bs,all hw,256] \u4f4d\u7f6e\u7f16\u7801\n        padding_mask: [bs, all hw]\n        \"\"\"\n        output = src\n        # \u83b7\u53d6\u53c2\u8003\u70b9\n        # encoder\u7684\u53c2\u8003\u70b9\u662fgrid\u751f\u6210\u7684\uff0c\u5e76\u4e14\u4e0d\u4f1a\u7cbe\u70bc\uff0c\u4e0d\u4f1a\u6709\u8fed\u4ee3\u7684\u66f4\u65b0\n        # [bs,all hw,4,2]\n        reference_points = self.get_reference_points(spatial_shapes, valid_ratios, device=src.device)\n        for _, layer in enumerate(self.layers):\n            # [bs,all hw,256]\n            output = layer(output, pos, reference_points,\n                           spatial_shapes, level_start_index, padding_mask)\n\n        return output\n</code></pre> <pre><code>class DeformableTransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model=256, d_ffn=1024,\n                 dropout=0.1, activation=\"relu\",\n                 n_levels=4, n_heads=8, n_points=4):\n        super().__init__()\n        # Deformable DETR\u5b9e\u73b0\u7684Attention\n        # cross attention\n        self.cross_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        # \u6807\u51c6\u7684Attention\n        # self attention\n        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.norm2 = nn.LayerNorm(d_model)\n\n        # ffn\n        self.linear1 = nn.Linear(d_model, d_ffn)\n        self.activation = _get_activation_fn(activation)\n        self.dropout3 = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ffn, d_model)\n        self.dropout4 = nn.Dropout(dropout)\n        self.norm3 = nn.LayerNorm(d_model)\n\n    @staticmethod\n    def with_pos_embed(tensor, pos):\n        return tensor if pos is None else tensor + pos\n\n    def forward_ffn(self, tgt):\n        tgt2 = self.linear2(self.dropout3(self.activation(self.linear1(tgt))))\n        tgt = tgt + self.dropout4(tgt2)\n        tgt = self.norm3(tgt)\n        return tgt\n\n    def forward(self, tgt, query_pos, reference_points,\n                src, src_spatial_shapes, level_start_index,\n                src_padding_mask=None):\n        \"\"\"\n        tgt \u662f\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa [bs,300,256]\n        query_pos \u5c31\u662f\u5916\u9762\u7684query_embed [bs,300,256]\n        reference_points \u5404\u4e2aimage\u5728300\u4e2aquery\u4e0a\u6bcf\u4e2a\u7279\u5f81\u5c42\u4e0a\u7684\u53c2\u8003\u70b9\u7684\u5750\u6807 [bs,300,4,2]\n        src\u662fencoder\u7684\u8f93\u51fa [bs,all hw,256]\n        src_spatial_shapes \u5404\u4e2a\u7279\u5f81\u5c42\u7684\u9ad8\u5bbd [4,2]\n        level_start_index \u5404\u4e2a\u7279\u5f81\u5c42\u7684\u8d77\u59cb\u4e0b\u6807\n        src_padding_mask mask\n        \"\"\"\n\n        # self attention\n        # tgt\u548cquery_pos\u76f8\u52a0, q k\u5c31\u662f\u8fd9\u4e24\u4e2a\u6784\u6210\u4e86\n        q = k = self.with_pos_embed(tgt, query_pos)\n        # 1. self-attention, q k\u5728\u4e0a\u9762\u521b\u5efa\u4e86\uff0cvalue\u5c31\u8fd8\u662ftgt\n        tgt2 = self.self_attn(q.transpose(0, 1), k.transpose(0, 1), tgt.transpose(0, 1))[0].transpose(0, 1)\n        # 2. add\n        tgt = tgt + self.dropout2(tgt2)\n        # 3. norm\n        tgt = self.norm2(tgt)\n\n        # Deformable DETR\u5b9e\u73b0\u7684attention\n        # 4. cross attention\n\n        # reference_points \u5404\u4e2aimage\u5728300\u4e2aquery\u4e0a\u6bcf\u4e2a\u7279\u5f81\u5c42\u4e0a\u7684\u76ee\u6807\u70b9\u4f4d [bs,300,4,2]\n        # src\u662fencoder\u7684\u8f93\u51fa [bs,all hw,256]\n        # src_spatial_shapes \u5404\u4e2a\u7279\u5f81\u5c42\u7684\u9ad8\u5bbd [4,2]\n        # level_start_index \u5404\u4e2a\u7279\u5f81\u5c42\u7684\u8d77\u59cb\u4e0b\u6807\n        # src_padding_mask mask\n        tgt2 = self.cross_attn(self.with_pos_embed(tgt, query_pos),\n                               reference_points,\n                               src,\n                               src_spatial_shapes, level_start_index, src_padding_mask)\n\n        # 5. add\n        tgt = tgt + self.dropout1(tgt2)\n        # 6. norm\n        tgt = self.norm1(tgt)\n\n        # 7. ffn add &amp; norm\n        tgt = self.forward_ffn(tgt)\n\n        return tgt\n</code></pre> <pre><code>class DeformableTransformerDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, return_intermediate=False):\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.return_intermediate = return_intermediate\n\n        # hack implementation for iterative bounding box refinement and two-stage Deformable DETR\n        self.bbox_embed = None\n        self.class_embed = None\n\n    def forward(self, tgt, reference_points, src,\n                src_spatial_shapes, src_level_start_index, src_valid_ratios,\n                query_pos=None, src_padding_mask=None):\n        \"\"\"\n        tgt \u4ecequery_embed\u5206\u51fa\u6765\u7684 [bs,300,256]\n        reference_points reference_point query_embed\u7ecf\u8fc7\u5168\u8fde\u63a5\u7f51\u7edc\u751f\u6210\u7684 [bs,300,2], \u53c2\u8003\u70b9\u7684\u521d\u59cb\u53c2\u8003\u70b9\n        src encoder\u7684\u8f93\u51fa [bs, all hw,256]\n        src_spatial_shapes \u5404\u4e2a\u7279\u5f81\u5c42\u7684\u9ad8\u5bbd [4,2]\n        src_level_start_index \u5404\u4e2a\u7279\u5f81\u5c42\u7684\u8d77\u59cb\u4e0b\u6807\n        src_valid_ratios [bs,4,2] \u5404\u4e2a\u56fe\u50cf\u771f\u5b9e\u7684\u9ad8\u5bbd\u5360\u636eMask\u5927\u5c0f\u4e2d\u7684\u6bd4\u7387\n        query_pos \u5c31\u662f\u5916\u9762\u7684query_embed [bs,300,256]\n        src_padding_mask [bs,all hw]\n        \"\"\"\n\n        output = tgt\n\n        intermediate = []\n\n        intermediate_reference_points = []\n\n        for lid, layer in enumerate(self.layers):\n            # \u53c2\u8003\u70b9\u7684\u5750\u6807\n            assert reference_points.shape[-1] == 2\n            # [bs,300,2] -&gt; [bs,300,1,2] [bs,4,2] -&gt; [bs,1,4,2]\n            # reference_points_input [bs,300,4,2]\n            # \u53c2\u8003\u70b9\u5750\u6807\u4e5f\u8981\u6309\u6bd4\u4f8b\u7684\u8fdb\u884c\u7f29\u653e\n            reference_points_input = reference_points[:, :, None] * src_valid_ratios[:, None]\n\n            # \u5982\u679c\u6ca1\u6709\u4f7f\u7528bbox\u7cbe\u70bc\uff0c\u90a3\u4e48\u6bcf\u6b21\u7684reference_points_input \u5176\u5b9e\u662f\u76f8\u540c\u7684\n            # \u5982\u679c\u4f7f\u7528\u4e86bbox\u7cbe\u70bc\uff0c\u90a3\u4e48\u6bcf\u6b21\u7684reference_points_input \u662f\u4e0d\u540c\u7684\uff0c\u4f1a\u4f7f\u7528decoder\u7684\u8f93\u51fa\u5f97\u5230\u504f\u79fb\u91cf\u4fee\u6b63\n\n            # output \u662f\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\n            # query_pos \u5c31\u662f\u5916\u9762\u7684query_embed\n            # reference_points_input \u5404\u4e2aimage\u5728300\u4e2aquery\u4e0a\u6bcf\u4e2a\u7279\u5f81\u5c42\u4e0a\u7684\u76ee\u6807\u70b9\u4f4d\n            # src\u662fencoder\u7684\u8f93\u51fa\n            # src_spatial_shapes \u5404\u4e2a\u7279\u5f81\u5c42\u7684\u9ad8\u5bbd\n            # src_level_start_index \u5404\u4e2a\u7279\u5f81\u5c42\u7684\u8d77\u59cb\u4e0b\u6807\n            # src_padding_mask mask\n            output = layer(output, query_pos, reference_points_input,\n                           src, src_spatial_shapes, src_level_start_index,\n                           src_padding_mask)\n\n            # \u5982\u679c\u4f7f\u7528\u4e86with_box_refine\u6a21\u5f0f, \u8fd9\u4e2a\u5730\u65b9\u7684bbox_embed \u662f\u975eNone\u7684\n            # hack implementation for iterative bounding box refinement\n            if self.bbox_embed is not None:\n                # output \u662f\u4e0a\u4e00\u5c42decoder\u7684\u8f93\u51fa\uff0c\u7ecf\u8fc7bbox\u9884\u6d4b\u7f51\u7edc [bs,300,4]\n                # \u5f97\u5230\u6b64\u6b21\u7684bbox\u7684\u504f\u79fb\u91cf\u4fee\u6b63\n                tmp = self.bbox_embed[lid](output)\n\n                if reference_points.shape[-1] == 4:\n                    # inverse_sigmoid sigmoid\u7684\u53cd\u51fd\u6570\n                    new_reference_points = tmp + inverse_sigmoid(reference_points)\n\n                    new_reference_points = new_reference_points.sigmoid()\n                else:\n                    assert reference_points.shape[-1] == 2  # [bs,300,2]\n                    # [bs,300,4]\n                    new_reference_points = tmp\n                    # \u524d\u4e24\u4e2a\u662fbbox\u7684\u4e2d\u5fc3\u5750\u6807\uff0c\u4e2d\u5fc3\u5750\u6807\u7528reference_points\u7684\u5185\u5bb9+\u504f\u79fb\u91cf\u4fee\u6b63\n                    new_reference_points[..., :2] = tmp[..., :2] + inverse_sigmoid(reference_points)\n                    # [bs,300,4] \u7ecf\u8fc7sigmoid \u7ea6\u675f\u57280-1\n                    new_reference_points = new_reference_points.sigmoid()\n                # \u66ff\u6362\u4e86\u539f\u6765\u7684reference_points\n                reference_points = new_reference_points.detach()\n\n            # \u8fd4\u56de\u524d\u51e0\u5c42\uff0c\u5426\u5219\u53ea\u8fd4\u56de\u6700\u540e\u4e00\u5c42\n            if self.return_intermediate:\n                intermediate.append(output)\n                # \u5982\u679c\u662f\u7cbe\u70bc\u6a21\u5f0f\uff0creference_points\u5728\u5404\u4e2a\u5c42\u4e4b\u540e\u662f\u4e0d\u540c\u7684\n                # \u5982\u679c\u4e0d\u662f\u7cbe\u70bc\u6a21\u578b\uff0creference_points\u5728\u5404\u4e2a\u5c42\u4e4b\u540e\u8fd8\u662f\u540c\u4e00\u4efd\u5185\u5bb9\n                intermediate_reference_points.append(reference_points)\n\n        if self.return_intermediate:\n            return torch.stack(intermediate), torch.stack(intermediate_reference_points)\n\n        return output, reference_points\n</code></pre> <p>\u5f53\u4e0d\u662ftwo_stage\u7684\u60c5\u51b5</p> <pre><code>class DeformableTransformer(nn.Module):\n    def __init__(self, d_model=256, nhead=8,\n                 num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=1024, dropout=0.1,\n                 activation=\"relu\", return_intermediate_dec=False,\n                 num_feature_levels=4,\n                 # \u8fd9\u4e24\u4e2a4\u5e94\u8be5\u5c31\u662f\u8fd9\u4e2a\u8bba\u6587\u4e3b\u8981\u7684Deformable\u7684\u5185\u5bb9\n                 dec_n_points=4,\n                 enc_n_points=4,\n\n                 two_stage=False, two_stage_num_proposals=300):\n        super().__init__()\n\n        self.d_model = d_model\n        self.nhead = nhead\n        self.two_stage = two_stage\n        self.two_stage_num_proposals = two_stage_num_proposals\n\n        encoder_layer = DeformableTransformerEncoderLayer(d_model, dim_feedforward,\n                                                          dropout, activation,\n                                                          num_feature_levels, nhead, enc_n_points)\n        self.encoder = DeformableTransformerEncoder(encoder_layer, num_encoder_layers)\n\n        decoder_layer = DeformableTransformerDecoderLayer(d_model, dim_feedforward,\n                                                          dropout, activation,\n                                                          num_feature_levels, nhead, dec_n_points)\n        self.decoder = DeformableTransformerDecoder(decoder_layer, num_decoder_layers, return_intermediate_dec)\n\n        # deformable detr \u591a\u7684\u5185\u5bb9 \u8bba\u6587\u4e2d\u6709\u63d0\u53ca\n        self.level_embed = nn.Parameter(torch.Tensor(num_feature_levels, d_model))\n\n        self.reference_points = nn.Linear(d_model, 2)\n        # \u53c2\u6570\u91cd\u5236\n        self._reset_parameters()\n\n    def forward(self, srcs, masks, pos_embeds, query_embed=None):\n        \"\"\"\n        srcs backbone\u7684\u7279\u5f81\n        masks seg\u4f7f\u7528\u7684\n        pos_embeds \u4f4d\u7f6e\u7f16\u7801\n        query_embed decoder\u4f7f\u7528\n        \"\"\"\n        # \u6709two_stage \u6216\u8005\u4e0d\u662ftwo_stage\u4f46\u662f\u8981\u6709query_embed\n        assert self.two_stage or query_embed is not None\n\n        # prepare input for encoder\n        src_flatten = []\n\n        mask_flatten = []\n\n        lvl_pos_embed_flatten = []\n        # \u7279\u5f81\u56fe\u7684\u9ad8\u5bbd\n        spatial_shapes = []\n        # src\u5185\u7684\u7279\u5f81\u56fe\u662f\u4ece\u5927\u5c3a\u5bf8\u5230\u5c0f\u5c3a\u5bf8\n        for lvl, (src, mask, pos_embed) in enumerate(zip(srcs, masks, pos_embeds)):\n            bs, c, h, w = src.shape\n            # \u7279\u5f81\u56fe\u7684\u9ad8\u5bbd\n            spatial_shape = (h, w)\n\n            spatial_shapes.append(spatial_shape)\n            # [bs,hw,256]\n            src = src.flatten(2).transpose(1, 2)  # \u8fd9\u5757\u7ef4\u5ea6\u7684\u987a\u5e8f\u4e0edetr\u4e0d\u540c detr \u662f hw\uff0cbs\uff0cdim\uff0c\u8fd9\u91cc\u662f bs,hw, dim\n            # [bs,hw]\n            mask = mask.flatten(1)\n            # [bs,hw,256]\n            pos_embed = pos_embed.flatten(2).transpose(1, 2)\n            # level embed \u4f1a\u88ab\u52a0\u5165\u5230\u4f4d\u7f6e\u7f16\u7801\u4e2d,level\u662f\u5728\u8bba\u6587\u4e2d\u63d0\u5230\u8fc7\u7684\n            # \u8fd9\u4e2a\u8fd8\u591a\u4e2alevel pos embed [bs,hw,256]\n            lvl_pos_embed = pos_embed + self.level_embed[lvl].view(1, 1, -1)\n\n            lvl_pos_embed_flatten.append(lvl_pos_embed)\n            src_flatten.append(src)\n            mask_flatten.append(mask)\n\n        # [bs,all hw,256]\n        # \u6240\u6709\u7279\u5f81\u5c42\u7684\u62fc\u5728\u4e00\u8d77\n        # \u4ed6\u4eec\u5c31\u662f\u5728\u7ef4\u5ea61\u4e0a\u957f\u5ea6\u4e0d\u540c\uff0c\u5c3a\u5bf8\u8d8a\u5927\u7684\u7279\u5f81\u5c42\uff0c1\u4e0a\u7684\u6570\u91cf\u8d8a\u591a\n        src_flatten = torch.cat(src_flatten, 1)\n\n        # [bs,all hw]\n        mask_flatten = torch.cat(mask_flatten, 1)\n\n        # [bs, all hw,256]\n        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)\n\n        # [\u7279\u5f81\u5c42\u7684\u6570\u91cf\uff0c2] \u5b58\u50a8\u7684\u662f\u9ad8\u5bbd\n        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src_flatten.device)\n\n        # \u5404\u4e2asrc\u5c42 \u8d77\u59cb\u7684\u4f4d\u7f6e, \u7b2c\u4e00\u4e2aspatial_shapes.new_zeros((1,))\u662f\u5728\u8d77\u59cb\u4f4d\u7f6e\u586b\u76840\n        level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))\n\n        # \u6709\u6548\u9ad8\u5bbd\u5360\u603b\u7684batch\u9ad8\u5bbd\u7684\u6bd4\u7387 [bs,4,2]\n        valid_ratios = torch.stack([self.get_valid_ratio(m) for m in masks], 1)\n\n        # src_flatten \u7c7b\u4f3cdetr\u4e2d\u7684src\uff0cmask_flatten\u5e94\u8be5\u662f\u5bf9\u5e94\u4e86src_key_padding_mask\uff0clvl_pos_embed_flatten\u5bf9\u5e94\u4e86pos\n        # \u5176\u4ed6\u7684\u4e09\u4e2a\u5b8c\u5168\u662f\u8fd9\u91cc\u591a\u51fa\u6765\u7684\u53c2\u6570\n        # encoder\n        # [bs,all hw,256]\n        memory = self.encoder(src_flatten, spatial_shapes, level_start_index,\n                              valid_ratios, lvl_pos_embed_flatten, mask_flatten)\n\n        # prepare input for decoder\n        bs, _, c = memory.shape\n\n\n        # query_embed [300,512] -&gt; [300,256] tgt [300,256]\n        query_embed, tgt = torch.split(query_embed, c, dim=1)\n\n        # \u6269\u51fa\u7b2c0\u7ef4\u5ea6\uff0cbs\n        query_embed = query_embed.unsqueeze(0).expand(bs, -1, -1)\n        # \u6269\u5145\u7b2c0\u7ef4\u5ea6\u6210\u4e3abs\n        tgt = tgt.unsqueeze(0).expand(bs, -1, -1)\n\n        # [bs,300,2]  reference_points\u4e3a\u5168\u8fde\u63a5\u5c42 512-2\uff0c\u751f\u6210\u51fa\u53c2\u8003\u70b9\u4f4d\u5750\u6807\n        # \u8fd9\u4e9b\u5750\u6807\u7ecf\u8fc7\u4e86sigmoid\u5904\u7406\uff0c\u5728\u6700\u540e\u5f97\u5230\u4fee\u6b63\u7684\u70b9\u7684\u5750\u6807\u65f6\uff0c\u8fd8\u4f1a\u4f7f\u7528sigmoid\u7684\u53cd\u51fd\u6570\n        reference_points = self.reference_points(query_embed).sigmoid()\n        # \u7ecf\u8fc7\u7f51\u7edc\u521d\u59cb\u751f\u6210\u51fa\u7684\u53c2\u8003\u70b9\u5750\u6807\n        init_reference_out = reference_points\n\n        # decoder\n        # tgt \u4ecequery_embed\u5206\u51fa\u6765\u7684 [bs,300,256]\n        # reference_points  query_embed\u7ecf\u8fc7\u5168\u8fde\u63a5\u7f51\u7edc\u751f\u6210\u7684 [bs,300,2] \u4e3a\u521d\u59cb\u7684\u53c2\u8003\u70b9\u5750\u6807\n        # memory encoder\u7684\u8f93\u51fa [bs, all hw,256]\n        # spatial_shapes \u5404\u4e2a\u7279\u5f81\u5c42\u7684\u9ad8\u5bbd [4,2]\n        # level_start_index \u5404\u4e2a\u7279\u5f81\u5c42\u7684\u8d77\u59cb\u4e0b\u6807\n        # valid_ratios [bs,4,2]\n        # query_embed [bs,300,256]\n        # mask_flatten [bs,all hw]\n        # hs [6,bs,300,256]\n        # inter_references [6,bs,300,2]\n        hs, inter_references = self.decoder(tgt, reference_points, memory,\n                                            spatial_shapes, level_start_index, valid_ratios,\n                                            query_embed, mask_flatten)\n\n        inter_references_out = inter_references\n\n        return hs, init_reference_out, inter_references_out, None, None\n</code></pre>"},{"location":"DeepLearning/GPT/","title":"GPT","text":""},{"location":"DeepLearning/GPT/#_1","title":"\u8bba\u6587\u9605\u8bfb","text":""},{"location":"DeepLearning/GPT/#gpt1","title":"GPT1","text":"<p>Improving Language Understanding by Generative Pre-Training</p> <p>\u73b0\u5728\u6ca1\u6709\u6807\u7b7e\u7684\u6570\u636e\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u6709\u76d1\u7763\u5fae\u8c03</p>"},{"location":"DeepLearning/GPT/#unsupervised-pre-training","title":"Unsupervised pre-training","text":"<p>\u7ed9\u5b9a\u8bcd\u5143\u8bed\u6599\u5e93 \\(\\mathcal{U} = \\{u_1,u_2,...,u_n\\}\\) \uff0c\u6700\u5927\u5316\u4ee5\u4e0b\u4f3c\u7136\u51fd\u6570</p> \\[     L_1(\\mathcal{U}) = \\sum_i \\log(P(u_i \\mid u_{i-k},...,u_{i-1};\\mathcal{\\Theta})) \\] <p>\u5176\u4e2d\\(k\\)\u662f\u4e0a\u4e0b\u6587\u7a97\u53e3\u5927\u5c0f\uff0c\\(\\Theta\\)\u8868\u793a\u6a21\u578b\u53c2\u6570</p> <p>\u4f7f\u7528\u7684\u6a21\u578b\u662fTransformer\u7684Decoder</p> \\[     h_0 = UW_e + W_p \\] \\[     h_l = \\text{transformerblock}(h_{l-1})  \\forall i \\in [1,n] \\] \\[     P(u) = \\text{softmax}(h_n W_e^T) \\] <p>\u5176\u4e2d\\(U = (u_{i-k},...,u_{i-1})\\)\u8868\u793a\u4e0a\u4e0b\u6587\u8bcd\u5411\u91cf\uff0c\\(n\\)\u8868\u793adecoder\u5c42\u6570\uff0c\\(W_e\\)\u662ftoken embedding\u77e9\u9635\uff0c\\(W_p\\)\u662fposition embedding\u77e9\u9635\u3002</p> <p>Transformer encoder \u548c decoder \u6700\u4e3b\u8981\u7684\u533a\u522b\u5c31\u662fdecoder\u4f7f\u7528mask\uff0cencoder\u53ef\u4ee5\u770b\u5230\u6574\u4e2a\u5e8f\u5217\u800cdecoder\u53ea\u80fd\u591f\u770b\u5230\u524d\u9762\u7684</p>"},{"location":"DeepLearning/GPT/#supervised-fine-tuning","title":"Supervised fine-tuning","text":"<p>\u7ed9\u5b9a\u4e00\u4e2a\u6709\u6807\u7b7e\u7684\u6570\u636e\u96c6\\(\\mathcal{C}\\)\uff0c\u6bcf\u4e2a\u5b9e\u4f8b\u662f\u8f93\u5165tokens\u7ec4\u6210\u7684\u53e5\u5b50\\(x^1,x^2,...,x^m\\)\u548c\u4e00\u4e2alabel \\(y\\)\uff0c\u628a\u8f93\u5165\u5e8f\u5217\u653e\u8fdbtransformer\u5757\u4e2d\u5f97\u5230\u6700\u540e\u4e00\u5c42\u4e2d\\(x_m\\)\u8fd9\u4e2a\u8bcd\u5bf9\u5e94\u7684\u8f93\u51fa\\(h_l^m\\)\u5728\u8fc7\u5168\u94fe\u63a5\u5c42\u548csoftmax</p> \\[     P(y \\mid x^1,...,x^m) = \\text{softmax}(h_l^m W_y) \\] <p>\u76ee\u6807\u51fd\u6570\u662f\u6700\u5927\u5316</p> \\[     L_2(\\mathcal{C}) = \\sum_{(x,y)}\\log(y \\mid x^1,...,x^m) \\] <p>\u4f7f\u7528\u4ee5\u4e0b\u76ee\u6807\u51fd\u6570\u80fd\u591f\u63d0\u9ad8\u6027\u80fd</p> \\[     L_3(\\mathcal{C}) = L_2(\\mathcal{C}) + \\lambda L_1(\\mathcal{{C}}) \\]"},{"location":"DeepLearning/GPT/#task-specific-input-transformations","title":"Task-specific input transformations","text":"<p>\u7b2c\u4e00\u7c7b\u662f\u5bf9\u6587\u5b57\u8fdb\u884c\u5206\u7c7b\uff0c\u628a\u60f3\u8981\u5206\u7c7b\u7684\u6587\u672c\u7684\u524d\u9762\u548c\u672b\u5c3e\u52a0\u4e00\u4e2a\u7279\u6b8a\u5b57\u7b26\uff0c\u653e\u8fdbtransformer\uff0c\u6700\u540e\u4e00\u4e2a\u5b57\u7b26(extract)\u7684\u8f93\u51fa\u653e\u8fdb\u7ebf\u6027\u5c42\u8fdb\u884c\u8bad\u7ec3\uff0c\u591a\u5206\u7c7b\u95ee\u9898\uff0c\u5206\u7c7b\u4e2a\u6570\u7531\u4efb\u52a1\u786e\u5b9a</p> <p>\u7b2c\u4e8c\u7c7b\u662f\u8574\u542b\uff0c\u7ed9\u4e24\u53e5\u8bddA\uff0cB\u662f\u5426\u6709\u8574\u542b\u5173\u7cfb\uff0c\u4f8b\u5982A\u662f1\u90012\u73ab\u7470\uff0cB\u662f1\u559c\u6b222\uff08A\u8574\u542bB\uff09\uff0cB\u662f1\u8ba8\u538c2\uff08A\u4e0d\u8574\u542bB\uff09\uff0cB\u662f1\u548c2\u662f\u90bb\u5c45\uff08AB\u6ca1\u5173\u7cfb\uff09\uff0c\u5c31\u662f\u4e00\u4e2a\u4e09\u5206\u7c7b\u7684\u95ee\u9898</p> <p>\u7b2c\u4e09\u7c7b\u662f\u76f8\u4f3c\uff0c\u5224\u65adtext1\u548ctext2\u662f\u5426\u76f8\u4f3c\uff0c\u7531\u4e8e\u8bed\u8a00\u6a21\u578b\u4e2d\u6709\u5148\u540e\u987a\u5e8f\uff0c\u6240\u4ee5\u8fd9\u91cc\u7528\u4e24\u4e2a\uff0c\u4ea4\u6362\u987a\u5e8f\uff0c\u5206\u522b\u8fdb\u5165\u6a21\u578b\u540e\u7684\u8f93\u51fa\u76f8\u52a0\u8fc7\u7ebf\u6027\u5c42\uff0c\u4e8c\u5206\u7c7b\u95ee\u9898</p> <p>\u7b2c\u56db\u7c7b\u662f\u591a\u9009\u9898\uff0c\u5bf9\u6bcf\u4e2a\u7b54\u6848\uff0c\u90fd\u628a\u95ee\u9898+\u7b54\u6848\u653e\u8fdb\u53bb\uff0c\u8f93\u51fa\u7ecf\u8fc7\u7ebf\u6027\u5c42\u5f97\u5230\u7684\u6807\u91cf\u505asoftmax\uff0c\u8ba1\u7b97\u5f97\u5230\u6bcf\u4e2a\u95ee\u9898\u7684\u662f\u6b63\u786e\u7b54\u6848\u7684\u7f6e\u4fe1\u5ea6</p>"},{"location":"DeepLearning/GPT/#gpt2","title":"GPT2","text":"<p>Language Models are Unsupervised Multitask Learners</p> <p>\u5728\u66f4\u5927\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u66f4\u5927\u7684\u6a21\u578b\uff0c\u4f46\u6548\u679c\u5e76\u6ca1\u6709\u6bd4BERT\u597d\u5f88\u591a\uff0czero-shot\u662f\u4e3b\u8981\u7684\u4f18\u52bf\uff08\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u4e0d\u9700\u8981\u6709\u6807\u6ce8\u7684\u6570\u636e\u4e5f\u4e0d\u7528\u5fae\u8c03\uff0c\u76f4\u63a5\u5c31\u80fd\u591f\u7528\uff09</p> <p>GPT2\u7684\u6a21\u578b\u548cGPT1\u76f8\u540c\uff0c\u5728GPT1\u7684\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u4e2d\u52a0\u5165\u4e86\u4e00\u4e9b\u7279\u6b8a\u7684\u7b26\u53f7\uff0c\u4f46\u662fGPT2\u5e0c\u671b\u505azero-shot\uff0c\u56e0\u6b64\u6a21\u578b\u5e76\u4e0d\u77e5\u9053\u8fd9\u4e9b\u7b26\u53f7\u6240\u4ee3\u8868\u7684\u542b\u4e49\uff0c\u56e0\u6b64\u5728\u6784\u9020\u8f93\u5165\u7684\u65f6\u5019\uff0c\u8981\u4f7f\u5f97\u8f93\u5165\u6784\u9020\u7684\u548c\u4e4b\u524d\u9884\u8bad\u7ec3\u7684\u65f6\u5019\u7684\u6587\u672c\u957f\u7684\u7c7b\u4f3c\uff0c\u4e5f\u5c31\u662f\u8f93\u5165\u7684\u5f62\u5f0f\u66f4\u50cf\u81ea\u7136\u8bed\u8a00\u3002\u4f8b\u5982\u7ffb\u8bd1\u4efb\u52a1\u7ed9\u8f93\u5165(translate to french,English text, French text)\uff08\u5c31\u662f\u540e\u9762\u8bf4\u7684prompt\uff09\u3002</p>"},{"location":"DeepLearning/GPT/#gpt3","title":"GPT3","text":"<p>Language Models are Few-Shot Learners</p> <p>\u5c1d\u8bd5\u63d0\u9ad8GPT2\u7684\u6709\u6548\u6027\uff0c\u4e0d\u8ffd\u6c42\u6781\u81f4\u7684zero-shot\uff0c\u800c\u662f\u91c7\u7528few-shot\uff0c\u4f7f\u5f97\u4e0b\u6e38\u4efb\u52a1\u7684\u8bad\u7ec3\u6210\u672c\u8f83\u5c0f</p> <p>\u6a21\u578b\u5927\u5c0f\u4e3a175B\uff0c\u5728\u8fd9\u4e48\u5927\u6570\u91cf\u7684\u53c2\u6570\u4e0b\u505a\u5fae\u8c03\u7684\u6210\u672c\u5f88\u9ad8\uff0c\u56e0\u6b64\u5728\u505a\u5b50\u4efb\u52a1\u4e5f\u4e0d\u505a\u4efb\u4f55\u7684\u68af\u5ea6\u66f4\u65b0\uff0c\u800c\u662f\u8f93\u5165\u63d0\u4f9b\u6837\u4f8b\u3002</p> <p></p>"},{"location":"DeepLearning/MLP/","title":"MLP","text":"<p>dive into deep learning \u591a\u5c42\u611f\u77e5\u673a\u7ae0\u8282</p> <pre><code>num_inputs, num_outputs, num_hiddens = 784, 10, 256\n\nW1 = nn.Parameter(torch.randn(\n    num_inputs, num_hiddens, requires_grad=True) * 0.01)\nb1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\nW2 = nn.Parameter(torch.randn(\n    num_hiddens, num_outputs, requires_grad=True) * 0.01)\nb2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\n\nparams = [W1, b1, W2, b2]\n\n\ndef relu(X):\n    a = torch.zeros_like(X)\n    return torch.max(X, a)\n\ndef net(X):\n    X = X.reshape((-1, num_inputs))\n    H = relu(X@W1 + b1)  # \u8fd9\u91cc\u201c@\u201d\u4ee3\u8868\u77e9\u9635\u4e58\u6cd5\n    return (H@W2 + b2)\n</code></pre> <p>\u7528torch.nn</p> <pre><code>net = nn.Sequential(nn.Flatten(),\n                    nn.Linear(784, 256),\n                    nn.ReLU(),\n                    nn.Linear(256, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights);\n</code></pre> <p><code>X = X.reshape((-1, num_inputs))</code> -- <code>nn.Flatten()</code></p> <p><code>H = relu(X@W1 + b1)</code>--<code>nn.Linear(784, 256)</code>\u548c<code>nn.ReLu()</code></p> <p><code>(H@W2 + b2)</code>--<code>nn.Linear(256, 10)</code></p>"},{"location":"DeepLearning/MLP/#_1","title":"\u8fc7\u62df\u5408","text":""},{"location":"DeepLearning/MLP/#_2","title":"\u6b63\u5219\u5316","text":"\\[ \\begin{aligned} L(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 \\end{aligned} \\] \\[ \\begin{aligned} \\mathbf{w} &amp; \\leftarrow \\left(1- \\eta\\lambda \\right) \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned} \\]"},{"location":"DeepLearning/MLP/#drop-out","title":"Drop out","text":"<p>$$ \\begin{aligned} h' = \\begin{cases}     0 &amp; \\text{ \u6982\u7387\u4e3a } p \\     \\frac{h}{1-p} &amp; \\text{ \u5176\u4ed6\u60c5\u51b5} \\end{cases} \\end{aligned} $$ \u671f\u671b\u503c\u4fdd\u6301\u4e0d\u53d8\uff0c\u5373\\(E[h'] = h\\)\u3002</p> <pre><code>def dropout_layer(X, dropout):\n    assert 0 &lt;= dropout &lt;= 1\n    # \u5728\u672c\u60c5\u51b5\u4e2d\uff0c\u6240\u6709\u5143\u7d20\u90fd\u88ab\u4e22\u5f03\n    if dropout == 1:\n        return torch.zeros_like(X)\n    # \u5728\u672c\u60c5\u51b5\u4e2d\uff0c\u6240\u6709\u5143\u7d20\u90fd\u88ab\u4fdd\u7559\n    if dropout == 0:\n        return X\n    mask = (torch.rand(X.shape) &gt; dropout).float()\n    return mask * X / (1.0 - dropout)\n</code></pre> <pre><code>dropout1, dropout2 = 0.2, 0.5\n\nclass Net(nn.Module):\n    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,\n                 is_training = True):\n        super(Net, self).__init__()\n        self.num_inputs = num_inputs\n        self.training = is_training\n        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\n        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\n        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\n        # \u53ea\u6709\u5728\u8bad\u7ec3\u6a21\u578b\u65f6\u624d\u4f7f\u7528dropout\n        if self.training == True:\n            # \u5728\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42\n            H1 = dropout_layer(H1, dropout1)\n        H2 = self.relu(self.lin2(H1))\n        if self.training == True:\n            # \u5728\u7b2c\u4e8c\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42\n            H2 = dropout_layer(H2, dropout2)\n        out = self.lin3(H2)\n        return out\n\n\nnet = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)\n</code></pre> <p>\u76f4\u63a5\u7528<code>nn.Dropout(p)</code></p> <pre><code>net = nn.Sequential(nn.Flatten(),\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        # \u5728\u7b2c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42\n        nn.Dropout(dropout1),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n        # \u5728\u7b2c\u4e8c\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u540e\u6dfb\u52a0\u4e00\u4e2adropout\u5c42\n        nn.Dropout(dropout2),\n        nn.Linear(256, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights);\n</code></pre>"},{"location":"DeepLearning/MLP/#_3","title":"\u53c2\u6570\u521d\u59cb\u5316","text":""},{"location":"DeepLearning/MLP/#xavier","title":"Xavier\u521d\u59cb\u5316","text":"\\[o_{i} = \\sum_{j=1}^{n_\\mathrm{in}} w_{ij} x_j.\\] \\[ \\begin{aligned}     E[o_i] &amp; = \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij} x_j] \\\\&amp;= \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij}] E[x_j] \\\\&amp;= 0, \\\\     \\mathrm{Var}[o_i] &amp; = E[o_i^2] - (E[o_i])^2 \\\\         &amp; = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\\\         &amp; = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\\\         &amp; = n_\\mathrm{in} \\sigma^2 \\gamma^2. \\end{aligned} \\] <p>\u4f7f\u65b9\u5dee\u6ee1\u8db3</p> \\[ \\begin{aligned} \\frac{1}{2} (n_\\mathrm{in} + n_\\mathrm{out}) \\sigma^2 = 1 \\text{ or } \\sigma = \\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}. \\end{aligned} \\] <p>Xavier\u521d\u59cb\u5316\u901a\u5e38\u4ece\u65b9\u5dee\u6ee1\u8db3\u4e0a\u8ff0\u7684\u5747\u5300\u5206\u5e03\u6216\u9ad8\u65af\u5206\u5e03\u4e2d\u91c7\u6837\u6743\u91cd</p>"},{"location":"DeepLearning/PyTorch/","title":"PyTorch","text":"<p>\u5b98\u65b9\u6587\u6863</p> <p>\u5e38\u7528API</p> <ul> <li>torch.split</li> </ul>"},{"location":"DeepLearning/PyTorch/#pytorch_1","title":"PyTorch\u57fa\u7840","text":""},{"location":"DeepLearning/PyTorch/#tensor","title":"Tensor","text":""},{"location":"DeepLearning/PyTorch/#tensor_1","title":"tensor\u51fd\u6570","text":"<ul> <li>cat\u548cstack \u53c2\u8003\u535a\u5ba2</li> <li>\u6279\u91cf\u77e9\u9635\u4e58\u6cd5 torch.bmm(X,Y),X\u7684shape\u4e3a(n,a,b)\uff0cY\u7684shape\u4e3a(n,b,c)\uff0c\u8f93\u51fa\u5f62\u72b6(n,a,c)</li> <li>torch.unsqueeze() \u5728\u6307\u5b9a\u7eac\u5ea6\u63d2\u5165\u7eac\u5ea61</li> <li>X.shape = (2,3) X.unsqueeze(0).shape = (1\uff0c2\uff0c3) X.unsqueeze(1).shape = (2,1,3)</li> <li>view \u548c reshape \u53c2\u8003\u535a\u5ba2</li> <li>contiguous \u53c2\u8003\u535a\u5ba2</li> </ul>"},{"location":"DeepLearning/PyTorch/#broadcast","title":"broadcast","text":"<p>broadcast\u7684\u6761\u4ef6\uff1a</p> <ol> <li>\u6bcf\u4e2aTensor\u81f3\u5c11\u6709\u4e00\u4e2a\u7ef4\u5ea6</li> <li>\u8fed\u4ee3\u7ef4\u5ea6\u5c3a\u5bf8\u65f6\uff0c\u4ece\u5c3e\u90e8\u5f00\u59cb\uff0c\u4f9d\u6b21\u6bcf\u4e2a\u7ef4\u5ea6\u5c3a\u5bf8\u5fc5\u987b\u6ee1\u8db3\u4e00\u4e0b\u6761\u4ef6\u4e4b\u4e00\uff1a</li> <li>\u76f8\u7b49</li> <li>\u5176\u4e2d\u4e00\u4e2atensor\u7684\u7ef4\u5ea6\u4e3a1</li> <li>\u5176\u4e2d\u4e00\u4e2atensor\u7684\u7ef4\u5ea6\u4e0d\u5b58\u5728</li> </ol> <p>code\u793a\u4f8b</p>"},{"location":"DeepLearning/PyTorch/#pytorch_2","title":"PyTorch\u81ea\u52a8\u6c42\u5bfc","text":"<p>\u6df1\u5165\u6d45\u51faPytroch \u7b2c\u4e8c\u7ae0</p>"},{"location":"DeepLearning/PyTorch/#_1","title":"\u8ba1\u7b97\u56fe","text":"<p>\u5c06\u4ee3\u7801\u5206\u89e3\u6210\u64cd\u4f5c\u5b50\uff0c\u5c06\u8ba1\u7b97\u8868\u793a\u4e3a\u4e00\u4e2a\u65e0\u73af\u56fe \u4e00\u822c\u53ea\u6709leaf node\u6709grad</p> <p>\u6700\u540e\u7684\u8f93\u51fa\u770b\u6210\u5173\u4e8e\u7f51\u7edc\u6743\u91cd\u7684\u51fd\u6570\uff0cbackward\u51fd\u6570\u8ba1\u7b97\u51fa\u6743\u91cd\u7684\u68af\u5ea6\uff08\u5168\u5fae\u5206\uff09</p> <p>\u5bf9\u4e8eleaf\u548crequire_grad\u7684\u8282\u70b9\u4e0d\u80fd\u591f\u8fdb\u884cinplace operation</p> <p>retain_graph \u9632\u6b62backward\u4e4b\u540e\u91ca\u653e\u76f8\u5173\u5185\u5b58</p> <p>.detach return a new tensor ,detached from the current graph,the result will never require gradient \u5c06\u8ba1\u7b97\u79fb\u52a8\u5230\u8ba1\u7b97\u56fe\u4e4b\u5916\uff0c\u5f53\u4f5c\u5e38\u6570</p>"},{"location":"DeepLearning/PyTorch/#_2","title":"\u6570\u636e\u8bfb\u53d6","text":""},{"location":"DeepLearning/PyTorch/#_3","title":"\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d","text":"<p>\u968f\u673a\u91c7\u6837b\u4e2a\u6837\u672c \\(i_1,i_2,...,i_b\\)\u6765\u8fd1\u4f3c\u635f\u5931 $$     \\frac{1}{b}\\sum_{i \\in I_b}l(\\hat y_i,y_i) $$ \u4e00\u6b21\u8fed\u4ee3\u7528b\u4e2a\u6570\u636e\u8ba1\u7b97\u540e\u66f4\u65b0\u53c2\u6570 \u4e00\u4e2aepoch\u5c06\u6570\u636e\u96c6\u7684\u6570\u636e\u90fd\u7528\u4e00\u904d</p>"},{"location":"DeepLearning/PyTorch/#data-iterator","title":"data iterator","text":"<p>\u4e00\u6b21\u968f\u673a\u8bfb\u53d6batch_size\u4e2a\u6570\u636e</p> <pre><code>def data_iter(batch_size, features, labels):\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)\n    for i in range(0, num_examples, batch_size):\n        batch_indices = torch.tensor(\n            indices[i: min(i + batch_size, num_examples)])\n        yield features[batch_indices], labels[batch_indices]\n</code></pre>"},{"location":"DeepLearning/PyTorch/#tools","title":"Tools","text":""},{"location":"DeepLearning/PyTorch/#torchsummay","title":"torchsummay","text":"<p>\u53ef\u4ee5\u7528\u6765\u6253\u5370\u7f51\u7edc\u7684\u7ed3\u6784\u53c2\u6570</p> <p>\u7528\u6cd5\u793a\u4f8b</p> <pre><code>from torchsummary import summary\nfrom torchvision.models import resnet18\n\nmodel = resnet18()\nsummary(model, input_size=[(3, 256, 256)], batch_size=2, device=\"cpu\")\n</code></pre> <pre><code>----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1          [2, 64, 128, 128]           9,408\n       BatchNorm2d-2          [2, 64, 128, 128]             128\n              ReLU-3          [2, 64, 128, 128]               0\n         MaxPool2d-4            [2, 64, 64, 64]               0\n            Conv2d-5            [2, 64, 64, 64]          36,864\n       BatchNorm2d-6            [2, 64, 64, 64]             128\n              ReLU-7            [2, 64, 64, 64]               0\n            Conv2d-8            [2, 64, 64, 64]          36,864\n       BatchNorm2d-9            [2, 64, 64, 64]             128\n             ReLU-10            [2, 64, 64, 64]               0\n       BasicBlock-11            [2, 64, 64, 64]               0\n           Conv2d-12            [2, 64, 64, 64]          36,864\n      BatchNorm2d-13            [2, 64, 64, 64]             128\n             ReLU-14            [2, 64, 64, 64]               0\n           Conv2d-15            [2, 64, 64, 64]          36,864\n      BatchNorm2d-16            [2, 64, 64, 64]             128\n             ReLU-17            [2, 64, 64, 64]               0\n       BasicBlock-18            [2, 64, 64, 64]               0\n           Conv2d-19           [2, 128, 32, 32]          73,728\n      BatchNorm2d-20           [2, 128, 32, 32]             256\n             ReLU-21           [2, 128, 32, 32]               0\n           Conv2d-22           [2, 128, 32, 32]         147,456\n...\nForward/backward pass size (MB): 164.02\nParams size (MB): 44.59\nEstimated Total Size (MB): 210.12\n----------------------------------------------------------------\n</code></pre>"},{"location":"DeepLearning/PyTorch/#multiple-gpus","title":"Multiple GPUs","text":""},{"location":"DeepLearning/PyTorch/#_4","title":"\u62c6\u5206\u6570\u636e","text":"<p>\u8fd9\u79cd\u65b9\u5f0f\u4e0b\uff0c\u6240\u6709GPU\u5c3d\u7ba1\u6709\u4e0d\u540c\u7684\u89c2\u6d4b\u7ed3\u679c\uff0c\u4f46\u662f\u6267\u884c\u7740\u76f8\u540c\u7c7b\u578b\u7684\u5de5\u4f5c\u3002\u5728\u5b8c\u6210\u6bcf\u4e2a\u5c0f\u6279\u91cf\u6570\u636e\u7684\u8bad\u7ec3\u4e4b\u540e\uff0c\u68af\u5ea6\u5728GPU\u4e0a\u805a\u5408\u3002 GPU\u7684\u6570\u91cf\u8d8a\u591a\uff0c\u5c0f\u6279\u91cf\u5305\u542b\u7684\u6570\u636e\u91cf\u5c31\u8d8a\u5927\uff0c\u4ece\u800c\u5c31\u80fd\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002 \u7f3a\u70b9\uff1a\u4e0d\u80fd\u591f\u8bad\u7ec3\u66f4\u5927\u7684\u6a21\u578b</p> <p>\\(k\\)\u4e2aGPU\u5e76\u884c\u8bad\u7ec3\u8fc7\u7a0b\u5982\u4e0b\uff1a \u5728\u4efb\u4f55\u4e00\u6b21\u8bad\u7ec3\u8fed\u4ee3\u4e2d\uff0c\u7ed9\u5b9a\u7684\u968f\u673a\u7684\u5c0f\u6279\u91cf\u6837\u672c\u90fd\u5c06\u88ab\u5206\u6210\\(k\\)\u4e2a\u90e8\u5206\uff0c\u5e76\u5747\u5300\u5730\u5206\u914d\u5230GPU\u4e0a\uff1b \u6bcf\u4e2aGPU\u6839\u636e\u5206\u914d\u7ed9\u5b83\u7684\u5c0f\u6279\u91cf\u5b50\u96c6\uff0c\u8ba1\u7b97\u6a21\u578b\u53c2\u6570\u7684\u635f\u5931\u548c\u68af\u5ea6\uff1b \u5c06\\(k\\)\u4e2aGPU\u4e2d\u7684\u5c40\u90e8\u68af\u5ea6\u805a\u5408\uff0c\u4ee5\u83b7\u5f97\u5f53\u524d\u5c0f\u6279\u91cf\u7684\u968f\u673a\u68af\u5ea6\uff1b \u805a\u5408\u68af\u5ea6\u88ab\u91cd\u65b0\u5206\u53d1\u5230\u6bcf\u4e2aGPU\u4e2d\uff1b *\u6bcf\u4e2aGPU\u4f7f\u7528\u8fd9\u4e2a\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\uff0c\u6765\u66f4\u65b0\u5b83\u6240\u7ef4\u62a4\u7684\u5b8c\u6574\u7684\u6a21\u578b\u53c2\u6570\u96c6\u3002</p> <p>\u5411\u591a\u4e2a\u8bbe\u5907\u590d\u5236\u53c2\u6570</p> <pre><code>def get_params(params, device):\n    new_params = [p.to(device) for p in params]\n    for p in new_params:\n        p.requires_grad_()\n    return new_params\n</code></pre> <p>\u80fd\u591f\u5c06\u6240\u6709\u8bbe\u5907\u4e0a\u7684\u68af\u5ea6\u8fdb\u884c\u76f8\u52a0</p> <pre><code>def allreduce(data):\n    for i in range(1, len(data)):\n        data[0][:] += data[i].to(data[0].device)\n    for i in range(1, len(data)):\n        data[i][:] = data[0].to(data[i].device)\n</code></pre> <p>\u5206\u53d1\u6570\u636e <code>nn.parallel.scatter(data, devices)</code></p> <pre><code>data = torch.arange(20).reshape(4, 5)\ndevices = [torch.device('cuda:0'), torch.device('cuda:1')]\nsplit = nn.parallel.scatter(data, devices)\nprint('input :', data)\nprint('load into', devices)\nprint('output:', split)\n</code></pre> <pre><code>input : tensor([ [ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19] ])\nload into [device(type='cuda', index=0), device(type='cuda', index=1)]\noutput: (tensor([ [0, 1, 2, 3, 4],\n        [5, 6, 7, 8, 9] ], device='cuda:0'), tensor([ [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19] ], device='cuda:1'))\n</code></pre> <p>\u5206\u53d1\u6570\u636e\u548c\u6807\u7b7e</p> <pre><code>def split_batch(X, y, devices):\n    assert X.shape[0] == y.shape[0]\n    return (nn.parallel.scatter(X, devices),\n            nn.parallel.scatter(y, devices))\n</code></pre> <p>\u5b9e\u73b0\u591aGPU\u8bad\u7ec3</p> <pre><code>def train_batch(X, y, device_params, devices, lr):\n    X_shards, y_shards = split_batch(X, y, devices)\n    # \u5728\u6bcf\u4e2aGPU\u4e0a\u5206\u522b\u8ba1\u7b97\u635f\u5931\n    ls = [loss(lenet(X_shard, device_W), y_shard).sum()\n          for X_shard, y_shard, device_W in zip(\n              X_shards, y_shards, device_params)]\n    for l in ls:  # \u53cd\u5411\u4f20\u64ad\u5728\u6bcf\u4e2aGPU\u4e0a\u5206\u522b\u6267\u884c\n        l.backward()\n    # \u5c06\u6bcf\u4e2aGPU\u7684\u6240\u6709\u68af\u5ea6\u76f8\u52a0\uff0c\u5e76\u5c06\u5176\u5e7f\u64ad\u5230\u6240\u6709GPU\n    with torch.no_grad():\n        for i in range(len(device_params[0])):\n            allreduce([device_params[c][i].grad for c in range(len(devices))])\n    # \u5728\u6bcf\u4e2aGPU\u4e0a\u5206\u522b\u66f4\u65b0\u6a21\u578b\u53c2\u6570\n    for param in device_params:\n        d2l.sgd(param, lr, X.shape[0]) # \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4f7f\u7528\u5168\u5c3a\u5bf8\u7684\u5c0f\u6279\u91cf\n</code></pre> <p>\u8bc4\u4f30\u6a21\u578b\u7684\u65f6\u5019\u53ea\u5728\u4e00\u4e2aGPU\u4e0a</p> <pre><code>def train(num_gpus, batch_size, lr):\n    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n    # \u5c06\u6a21\u578b\u53c2\u6570\u590d\u5236\u5230num_gpus\u4e2aGPU\n    device_params = [get_params(params, d) for d in devices]\n    num_epochs = 10\n    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n    timer = d2l.Timer()\n    for epoch in range(num_epochs):\n        timer.start()\n        for X, y in train_iter:\n            # \u4e3a\u5355\u4e2a\u5c0f\u6279\u91cf\u6267\u884c\u591aGPU\u8bad\u7ec3\n            train_batch(X, y, device_params, devices, lr)\n            torch.cuda.synchronize()\n        timer.stop()\n        # \u5728GPU0\u4e0a\u8bc4\u4f30\u6a21\u578b\n        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(\n            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))\n    print(f'\u6d4b\u8bd5\u7cbe\u5ea6\uff1a{animator.Y[0][-1]:.2f}\uff0c{timer.avg():.1f}\u79d2/\u8f6e\uff0c'\n          f'\u5728{str(devices)}')\n</code></pre> <p>\u7b80\u6d01\u5b9e\u73b0<code>net = nn.DataParallel(net, device_ids=devices)</code>\uff0c\u548c\u4e4b\u524d\u51e0\u4e4e\u6ca1\u4ec0\u4e48\u533a\u522b</p> <pre><code>def train(net, num_gpus, batch_size, lr):\n    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n    def init_weights(m):\n        if type(m) in [nn.Linear, nn.Conv2d]:\n            nn.init.normal_(m.weight, std=0.01)\n    net.apply(init_weights)\n    # \u5728\u591a\u4e2aGPU\u4e0a\u8bbe\u7f6e\u6a21\u578b\n    net = nn.DataParallel(net, device_ids=devices)\n    trainer = torch.optim.SGD(net.parameters(), lr)\n    loss = nn.CrossEntropyLoss()\n    timer, num_epochs = d2l.Timer(), 10\n    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n    for epoch in range(num_epochs):\n        net.train()\n        timer.start()\n        for X, y in train_iter:\n            trainer.zero_grad()\n            X, y = X.to(devices[0]), y.to(devices[0])\n            l = loss(net(X), y)\n            l.backward()\n            trainer.step()\n        timer.stop()\n        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))\n    print(f'\u6d4b\u8bd5\u7cbe\u5ea6\uff1a{animator.Y[0][-1]:.2f}\uff0c{timer.avg():.1f}\u79d2/\u8f6e\uff0c'\n          f'\u5728{str(devices)}')\n</code></pre>"},{"location":"DeepLearning/RNN/","title":"RNN","text":""},{"location":"DeepLearning/RNN/#_1","title":"\u5e8f\u5217\u6a21\u578b","text":"<p>\u901a\u8fc7\\(\\mathbf{x}_t = [x_{t-\\tau}, \\ldots, x_{t-1}]\\) \u7684\u53d6\u503c\uff0c\u9884\u6d4b\\(y_t = x_t\\)\u3002</p>"},{"location":"DeepLearning/RNN/#_2","title":"\u6587\u672c\u9884\u5904\u7406","text":"<ol> <li>\u8bfb\u53d6\u6570\u636e\u96c6\u5f97\u5230\u6240\u6709\u6587\u672c\u884c\uff08lines\uff09 <code>the time machine by h g wells</code></li> <li>\u8bcd\u5143\u5316\uff08tokenize\uff09 \u628alines\u5206\u6210\u4e00\u4e2a\u4e00\u4e2a\u8bcd <code>['the', 'time', 'machine', 'by', 'h', 'g', 'wells']</code></li> <li>\u5efa\u7acb\u8bcd\u8868\uff08vocabulary\uff09</li> </ol> <pre><code>class Vocab:  \n    \"\"\"\u6587\u672c\u8bcd\u8868\"\"\"\n    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n        if tokens is None:\n            tokens = []\n        if reserved_tokens is None:\n            reserved_tokens = []\n        # \u6309\u51fa\u73b0\u9891\u7387\u6392\u5e8f\n        counter = count_corpus(tokens)\n        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n                                   reverse=True)\n        # \u672a\u77e5\u8bcd\u5143\u7684\u7d22\u5f15\u4e3a0\n        self.idx_to_token = ['&lt;unk&gt;'] + reserved_tokens\n        self.token_to_idx = {token: idx\n                             for idx, token in enumerate(self.idx_to_token)}\n        for token, freq in self._token_freqs:\n            if freq &lt; min_freq:\n                break\n            if token not in self.token_to_idx:\n                self.idx_to_token.append(token)\n                self.token_to_idx[token] = len(self.idx_to_token) - 1\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):#\u7ed9token\u8fd4\u56deindex\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):#\u7ed9index\u8fd4\u56detoken\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n\n    @property\n    def unk(self):  # \u672a\u77e5\u8bcd\u5143\u7684\u7d22\u5f15\u4e3a0\n        return 0\n\n    @property\n    def token_freqs(self):\n        return self._token_freqs\n\ndef count_corpus(tokens):  #@save\n    \"\"\"\u7edf\u8ba1\u8bcd\u5143\u7684\u9891\u7387\"\"\"\n    # \u8fd9\u91cc\u7684tokens\u662f1D\u5217\u8868\u62162D\u5217\u8868\n    if len(tokens) == 0 or isinstance(tokens[0], list):\n        # \u5c06\u8bcd\u5143\u5217\u8868\u5c55\u5e73\u6210\u4e00\u4e2a\u5217\u8868\n        tokens = [token for line in tokens for token in line]\n    return collections.Counter(tokens)\n</code></pre> <p>\u8f6c\u6362\u7ed3\u679c\uff08\u8bcd\u9891\u9ad8\u4e0b\u6807\u5c0f\uff09</p> <pre><code>vocab = Vocab(tokens)\nprint(list(vocab.token_to_idx.items())[:10])\n</code></pre> <pre><code>[('&lt;unk&gt;', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n</code></pre>"},{"location":"DeepLearning/RNN/#language-model","title":"Language Model","text":"<p>n\u5143\u8bed\u6cd5</p> \\[ \\begin{aligned} P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2) P(x_3) P(x_4),\\\\ P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \\mid  x_1) P(x_3  \\mid  x_2) P(x_4  \\mid  x_3),\\\\ P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \\mid  x_1) P(x_3  \\mid  x_1, x_2) P(x_4  \\mid  x_2, x_3). \\end{aligned} \\] <p>\u5f97\u5230\u4e8c\u5143\u8bcd</p> <pre><code>bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]\nbigram_vocab = d2l.Vocab(bigram_tokens)\n</code></pre> <p>\u7b2c\\(i\\)\u4e2a\u6700\u5e38\u7528\u7684\u9891\u7387 \\(n_i\\)</p> \\[\\log n_i = -\\alpha \\log i + c\\] <p>\u867d\u7136n\u5143\u8bcd\u7ec4\u5408\u65b9\u5f0f\u589e\u52a0\uff0c\u4f46\u662f\u7531\u4e8e\u5927\u81f4\u9075\u5faa\u4e0a\u8ff0\u89c4\u5f8b\uff0c\u5c06\u8bcd\u9891\u5c0f\u4e8e\u67d0\u4e2a\u503c\u7684\u7565\u53bb\uff0c\u8bcd\u8868\u4e2d\u8bb0\u5f55\u53cd\u800c\u66f4\u5c11</p>"},{"location":"DeepLearning/RNN/#_3","title":"\u8bfb\u53d6\u957f\u5e8f\u5217\u6570\u636e","text":""},{"location":"DeepLearning/RNN/#_4","title":"\u968f\u673a\u91c7\u6837","text":"<pre><code>def seq_data_iter_random(corpus, batch_size, num_steps):\n    \"\"\"\u4f7f\u7528\u968f\u673a\u62bd\u6837\u751f\u6210\u4e00\u4e2a\u5c0f\u6279\u91cf\u5b50\u5e8f\u5217\"\"\"\n    # \u4ece\u968f\u673a\u504f\u79fb\u91cf\u5f00\u59cb\u5bf9\u5e8f\u5217\u8fdb\u884c\u5206\u533a\uff0c\u968f\u673a\u8303\u56f4\u5305\u62ecnum_steps-1\n    corpus = corpus[random.randint(0, num_steps - 1):]\n    # \u51cf\u53bb1\uff0c\u662f\u56e0\u4e3a\u6211\u4eec\u9700\u8981\u8003\u8651\u6807\u7b7e\n    num_subseqs = (len(corpus) - 1) // num_steps\n    # \u957f\u5ea6\u4e3anum_steps\u7684\u5b50\u5e8f\u5217\u7684\u8d77\u59cb\u7d22\u5f15\n    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n    # \u5728\u968f\u673a\u62bd\u6837\u7684\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\uff0c\n    # \u6765\u81ea\u4e24\u4e2a\u76f8\u90bb\u7684\u3001\u968f\u673a\u7684\u3001\u5c0f\u6279\u91cf\u4e2d\u7684\u5b50\u5e8f\u5217\u4e0d\u4e00\u5b9a\u5728\u539f\u59cb\u5e8f\u5217\u4e0a\u76f8\u90bb\n    random.shuffle(initial_indices)\n\n    def data(pos):\n        # \u8fd4\u56de\u4ecepos\u4f4d\u7f6e\u5f00\u59cb\u7684\u957f\u5ea6\u4e3anum_steps\u7684\u5e8f\u5217\n        return corpus[pos: pos + num_steps]\n\n    num_batches = num_subseqs // batch_size\n    for i in range(0, batch_size * num_batches, batch_size):\n        # \u5728\u8fd9\u91cc\uff0cinitial_indices\u5305\u542b\u5b50\u5e8f\u5217\u7684\u968f\u673a\u8d77\u59cb\u7d22\u5f15\n        initial_indices_per_batch = initial_indices[i: i + batch_size]\n        X = [data(j) for j in initial_indices_per_batch]\n        Y = [data(j + 1) for j in initial_indices_per_batch]\n        yield torch.tensor(X), torch.tensor(Y)\n</code></pre> <p>\u751f\u6210\u6837\u4f8b(0-34\u5e8f\u5217,batch_size=2,num_steps=5)</p> <pre><code>X:  tensor([ [13, 14, 15, 16, 17],\n        [28, 29, 30, 31, 32] ])\nY: tensor([ [14, 15, 16, 17, 18],\n        [29, 30, 31, 32, 33] ])\nX:  tensor([ [ 3,  4,  5,  6,  7],\n        [18, 19, 20, 21, 22] ])\nY: tensor([ [ 4,  5,  6,  7,  8],\n        [19, 20, 21, 22, 23] ])\nX:  tensor([ [ 8,  9, 10, 11, 12],\n        [23, 24, 25, 26, 27] ])\nY: tensor([ [ 9, 10, 11, 12, 13],\n        [24, 25, 26, 27, 28] ])\n</code></pre>"},{"location":"DeepLearning/RNN/#_5","title":"\u987a\u5e8f\u91c7\u6837","text":"<pre><code>def seq_data_iter_sequential(corpus, batch_size, num_steps):\n    \"\"\"\u4f7f\u7528\u987a\u5e8f\u5206\u533a\u751f\u6210\u4e00\u4e2a\u5c0f\u6279\u91cf\u5b50\u5e8f\u5217\"\"\"\n    # \u4ece\u968f\u673a\u504f\u79fb\u91cf\u5f00\u59cb\u5212\u5206\u5e8f\u5217\n    offset = random.randint(0, num_steps)\n    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n    num_batches = Xs.shape[1] // num_steps\n    for i in range(0, num_steps * num_batches, num_steps):\n        X = Xs[:, i: i + num_steps]\n        Y = Ys[:, i: i + num_steps]\n        yield X, Y\n</code></pre> <p>\u751f\u6210\u6837\u4f8b</p> <pre><code>X:  tensor([ [ 0,  1,  2,  3,  4],\n        [17, 18, 19, 20, 21] ])\nY: tensor([ [ 1,  2,  3,  4,  5],\n        [18, 19, 20, 21, 22] ])\nX:  tensor([ [ 5,  6,  7,  8,  9],\n        [22, 23, 24, 25, 26] ])\nY: tensor([ [ 6,  7,  8,  9, 10],\n        [23, 24, 25, 26, 27] ])\nX:  tensor([ [10, 11, 12, 13, 14],\n        [27, 28, 29, 30, 31] ])\nY: tensor([ [11, 12, 13, 14, 15],\n        [28, 29, 30, 31, 32] ])\n</code></pre>"},{"location":"DeepLearning/RNN/#rnn_1","title":"RNN\u5b9e\u73b0","text":"<p>\u66f4\u65b0\u9690\u85cf\u72b6\u6001</p> \\[     h_t = \\phi(W_{hh} h_{t-1} + W_{hx}\\mathbf{x}_{t-1}+b_h) \\] <p>\u8f93\u51fa</p> \\[     o_t = \\phi(W_{ho} h_t + b_o) \\] <p>\u56f0\u60d1\u5ea6(perplexity)\uff1a \u8861\u91cf\u8bed\u8a00\u6a21\u578b\u7684\u597d\u574f\u53ef\u4ee5\u7528\u5e73\u5747\u4ea4\u53c9\u71b5</p> \\[\\pi = \\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\] <p>\u56f0\u60d1\u5ea6\u4e3a\\(\\exp(\\pi)\\)</p> <p>\u68af\u5ea6\u526a\u88c1\uff1a \u9884\u9632\u68af\u5ea6\u7206\u70b8 \u68af\u5ea6\u957f\u5ea6\u8d85\u8fc7\\(\\theta\\)\uff0c\u90a3\u4e48\u62d6\u56de\u957f\u5ea6\\(\\theta\\)</p> \\[     g \\leftarrow \\min(1,\\frac{\\theta}{\\Vert \\mathbf{g} \\Vert}) \\mathbf{g} \\] <p>\u53c2\u6570\u521d\u59cb\u5316:</p> <pre><code>def get_params(vocab_size, num_hiddens, device):\n    num_inputs = num_outputs = vocab_size\n    #\u56e0\u4e3a\u8fd9\u91cc\u7528\u7684onehot\u7801\n    def normal(shape):\n        return torch.randn(size=shape, device=device) * 0.01\n\n    # \u9690\u85cf\u5c42\u53c2\u6570\n    W_xh = normal((num_inputs, num_hiddens))\n    W_hh = normal((num_hiddens, num_hiddens))\n    b_h = torch.zeros(num_hiddens, device=device)\n    # \u8f93\u51fa\u5c42\u53c2\u6570\n    W_hq = normal((num_hiddens, num_outputs))\n    b_q = torch.zeros(num_outputs, device=device)\n    # \u9644\u52a0\u68af\u5ea6\n    params = [W_xh, W_hh, b_h, W_hq, b_q]\n    for param in params:\n        param.requires_grad_(True)\n    return params\n\n#\u521d\u59cb\u5316\u9690\u85cf\u72b6\u6001\ndef init_rnn_state(batch_size, num_hiddens, device):\n    return (torch.zeros((batch_size, num_hiddens), device=device), )\n</code></pre> <p>\\(X\\)\u7684\u5f62\u72b6\u4e3a\uff08batch_size,vocab_size\uff09\uff0c\u6267\u884c\u64cd\u4f5c</p> \\[     H_{b \\times h} = \\phi(X_{b \\times x}W_{x \\times h} + H_{b \\times h}W_{h \\times h } + b_h(Broadcast)) \\] <p>\u53ef\u4ee5\u770b\u51fa\u5bf9\u4e8e\u6bcf\u4e2a\u6279\u91cf\u7684\u9690\u72b6\u6001\u662f\u72ec\u7acb\u5b58\u50a8\u66f4\u65b0\u7684</p> <p>\u8f93\u51fa\\(Y\\)\u7684\u5f62\u72b6\u4e3a(batch_size,vocab_size)\uff0ccat\u5b8c\u4e86return\u7684\u5f62\u72b6\u4e3a(time_steps\\(\\times\\)batch_size,vocab_size)</p> <pre><code>def rnn(inputs, state, params):\n    # inputs\u7684\u5f62\u72b6\uff1a(\u65f6\u95f4\u6b65\u6570\u91cf\uff0c\u6279\u91cf\u5927\u5c0f\uff0c\u8bcd\u8868\u5927\u5c0f)\n    W_xh, W_hh, b_h, W_hq, b_q = params\n    H, = state\n    outputs = []\n    # X\u7684\u5f62\u72b6\uff1a(\u6279\u91cf\u5927\u5c0f\uff0c\u8bcd\u8868\u5927\u5c0f)\n    for X in inputs:\n        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh)+b_h)#torch.mm\u8868\u793a\u77e9\u9635\u4e58\u8d77\u6765\n        Y = torch.mm(H, W_hq) + b_q\n        outputs.append(Y)\n    return torch.cat(outputs, dim=0), (H,)\n</code></pre> <p>\u5b8c\u6574\u7684\u5982\u4e0b</p> <pre><code>class RNNModelScratch:\n    def __init__(self, vocab_size, num_hiddens, device,\n                 get_params, init_state, forward_fn):\n        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n        self.params = get_params(vocab_size, num_hiddens, device)\n        self.init_state, self.forward_fn = init_state, forward_fn\n\n    def __call__(self, X, state):\n        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n        return self.forward_fn(X, state, self.params)\n\n    def begin_state(self, batch_size, device):\n        return self.init_state(batch_size, self.num_hiddens, device)\n\nnum_hiddens = 512\nnet = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n                      init_rnn_state, rnn)\nstate = net.begin_state(X.shape[0], d2l.try_gpu())\nY, new_state = net(X.to(d2l.try_gpu()), state)\n</code></pre> <p>\u4f7f\u7528\u7f51\u7edc\u8fdb\u884c\u9884\u6d4b\uff0c\u524d\u51e0\u4e2a\uff08prefix\uff09\u4e0d\u4ea7\u751f\u8f93\u51fa\u4f46\u662f\u66f4\u65b0\u9690\u72b6\u6001</p> <pre><code>def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n    \"\"\"\u5728prefix\u540e\u9762\u751f\u6210\u65b0\u5b57\u7b26\"\"\"\n    state = net.begin_state(batch_size=1, device=device)\n    outputs = [vocab[prefix[0]]]\n    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1)) #\u7528\u4e0a\u6b21\u8f93\u51fa\u7684\u6700\u540e\u7684\u5b57\u7b26\n    #\u4e00\u4e2a\u4e00\u4e2a\u8fdb\u53bb\u5f97\u5230\u8f93\u51fa\n    for y in prefix[1:]:  # \u9884\u70ed\u671f\n        _, state = net(get_input(), state)\n        outputs.append(vocab[y])\n    for _ in range(num_preds):  # \u9884\u6d4bnum_preds\u6b65\n        y, state = net(get_input(), state)\n        outputs.append(int(y.argmax(dim=1).reshape(1)))\n    return ''.join([vocab.idx_to_token[i] for i in outputs])\n</code></pre> <p>\u8bad\u7ec3\u793a\u4f8b</p> <pre><code>def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n    state, timer = None, d2l.Timer()\n    metric = d2l.Accumulator(2)  # \u8bad\u7ec3\u635f\u5931\u4e4b\u548c,\u8bcd\u5143\u6570\u91cf\n    for X, Y in train_iter:\n        if state is None or use_random_iter:\n            # \u5728\u7b2c\u4e00\u6b21\u8fed\u4ee3\u6216\u4f7f\u7528\u968f\u673a\u62bd\u6837\u65f6\u8981\u91cd\u65b0\u521d\u59cb\u5316state\n            state = net.begin_state(batch_size=X.shape[0], device=device)\n        else:\n            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n                # state\u5bf9\u4e8enn.GRU\u662f\u4e2a\u5f20\u91cf\n                state.detach_()\n            else:\n                for s in state:\n                    s.detach_()\n        y = Y.T.reshape(-1)\n        X, y = X.to(device), y.to(device)\n        y_hat, state = net(X, state)\n        l = loss(y_hat, y.long()).mean()\n        #\u8fd9\u91ccy_hat\u4e3a\u4e8c\u7ef4\u5f20\u91cf\uff0cy\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u7c7b\u4f3c\u4e8e\u591a\u5206\u7c7b\u95ee\u9898\n        if isinstance(updater, torch.optim.Optimizer):\n            updater.zero_grad()\n            l.backward()\n            grad_clipping(net, 1)#\u68af\u5ea6\u526a\u88c1\n            updater.step()\n        else:\n            l.backward()\n            grad_clipping(net, 1)\n            # \u56e0\u4e3a\u5df2\u7ecf\u8c03\u7528\u4e86mean\u51fd\u6570\n            updater(batch_size=1)\n        metric.add(l * y.numel(), y.numel())#numel\u51fd\u6570\u8fd4\u56de\u5f20\u91cf\u5143\u7d20\u603b\u6570\u91cf\uff0c\n    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n</code></pre> <p>\u4f7f\u7528pytorch API</p> <pre><code>rnn_layer = nn.RNN(len(vocab), num_hiddens)\nstate = torch.zeros((1, batch_size, num_hiddens))#(\u9690\u85cf\u5c42\u6570\uff0c\u6279\u91cf\u5927\u5c0f\uff0c\u9690\u53d8\u91cf\u957f\u5ea6)\nX = torch.rand(size=(num_steps, batch_size, len(vocab)))\nY, state_new = rnn_layer(X, state)\n</code></pre>"},{"location":"DeepLearning/RNN/#_6","title":"\u53cd\u5411\u4f20\u64ad","text":"<p>\u4e3b\u8981\u662f\u5faa\u73af\u8ba1\u7b97\u68af\u5ea6\u7684\u65b9\u6cd5\u548c\u8fd1\u4f3c\u65b9\u6cd5\uff0ct\u5927\u65f6\u4ea7\u751f\u68af\u5ea6\u6d88\u5931\u6216\u8005\u7206\u70b8\u7684\u95ee\u9898 \u5177\u4f53\u53c2\u8003\u52a8\u624b\u6df1\u5ea6\u5b66\u4e60bptt\u7ae0\u8282</p>"},{"location":"DeepLearning/RNN/#gru","title":"GRU","text":"<p>\u91cd\u7f6e\u95e8\u548c\u66f4\u65b0\u95e8(\\(\\mathbb{R}_{b \\times h}\\))\u66f4\u65b0</p> \\[ \\begin{aligned} \\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r),\\\\ \\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z), \\end{aligned} \\] <p>($\\sigma \u4e3a $sigmoid\u51fd\u6570)</p> <p>\u9690\u5019\u9009\u72b6\u6001</p> \\[\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h),\\] <p>\u9690\u72b6\u6001\u66f4\u65b0</p> \\[\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.\\] <p>\u76f8\u6bd4\u524d\u9762\u57fa\u672cRNN\u53ea\u662f\u9690\u72b6\u6001\u66f4\u65b0\u516c\u5f0f\u66f4\u4e3a\u590d\u6742</p> <p>pytorch\u6846\u67b6</p> <pre><code>num_inputs = vocab_size\ngru_layer = nn.GRU(num_inputs, num_hiddens)\nmodel = d2l.RNNModel(gru_layer, len(vocab))\n</code></pre> <p>d2l.RNNModel</p> <pre><code>class RNNModel(nn.Module):\n    \"\"\"\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\"\"\"\n    def __init__(self, rnn_layer, vocab_size, **kwargs):\n        super(RNNModel, self).__init__(**kwargs)\n        self.rnn = rnn_layer\n        self.vocab_size = vocab_size\n        self.num_hiddens = self.rnn.hidden_size\n        # \u5982\u679cRNN\u662f\u53cc\u5411\u7684\uff08\u4e4b\u540e\u5c06\u4ecb\u7ecd\uff09\uff0cnum_directions\u5e94\u8be5\u662f2\uff0c\u5426\u5219\u5e94\u8be5\u662f1\n        if not self.rnn.bidirectional:\n            self.num_directions = 1\n            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n        else:\n            self.num_directions = 2\n            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n\n    def forward(self, inputs, state):\n        X = F.one_hot(inputs.T.long(), self.vocab_size)\n        X = X.to(torch.float32)\n        Y, state = self.rnn(X, state)\n        # \u5168\u8fde\u63a5\u5c42\u9996\u5148\u5c06Y\u7684\u5f62\u72b6\u6539\u4e3a(\u65f6\u95f4\u6b65\u6570*\u6279\u91cf\u5927\u5c0f,\u9690\u85cf\u5355\u5143\u6570)\n        # \u5b83\u7684\u8f93\u51fa\u5f62\u72b6\u662f(\u65f6\u95f4\u6b65\u6570*\u6279\u91cf\u5927\u5c0f,\u8bcd\u8868\u5927\u5c0f)\u3002\n        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n        return output, state\n\n    def begin_state(self, device, batch_size=1):\n        if not isinstance(self.rnn, nn.LSTM):\n            # nn.GRU\u4ee5\u5f20\u91cf\u4f5c\u4e3a\u9690\u72b6\u6001\n            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n                                 batch_size, self.num_hiddens),\n                                device=device)\n        else:\n            # nn.LSTM\u4ee5\u5143\u7ec4\u4f5c\u4e3a\u9690\u72b6\u6001\n            return (torch.zeros((\n                self.num_directions * self.rnn.num_layers,\n                batch_size, self.num_hiddens), device=device),\n                    torch.zeros((\n                        self.num_directions * self.rnn.num_layers,\n                        batch_size, self.num_hiddens), device=device))\n</code></pre>"},{"location":"DeepLearning/RNN/#lstm","title":"LSTM","text":"<p>\u8f93\u5165\u95e8\u662f\\(\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}\\)\uff0c \u9057\u5fd8\u95e8\u662f\\(\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}\\)\uff0c \u8f93\u51fa\u95e8\u662f\\(\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}\\)\u3002</p> <p>\u66f4\u65b0\u516c\u5f0f\u4e3a</p> \\[ \\begin{aligned} \\mathbf{I}_t &amp;= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i),\\\\ \\mathbf{F}_t &amp;= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f),\\\\ \\mathbf{O}_t &amp;= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o), \\end{aligned} \\] <p>\u5019\u9009\u8bb0\u5fc6\u5143</p> \\[\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c),\\] <p>\u8bb0\u5fc6\u5143</p> \\[\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t.\\] <p>\u9690\u72b6\u6001</p> \\[\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t).\\] <p>pytorch\u6846\u67b6</p> <pre><code>num_inputs = vocab_size\nlstm_layer = nn.LSTM(num_inputs, num_hiddens)\nmodel = d2l.RNNModel(lstm_layer, len(vocab))\n</code></pre>"},{"location":"DeepLearning/RNN/#deep-rnn","title":"Deep RNN","text":"<p>\u8bbe\u7f6e\\(\\mathbf{H}_t^{(0)} = \\mathbf{X}_t\\)\uff0c</p> <p>\u7b2c\\(l\\)\u5c42\u7684\u9690\u72b6\u6001\u66f4\u65b0</p> \\[\\mathbf{H}_t^{(l)} = \\phi_l(\\mathbf{H}_t^{(l-1)} \\mathbf{W}_{xh}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{hh}^{(l)}  + \\mathbf{b}_h^{(l)}),\\] <p>\u6700\u540e\uff0c\u8f93\u51fa\u5c42\u7684\u8ba1\u7b97\u4ec5\u57fa\u4e8e\u7b2c\\(l\\)\u4e2a\u9690\u85cf\u5c42\u6700\u7ec8\u7684\u9690\u72b6\u6001\uff1a</p> \\[\\mathbf{O}_t = \\mathbf{H}_t^{(L)} \\mathbf{W}_{hq} + \\mathbf{b}_q,\\] <p>\u4e5f\u80fd\u7528GRU\u6216\u8005LSTM\u6765\u4ee3\u66ff\u8fd9\u91cc\u7684\u9690\u72b6\u6001</p> <p>pytorch\u5b9e\u73b0\u6df1\u5ea6\u7684LSTM</p> <pre><code>lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)\nmodel = d2l.RNNModel(lstm_layer, len(vocab))\n</code></pre>"},{"location":"DeepLearning/RNN/#brnn","title":"BRNN","text":"\\[ \\begin{aligned} \\overrightarrow{\\mathbf{H}}_t &amp;= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{hh}^{(f)}  + \\mathbf{b}_h^{(f)}),\\\\ \\overleftarrow{\\mathbf{H}}_t &amp;= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{hh}^{(b)}  + \\mathbf{b}_h^{(b)}), \\end{aligned} \\] <p>\u6b63\u5411\u9690\u72b6\u6001\\(\\overrightarrow{\\mathbf{H}}_t\\)\u548c\u53cd\u5411\u9690\u72b6\u6001\\(\\overleftarrow{\\mathbf{H}}_t\\)\u8fde\u63a5\u8d77\u6765\uff0c\u83b7\u5f97\u9700\u8981\u9001\u5165\u8f93\u51fa\u5c42\u7684\u9690\u72b6\u6001\\(\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}\\)\u3002</p> <p>\\(\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}\\)\uff08\\(q\\)\u662f\u8f93\u51fa\u5355\u5143\u7684\u6570\u76ee\uff09:</p> \\[\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.\\] <p>\u8fd9\u91cc\\(\\mathbf{W}_{hq} \\in \\mathbb{R}^{2h \\times q}\\)</p>"},{"location":"DeepLearning/RNN/#seq2seq","title":"Seq2Seq","text":"<p>\u673a\u5668\u7ffb\u8bd1</p> <p>\u7f16\u7801\u5668\u662f\u4e00\u4e2aRNN(\u53ef\u4ee5\u662f\u53cc\u5411)\uff0c\u8bfb\u53d6\u8f93\u5165\u53e5\u5b50 \u89e3\u7801\u5668\u7528\u53e6\u4e00\u4e2aRNN\u6765\u8f93\u51fa</p> <p>\u7f16\u7801\u5668\u6700\u540e\u65f6\u95f4\u6b65\u7684\u9690\u72b6\u6001\u7528\u4f5c\u7f16\u7801\u5668\u7684\u521d\u59cb\u9690\u72b6\u6001</p> <p>\u8bad\u7ec3\u65f6\u89e3\u7801\u5668\u4f7f\u7528\u76ee\u6807\u53e5\u5b50\u4f5c\u4e3a\u8f93\u5165\uff08\u5c31\u7b97\u67d0\u4e00\u6b65\u9884\u6d4b\u9519\u4e86\uff0c\u4e0b\u4e00\u6b65\u8f93\u5165\u7684\u8fd8\u662f\u6b63\u786e\u7684\u7ffb\u8bd1\u7ed3\u679c\uff09</p>"},{"location":"DeepLearning/RNN/#_7","title":"\u9884\u6d4b\u5e8f\u5217\u8bc4\u4f30","text":"<p>\u8861\u91cf\u751f\u6210\u5e8f\u5217\u597d\u574f\u7684BLEU</p> \\[ \\exp\\left(\\min\\left(0, 1 - \\frac{\\mathrm{len}_{\\text{label}}}{\\mathrm{len}_{\\text{pred}}}\\right)\\right) \\prod_{n=1}^k p_n^{1/2^n},\\] <p>\\(p_n\\)\u8868\u793a\u9884\u6d4b\u4e2d\u6240\u6709n_gram\uff08n\u5143\u8bed\u6cd5\uff09\u7684\u7cbe\u5ea6</p> <p>Example \u6807\u7b7e\u5e8f\u5217ABCDEF\uff0c\u9884\u6d4b\u5e8f\u5217\u4e3aABBCD \\(p_1\\) = 4/5 \\(p_2\\) = 3/4 \\(p_3\\) = 1/3 \\(p_4\\) = 0</p> <pre><code>class Seq2SeqEncoder(d2l.Encoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqEncoder, self).__init__(**kwargs)\n        # \u5d4c\u5165\u5c42\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n                          dropout=dropout)\n\n    def forward(self, X, *args):\n        # \u8f93\u5165'X'\u5f62\u72b6(batch_size,num_steps) \u8f93\u51fa'X'\u7684\u5f62\u72b6\uff1a(batch_size,num_steps,embed_size)\n        X = self.embedding(X)\n        # \u5728\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\uff0c\u7b2c\u4e00\u4e2a\u8f74\u5bf9\u5e94\u4e8e\u65f6\u95f4\u6b65\n        X = X.permute(1, 0, 2)\n        # \u5982\u679c\u672a\u63d0\u53ca\u72b6\u6001\uff0c\u5219\u9ed8\u8ba4\u4e3a0\n        output, state = self.rnn(X)\n        # output\u7684\u5f62\u72b6:(num_steps,batch_size,num_hiddens)\n        # state\u7684\u5f62\u72b6:(num_layers,batch_size,num_hiddens)\n        return output, state\n</code></pre> <p>Decoder\u7684embedding\u548cEncoder\u4e0d\u540c</p> <pre><code>class Seq2SeqDecoder(d2l.Decoder):\n    \"\"\"\u7528\u4e8e\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u89e3\u7801\u5668\"\"\"\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqDecoder, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n                          dropout=dropout)\n        self.dense = nn.Linear(num_hiddens, vocab_size)\n\n    def init_state(self, enc_outputs, *args):\n        #outputs:(output,state)\n        return enc_outputs[1]\n\n    def forward(self, X, state):\n        # \u8f93\u51fa'X'\u7684\u5f62\u72b6\uff1a(batch_size,num_steps,embed_size)\n        X = self.embedding(X).permute(1, 0, 2)\n        # \u5e7f\u64adcontext\uff0c\u4f7f\u5176\u5177\u6709\u4e0eX\u76f8\u540c\u7684num_steps\n        context = state[-1].repeat(X.shape[0], 1, 1)#\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u6700\u540e\u4e00\u5c42\u8f93\u51fa\n        X_and_context = torch.cat((X, context), 2)\n        output, state = self.rnn(X_and_context, state)\n        output = self.dense(output).permute(1, 0, 2)\n        # output\u7684\u5f62\u72b6:(batch_size,num_steps,vocab_size)\n        # state\u7684\u5f62\u72b6:(num_layers,batch_size,num_hiddens)\n        return output, state\n</code></pre> <p>\u5728\u5e8f\u5217\u4e2d\u5c4f\u853d\u4e0d\u76f8\u5173\u7684\u9879</p> <pre><code>def sequence_mask(X, valid_len, value=0):\n    maxlen = X.size(1)\n    mask = torch.arange((maxlen), dtype=torch.float32,\n                        device=X.device)[None, :] &lt; valid_len[:, None]\n    X[~mask] = value\n    return X\n\nX = torch.tensor([[1, 2, 3], [4, 5, 6]])\nsequence_mask(X, torch.tensor([1, 2]))\n</code></pre> <p>tensor([ [1, 0, 0],         [4, 5, 0] ])</p> <pre><code>class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n    \"\"\"\u5e26\u906e\u853d\u7684softmax\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\"\"\"\n    # pred\u7684\u5f62\u72b6\uff1a(batch_size,num_steps,vocab_size)\n    # label\u7684\u5f62\u72b6\uff1a(batch_size,num_steps)\n    # valid_len\u7684\u5f62\u72b6\uff1a(batch_size,)\n    def forward(self, pred, label, valid_len):\n        weights = torch.ones_like(label)\n        weights = sequence_mask(weights, valid_len)\n        self.reduction='none'\n        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n            pred.permute(0, 2, 1), label)\n        weighted_loss = (unweighted_loss * weights).mean(dim=1) #\u65e0\u6548\u7684\u5168\u90e8\u7f6e\u4e3a0\uff0c\u5bf9\u6bcf\u4e2a\u6837\u672c\u8fd4\u56deloss\n        return weighted_loss\n</code></pre> <p>\uff08\u5177\u4f53\u6ca1\u770b\u5f88\u61c2\u600e\u4e48\u7ec3\u7684\uff0c\u5148\u8fc7\u4e86\u540e\u9762\u8865</p>"},{"location":"DeepLearning/SwinTransformer/","title":"Swin Transformer","text":"<p>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows \uff082021.5\uff09</p> <p>\u5b98\u65b9\u4ee3\u7801</p>"},{"location":"DeepLearning/SwinTransformer/#_1","title":"\u8bba\u6587\u9605\u8bfb","text":"<p>ViT\u53ea\u505a\u4e86\u5206\u7c7b\u4efb\u52a1\uff0cSwin Transformer\u8bf4\u660e\u4e86transformer\u53ef\u4ee5\u505a\u4e3aCV\u9886\u57df\u7684\u9aa8\u5e72\u7f51\u7edc</p> <p></p> <p>Transformer\u6bcf\u4e2apatch\u7684\u5c3a\u5bf8\u90fd\u662f\u76f8\u540c\u7684\uff0c\u867d\u7136\u80fd\u591f\u901a\u8fc7\u5168\u5c40\u7684\u81ea\u6ce8\u610f\u529b\u8fbe\u5230\u5168\u5c40\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u4f46\u662f\u5bf9\u4e8e\u591a\u5c3a\u5bf8\u7684\u4fe1\u606f\u628a\u63e1\u8f83\u5f31</p> <p>CNN\u591a\u5c3a\u5bf8\u7684\u4fe1\u606f</p> <p>\u76ee\u6807\u68c0\u6d4bFPN \u5229\u7528\u6bcf\u4e2a\u5377\u79ef\u5c42\u51fa\u6765\u7684\u7279\u5f81</p> <p>\u7269\u4f53\u5206\u5272UNet Skip Connection \u4e0a\u91c7\u6837\u7684\u8fc7\u7a0b\u4e2d\u8fd8\u5229\u7528\u4e4b\u524d\u4e0b\u91c7\u6837\u8fc7\u7a0b\u4e2d\u62ff\u51fa\u6765\u7684\u7279\u5f81</p> <p>ViT\u7531\u4e8e\u4f7f\u7528\u5168\u5c40\u5efa\u6a21\uff0c\u6a21\u578b\u7684\u590d\u6742\u5ea6\u548c\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u7684\u5e73\u65b9\u500d\u8fdb\u884c\u589e\u957f\uff1bSwin Transformer\u5728\u5c0f\u7a97\u53e3\u4e0a\u8fdb\u884c\u6ce8\u610f\u529b\u7684\u8ba1\u7b97</p> <p>\u7c7b\u4f3c\u4e8eCNN\u7684Pooling\u64cd\u4f5c\uff0cSwin Transformer\u4f7f\u7528\u4e86Patch Merging\u7684\u64cd\u4f5c\uff0c\u589e\u5927\u611f\u53d7\u91ce\uff0c\u5e76\u80fd\u591f\u6293\u4f4f\u56fe\u50cf\u7684\u591a\u5c3a\u5bf8\u4fe1\u606f</p>"},{"location":"DeepLearning/SwinTransformer/#_2","title":"\u6574\u4f53\u67b6\u6784","text":"<pre><code>\u8f93\u5165\u56fe\u7247\u4e3a224x224x3\n--&gt; Patch Partition 56x56x48\n--&gt; Liner Embedding 3136 x 96(\u8fd9\u7684\u7684Swin-T C\u4e3a96)\n--&gt; Swin Transformer Block \u4e0d\u6539\u53d8\u7ef4\u5ea6\uff083136\u663e\u7136\u592a\u5927\uff0c\u8fd9\u91cc\u57fa\u4e8e\u7a97\u53e3\u8ba1\u7b97\u81ea\u6ce8\u610f\u529b\uff09\n--&gt; Patch Merging \uff08\u5148\u5f97\u5230H/2 W/2 4*C \u8fc71x1\u7684\u5377\u79ef\u5f97\u52302*C \uff0928x28x192\n--&gt; stage3 14x14x384\n--&gt; stage4 7x7x768\n\uff08\u5206\u7c7b\u4efb\u52a1 --&gt;1x768-&gt;\u5168\u8fde\u63a5\uff09\n</code></pre> <p>patch merging\u600e\u4e48\u91c7\u6837\u7684\uff1a</p> <pre><code>x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\nx1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\nx2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\nx3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\nx = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n</code></pre>"},{"location":"DeepLearning/SwinTransformer/#_3","title":"\u57fa\u4e8e\u79fb\u52a8\u7a97\u53e3\u7684\u81ea\u6ce8\u610f\u529b","text":""},{"location":"DeepLearning/SwinTransformer/#_4","title":"\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97","text":"<pre><code>56x56x96\u7684\u8f93\u5165\uff0c\u5206\u6210\u5c0f\u7a97\u53e3\uff0c\u6bcf\u4e2a\u7a97\u53e3\u6709MxM\u4e2apatch\uff08M\u9ed8\u8ba4\u4e3a7\uff09\uff0c\u67098x8=64\u4e2a\u7a97\u53e3\n</code></pre> <p>\u8ba1\u7b97\u81ea\u6ce8\u610f\u529b\u7684\u590d\u6742\u5ea6\u5047\u8bbe\u56fe\u50cf\u7684\u9ad8\u5bbd\u4e3a \\(h \\times w\\)\uff0c\u6ce8\u610f\u529b\u7684\u8f93\u5165\u5c31\u662f \\(hw \\times C\\)\uff0c\u5982\u679c\u4f7f\u7528\u5168\u5c40\u81ea\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\uff0c\u5206\u4e3a\uff1a</p> <ol> <li>\u8ba1\u7b97qkv\u3002\u662f\\(hw \\times C\\) \u4e58\u4e00\u4e2a \\(C \\times C\\)\u7684\u77e9\u9635\uff0c\u590d\u6742\u5ea6\u4e3a \\(hwC^2 \\times 3\\)</li> <li>\u8ba1\u7b97\u6743\u91cd\u77e9\u9635A\uff0c\u7136\u540e\u4e58v\u3002\u662f q\u4e58v\u7684\u8f6c\u7f6e\uff0c\u5c31\u662f\\(hw \\times C\\)\u4e58\u4e0a \\(C \\times hw\\)\uff0c\u5f97\u5230\u6743\u91cd\u77e9\u9635\u548c\u548cv\u4e58\uff0c\u5c31\u662f \\(hw \\times hw\\)\u4e58\u4e0a\\(hw \\times C\\) \uff0c\u590d\u6742\u5ea6\u4e3a \\((hw)^2C + (hw)^2C\\)</li> <li>\u6295\u5f71\u5f97\u5230\u8f93\u51fa\u3002\u5c31\u662f\\(hw \\times C\\) \u4e58\u4e00\u4e2a \\(C \\times C\\)\u7684\u77e9\u9635\u3002</li> </ol> <p>\u56e0\u6b64\u590d\u6742\u5ea6\u4e3a\\(\\Omega(MSA) = 4hwC^2 + 2(hw)^2C\\)</p> <p>\u5982\u679c\u91c7\u7528\u7a97\u53e3\u8ba1\u7b97\uff0c\u4e0a\u9762\u516c\u5f0f\u7684\\(h=w=M\\)\uff0c\u590d\u6742\u5ea6\u5c31\u662f\\(4MC^2 + 2M^4C\\)\u603b\u5171\u6709\\(\\frac{h}{M} \\times \\frac{w}{M}\\)\u4e2a\u7a97\u53e3\uff0c\u56e0\u6b64\u590d\u6742\u5ea6\u4e3a\\(\\Omega(W-MSA) = 4hwC^2 + 2M^2 hwC\\)</p>"},{"location":"DeepLearning/SwinTransformer/#_5","title":"\u8fde\u7eed\u5757\u7684\u79fb\u52a8\u7a97\u53e3\u5206\u533a","text":"<p>\u867d\u7136\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u80fd\u591f\u5f88\u597d\u7684\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f46\u662f\u7a97\u53e3\u548c\u7a97\u53e3\u4e4b\u95f4\u5c31\u6ca1\u6709\u901a\u4fe1\u4e86\uff0c\u8fd9\u6837\u5c31\u8fbe\u4e0d\u5230\u5168\u5c40\u5efa\u6a21\u4e86\uff0c\u79fb\u52a8\u7a97\u53e3\u5c31\u662f\u4e3a\u4e86\u7a97\u53e3\u4e4b\u95f4\u7684\u901a\u4fe1\u3002\u6a21\u578b\u7684\u4e24\u4e2a\u8fde\u7eed\u7684tansformer\u5757\u4e2d\u4e00\u4e2a\u505a\u4e00\u6b21\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\uff0c\u4e00\u4e2a\u505a\u4e00\u6b21\u79fb\u52a8\u7a97\u53e3\u7684\u81ea\u6ce8\u610f\u529b\u3002</p> <p>\u4f46\u662f\u76f4\u63a5\u505a\u79fb\u52a8\u7a97\u53e3\u7684\u8bdd\u5728\u8fb9\u4e0a\u4f1a\u51fa\u73b0\u7a97\u53e3\u5927\u5c0f\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u5faa\u73af\u79fb\u4f4d\u7684\u81ea\u6ce8\u610f\u8ba1\u7b97\u65b9\u6cd5</p> <p></p> <p>\u7531\u4e8e\u8fb9\u4e0a\u7684\u7a97\u53e3\u7684\u4e1c\u897f\u662f\u4ece\u522b\u7684\u5730\u65b9\u79fb\u8fc7\u6765\u7684\uff0c\u56e0\u6b64\u4e0d\u5e94\u8be5\u5bf9\u6574\u4e2a\u7a97\u53e3\u505a\u81ea\u6ce8\u610f\u529b\uff0c\u56e0\u6b64\u4f7f\u7528\u63a9\u7801\u7684\u64cd\u4f5c\u7b97\u6ce8\u610f\u529b\uff0c\u5b8c\u4e86\u5728\u628a\u5faa\u73af\u4f4d\u79fb\u8fd8\u539f\u3002</p> <p>\u63a9\u7801\u64cd\u4f5c\uff1a</p> <p> \uff08window1\u8fd9\u6837\u662f\u56e0\u4e3aFlatten\u6a2a\u7740\u6765\u7684\uff0c\u987a\u5e8f\u5dee\u4e0d\u591a\u662f111..22...111....\uff09</p>"},{"location":"DeepLearning/ViT/","title":"Vision Transformer","text":"<p>An Image Is Worth 16X16 Words: Transformers For Image Recognition At Scale \uff082020.10\uff09</p> <p>\u5b98\u65b9\u4ee3\u7801</p> <p>\u53c2\u8003\u4ee3\u7801</p> <p></p> <p>\u4e3b\u8981\u5c31\u662f\u56fe\u7247\u8f6c\u6362tokens\uff0cposition embedding\u8fd9\u91cc\u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570</p> <pre><code>\u8f93\u5165224x224x3 \n-&gt; Embedding(16x16\u7684\u5377\u79ef\u6838\uff0c\u6b65\u8ddd\u4e3a16\u7684\u5377\u79ef\u5c42) 14x14x768 \n-&gt; Flatten 196x768 \n-&gt; Concat\u4e00\u4e2aClass token 197x768 \n-&gt; \u52a0\u4e0aposition embedding -&gt; Dropout \n-&gt; \u91cd\u590dL\u6b21 Transformer Encoder 197x768\n-&gt; LayerNorm \n-&gt; Extract class token 1x768(\u4e4b\u524dconcat\u7684class token)\n-&gt; MLP Head\n</code></pre> <pre><code>import torch\nfrom torch import nn\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.norm = nn.LayerNorm(dim)\n\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        x = self.norm(x)\n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -&gt; b h n d', h = self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.attend(dots)\n        attn = self.dropout(attn)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -&gt; b n (h d)')\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n                FeedForward(dim, mlp_dim, dropout = dropout)\n            ]))\n\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n\n        return self.norm(x)\n\nclass ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n        super().__init__()\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        patch_dim = channels * patch_height * patch_width\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n            nn.LayerNorm(patch_dim),\n            nn.Linear(patch_dim, dim),\n            nn.LayerNorm(dim),\n        )\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Linear(dim, num_classes)\n\n    def forward(self, img):\n        x = self.to_patch_embedding(img)\n        b, n, _ = x.shape\n\n        cls_tokens = repeat(self.cls_token, '1 1 d -&gt; b 1 d', b = b)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding[:, :(n + 1)]\n        x = self.dropout(x)\n\n        x = self.transformer(x)\n\n        #cls extract\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)\n</code></pre> <pre><code>from torchsummary import summary\n\nmodel_vit = ViT(\n        image_size = 224,\n        patch_size = 16,\n        num_classes = 1000,\n        dim = 768,\n        depth = 6,\n        heads = 16,\n        mlp_dim = 2048,\n        dropout = 0.1,\n        emb_dropout = 0.1\n    )\n\nimg = torch.randn(16, 3, 224, 224)\nsummary(model_vit, input_size=img.shape[1:],batch_size=img.shape[0])\nprint(model_vit(img).shape)\n</code></pre> <pre><code>----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n         Rearrange-1             [16, 196, 768]               0\n         LayerNorm-2             [16, 196, 768]           1,536\n            Linear-3             [16, 196, 768]         590,592\n         LayerNorm-4             [16, 196, 768]           1,536\n           Dropout-5             [16, 197, 768]               0\n         LayerNorm-6             [16, 197, 768]           1,536\n            Linear-7            [16, 197, 3072]       2,359,296\n           Softmax-8         [16, 16, 197, 197]               0\n           Dropout-9         [16, 16, 197, 197]               0\n           Linear-10             [16, 197, 768]         787,200\n          Dropout-11             [16, 197, 768]               0\n        Attention-12             [16, 197, 768]               0\n        LayerNorm-13             [16, 197, 768]           1,536\n           Linear-14            [16, 197, 2048]       1,574,912\n             GELU-15            [16, 197, 2048]               0\n          Dropout-16            [16, 197, 2048]               0\n           Linear-17             [16, 197, 768]       1,573,632\n          Dropout-18             [16, 197, 768]               0\n      FeedForward-19             [16, 197, 768]               0\n        LayerNorm-20             [16, 197, 768]           1,536\n           Linear-21            [16, 197, 3072]       2,359,296\n          Softmax-22         [16, 16, 197, 197]               0\n\ntorch.Size([16, 1000])\n</code></pre>"},{"location":"PEFT/CLIP/","title":"CLIP","text":"<p>Contrastive Language-Image Pre-training</p> <p>Learning Transferable Visual Models From Natural Language Supervision</p> <p>OpenAI Blog</p> <p></p>"},{"location":"PEFT/CLIP/#_1","title":"\u9884\u8bad\u7ec3\u65b9\u6cd5","text":"<p>\u6587\u672c\u548c\u56fe\u50cf\u5206\u522b\u4f7f\u7528\u9884\u8bad\u7ec3\u597d\u7684\u7f16\u7801\u5668\u5bf9\u6bcf\u4e2a\u8f93\u5165\u7684\u6587\u672c\u548c\u56fe\u50cf\u8fdb\u884c\u7f16\u7801\uff08ResNet50\u3001ViT...\uff09\uff0c\u7f16\u7801\u5b8c\u6210\u7684\u6587\u672c\u7279\u5f81\u548c\u89c6\u89c9\u7279\u5f81\u8ba1\u7b97\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0cCLIP\u7684\u8bad\u7ec3\u76ee\u6807\u5373\u4f7f\u6700\u5927\u5316\u8bc1\\(N\\)\u4e2a\u6b63\u6837\u672c\u7684\u76f8\u4f3c\u5ea6\uff0c\u6700\u5c0f\u5316\\(N^2 -N\\)\u4e2a\u6837\u672c\u7684\u76f8\u4f3c\u5ea6\u3002</p> <pre><code># image_encoder - ResNet or Vision Transformer \n# text_encoder - CBOW or Text Transformer \n# I[n, h, w, c] - minibatch of aligned images \n# T[n, l] - minibatch of aligned texts \n# W_i[d_i, d_e] - learned proj of image to embed \n# W_t[d_t, d_e] - learned proj of text to embed \n# t - learned temperature parameter \n\n# extract feature representations of each modality \nI_f = image_encoder(I) #[n, d_i] \nT_f = text_encoder(T) #[n, d_t] \n\n# joint multimodal embedding [n, d_e] \nI_e = l2_normalize(np.dot(I_f, W_i), axis=1) \nT_e = l2_normalize(np.dot(T_f, W_t), axis=1) # scaled pairwise cosine similarities [n, n] \n\nlogits = np.dot(I_e, T_e.T) * np.exp(t) \n\n# symmetric loss function \nlabels = np.arange(n) \nloss_i = cross_entropy_loss(logits, labels, axis=0) \nloss_t = cross_entropy_loss(logits, labels, axis=1) \nloss = (loss_i + loss_t)/2\n</code></pre>"},{"location":"PEFT/CLIP/#zero-shot","title":"Zero-shot\u63a8\u7406","text":"<p>Prompt Tempelate \u6bd4\u5982\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u628a\u6807\u7b7e\u641e\u6210 A photo of [object]\u7684\u6837\u5b50\uff0c\u8f93\u5165\u6587\u672c\u7f16\u7801\u5668\uff0c\u628a\u56fe\u50cf\u8f93\u5165\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u7136\u540e\u8ba1\u7b97\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6700\u5927\u7684\u5c31\u662f\u56fe\u50cf\u7684Label\u3002</p> <p>\u6a21\u578b\u4e0d\u9700\u8981\u50cf\u4ee5\u5f80\u7684\u5206\u7c7b\u6a21\u578b\u4e00\u6837\u8bbe\u7f6e\u4e00\u4e2a\u56fa\u5b9a\u7684\u7c7b\u522b\u96c6\u5408\uff0c\u53ea\u8981\u80fd\u591f\u5728\u6587\u672c\u8f93\u5165\u7684\u5730\u65b9\u52a0\u5165\u56fe\u50cf\u771f\u5b9e\u7684\u7c7b\u522b\u540d\u79f0\uff0c\u8fd9\u91cc\u4e0d\u4e00\u5b9a\u662fImageNet\u4e0a\u7684\u7c7b\u548c\u56fe\u50cf\u4e5f\u5f88\u6709\u53ef\u80fd\u88ab\u6b63\u786e\u5206\u7c7b\u3002\u4e0d\u4f7f\u7528ImageNet\u6570\u636e\u96c6\u4e0a\u4efb\u4f55\u4e00\u5f20\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u7684\u63a8\u7406\u6548\u679c\u548c\u6709\u76d1\u7763\u8bad\u7ec3\u597d\u7684ResNet50\u76f8\u5f53\u3002</p>"},{"location":"PEFT/CoCoOp/","title":"CoCoOp","text":"<p>Conditional Prompt Learning for Vision-Language Models</p> <p>CoOp\u7684\u6cdb\u5316\u6027\u5e76\u4e0d\u597d\uff0c\u5f88\u96be\u6cdb\u5316\u5230\u540c\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u7684unsee class\uff0c\u8bf4\u660e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fc7\u62df\u5408\u5230\u4e86base classes\u3002</p> <p></p>"},{"location":"PEFT/CoCoOp/#_1","title":"\u4e3b\u8981\u65b9\u6cd5","text":"<p>\u4ee4\\(x\\)\u8868\u793a\u56fe\u50cf\u7ecf\u8fc7image encoder\u7684\u7ed3\u679c\uff0c\\(h_{\\theta}\\) \u8868\u793a\u53c2\u6570\u4e3a \\(\\theta\\) \u7684Meta-Net\u3002\u6bcf\u4e2a\u4e0a\u4e0b\u6587token \\(v_m(x) = v_m + \\pi\\) \uff0c\\(\\pi = h_{\\theta}(x)\\) \uff0c\\(m \\in \\{1,2,...,M\\}\\) \uff0c\u8f93\u5165\u7684prompt \\(t_i(x) = \\{v_1(x),...v_M(x),c_i\\}\\)\uff0c\u7c7b\u522b\u6982\u7387\u8ba1\u7b97\u4e3a</p> \\[ p(y|x) = \\frac{\\exp(\\text{sim}(g(t_y(x)),x)/\\tau)}{\\sum_{j=1}^K\\exp(\\text{sim}(g(t_i(x)),x)/\\tau)} \\]"},{"location":"PEFT/CoOp/","title":"CoOp","text":"<p>Context Optimazation</p> <p>Learning to Prompt for Vision-Language Models</p> <p></p>"},{"location":"PEFT/CoOp/#unified-context","title":"Unified Context","text":"<p>\u6240\u6709\u7684\u7c7b\u522b\u4f7f\u7528\u76f8\u540c\u7684\u4e0a\u4e0b\u6587</p> \\[ t = [\\mathbf{V}]_1[\\mathbf{V}]_2...[\\mathbf{V}]_M[\\text{CLASS}] \\] <p>\u8fd9\u91cc\\([\\mathbf{V}]_m\\)\u662fembedding\u540e\u7684\u5411\u91cf\uff08512 in CLIP\uff09\uff0cM\u662f\u8d85\u53c2\u6570\u3002</p> <p>\u4ee4g\u8868\u793atext encoder\uff0c\u6bcf\u4e2a\u7c7b\u522b\u7684\u9884\u6d4b\u4e3a</p> \\[ p(y=i|x) = \\frac{\\exp(\\cos(g(t_i),f)/\\tau)}{\\sum_{j=1}^K\\exp(\\cos(g(t_j),f)/\\tau)} \\] <p>\u540c\u6837\u4e5f\u80fd\u591f\u628a[CLASS]\u653e\u5728\u4e2d\u95f4</p> \\[ t = [\\mathbf{V}]_1[\\mathbf{V}]_2...[\\mathbf{V}]_{\\frac{M}{2}}[\\text{CLASS}][\\mathbf{V}]_{\\frac{M}{2} + 1}...[\\mathbf{V}]_{M} \\]"},{"location":"PEFT/CoOp/#class-specific-context","title":"Class-Specific Context","text":"<p>\u5bf9\u4e8e\u6bcf\u4e2a\u7c7b\u522b\\(i,j \\in \\{1,2,...,K\\}\\)\uff0c</p> \\[ [\\mathbf{V}]_1^i[\\mathbf{V}]_2^i...[\\mathbf{V}]_M^i  \\neq [\\mathbf{V}]_1^j[\\mathbf{V}]_2^j...[\\mathbf{V}]_M^j ,i \\neq j \\]"},{"location":"PEFT/CoOp/#training","title":"Training","text":"<p>\u6700\u5c0f\u5316\u4ea4\u53c9\u71b5\u8fdb\u884c\u68af\u5ea6\u53cd\u4f20</p>"},{"location":"PEFT/LoRA/","title":"LoRA","text":"<p>Low-Rank Adaptation of Large Language Models</p> <p>\u8bba\u6587</p>"},{"location":"PEFT/LoRA/#_1","title":"\u4e3b\u8981\u65b9\u6cd5","text":"<p>\u5bf9\u4e8e\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u7684\u4e00\u4e2a\u6743\u91cd\u77e9\u9635 \\(W_0 \\in \\mathbb{R}^{d \\times k}\\) \uff0c \u4f7f\u7528 \\(W_0 + \\Delta W = W_0 + BA\\) \u5176\u4e2d \\(B \\in \\mathbb{R}^{d \\times r}\\) \uff0c\\(A \\in \\mathbb{R}^{r \\times k}\\) \uff0c \\(r \\ll \\min(d,k)\\)\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\\(W_0\\)\u4fdd\u6301\u56fa\u5b9a\uff0c\\(B A\\)\u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u3002\u5bf9\u4e8e \\(h = W_0 x\\) \uff0c\u524d\u9012\u5c42\u53d8\u4e3a </p> \\[ h = W_0 x + BAx \\] <p>\u521d\u59cb\u5316\\(A\\)\u7528\u9ad8\u65af\uff0c\u521d\u59cb\u5316\\(B\\)\u4e3a0\u77e9\u9635\uff0c\u6240\u4ee5\u4e00\u5f00\u59cb\\(W_0\\)\u5c31\u662f0\u3002\u7528\\(\\frac{\\alpha}{r}\\)\u7f29\u653e\\(\\Delta Wx\\) \uff0c\\(\\alpha\\)\u662f\u5e38\u6570\uff0c\u4f7f\u7528Adam\u4f18\u5316\u5668\uff0c\u8c03\u6574\u4e3a\u548c\u521d\u59cb\u5b66\u4e60\u7387\u5927\u81f4\u76f8\u540c\u3002</p>"},{"location":"PEFT/LoRA/#_2","title":"\u68af\u5ea6\u8ba1\u7b97","text":"<p>todo</p>"},{"location":"PEFT/LoRA/#apply-to-transformer","title":"Apply to Transformer","text":"<p>\u5728Self-Attention \u6a21\u5757\u4e2d\uff0c\u5bf9\u56db\u4e2a\u53c2\u6570\\(W_q,W_k,W_v,W_o\\)\u8fdb\u884c\u4e0a\u8ff0</p>"}]}